{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccf7e7b-8e12-4d8d-baf5-c55c6ee106c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.signal import detrend\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e0ccaf-b38f-4afa-9ad4-56df047b12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdbaf7e0-7c1e-4727-9281-ca69a69db6a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detrend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetrend_signal\u001b[39m(signal):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from scipy.signal import detrend\n",
    "\n",
    "def detrend_signal(signal):\n",
    "    return cp.array(detrend(cp.asnumpy(signal), axis=-1))\n",
    "\n",
    "def baseline_correction(baseline, stimuli):\n",
    "    baseline_mean = cp.mean(baseline, axis=-1, keepdims=True)\n",
    "    return stimuli - baseline_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76db1f4f-e4a4-4fbd-b7d0-b17ba7dbae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function\n",
    "def feature_extraction_EEG(raw, secs):\n",
    "    # 128 Hz is the sampling rate for the EEG data\n",
    "    fs_EEG = 128\n",
    "    N_EEG = math.ceil(fs_EEG * secs)\n",
    "    features = []\n",
    "    for electrode in eeg_electrodes:\n",
    "        features += [f'{electrode}_raw']\n",
    "\n",
    "    # DataFrame to store features\n",
    "    columns = ['participant', 'video'] + features\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for participant in range(23):\n",
    "        for video in range(18):\n",
    "            feature_values = []\n",
    "            for i in range(14):\n",
    "                basl = raw['DREAMER'][0, 0]['Data'][0, participant]['EEG'][0, 0]['baseline'][0, 0][video, 0][-1 - N_EEG: -1, i]\n",
    "                stimuli = raw['DREAMER'][0, 0]['Data'][0, participant]['EEG'][0, 0]['stimuli'][0, 0][video, 0][-1 - N_EEG: -1, i]\n",
    "                \n",
    "                # Detrending\n",
    "                basl_detrended = detrend_signal(basl)\n",
    "                stimuli_detrended = detrend_signal(stimuli)\n",
    "                \n",
    "                # Baseline correction\n",
    "                stimuli_corrected = baseline_correction(basl_detrended, stimuli_detrended)\n",
    "                \n",
    "                # Append raw signal\n",
    "                feature_values.append(cp.asnumpy(stimuli_corrected))\n",
    "            \n",
    "            # Append to DataFrame\n",
    "            row = [participant + 1, video + 1] + feature_values\n",
    "            if len(row) == len(columns):\n",
    "                df.loc[len(df)] = row\n",
    "            else:\n",
    "                print(f'Skipping participant {participant + 1}, video {video + 1}')\n",
    "                print(len(row))\n",
    "                print(len(columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a942b1-bb9a-4f7a-8ce5-6114812088d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_electrodes = [str(e[0]) for e in raw['DREAMER']['EEG_Electrodes'][0][0][0]]\n",
    "df = feature_extraction_EEG(raw, secs=60)  # Adjust `secs` as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "11310726-207f-41c8-bca4-b84250ce1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "raw = sio.loadmat('../DREAMER_Dataset/DREAMER.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c79ce09-5dc4-450a-af57-24f8c8aee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Extracted_Features/Raw_Data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
    "# The following example reads the resulting pickled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8d3d60-7d58-4a95-a386-8a1030c97a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Extracted_Features/Raw_Data.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a4b98f-23b5-4b4e-88e2-bf6564ce233a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38710523,  7.8181817 ,  5.25423785, ...,  3.39546835,\n",
       "        9.0366527 , -0.70677832])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"AF3_raw\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d097597d-cf3e-4e6e-9e52-4d6ca534677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with emotion, participant, and video data\n",
    "a=np.zeros((23,18,9),dtype=object)\n",
    "for participant in range(0,23):\n",
    "    for video in range(0,18):\n",
    "        a[participant,video,0]=raw['DREAMER'][0,0]['Data'][0,participant]['Age'][0][0][0]\n",
    "        a[participant,video,1]=raw['DREAMER'][0,0]['Data'][0,participant]['Gender'][0][0][0]\n",
    "        a[participant,video,2]=participant+1\n",
    "        a[participant,video,3]=video+1\n",
    "        a[participant,video,4]=['Searching for Bobby Fischer','D.O.A.', 'The Hangover', 'The Ring', '300',\n",
    "                  'National Lampoon\\'s VanWilder', 'Wall-E', 'Crash', 'My Girl', 'The Fly',\n",
    "                  'Pride and Prejudice', 'Modern Times', 'Remember the Titans', 'Gentlemans Agreement',\n",
    "                  'Psycho', 'The Bourne Identitiy', 'The Shawshank Redemption', 'The Departed'][video]\n",
    "        a[participant,video,5]=['calmness', 'surprise', 'amusement', 'fear', 'excitement', 'disgust',\n",
    "                  'happiness', 'anger', 'sadness', 'disgust', 'calmness', 'amusement',\n",
    "                  'happiness', 'anger', 'fear', 'excitement', 'sadness', 'surprise'][video]\n",
    "        a[participant,video,6]=raw['DREAMER'][0,0]['Data'][0,participant]['ScoreValence'][0,0][video,0]\n",
    "        a[participant,video,7]=raw['DREAMER'][0,0]['Data'][0,participant]['ScoreArousal'][0,0][video,0]\n",
    "        a[participant,video,8]=raw['DREAMER'][0,0]['Data'][0,participant]['ScoreDominance'][0,0][video,0]\n",
    "b=pd.DataFrame(a.reshape((23*18,a.shape[2])),columns=['Age','Gender','Participant','Video','Video_Name','Target_Emotion','Valence','Arousal','Dominance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8816971-86c3-4589-bec9-225730936a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Video</th>\n",
       "      <th>Video_Name</th>\n",
       "      <th>Target_Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Searching for Bobby Fischer</td>\n",
       "      <td>calmness</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>D.O.A.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The Hangover</td>\n",
       "      <td>amusement</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The Ring</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>excitement</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>Gentlemans Agreement</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>Psycho</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>The Bourne Identitiy</td>\n",
       "      <td>excitement</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>The Departed</td>\n",
       "      <td>surprise</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age Gender Participant Video                   Video_Name Target_Emotion  \\\n",
       "0    22   male           1     1  Searching for Bobby Fischer       calmness   \n",
       "1    22   male           1     2                       D.O.A.       surprise   \n",
       "2    22   male           1     3                 The Hangover      amusement   \n",
       "3    22   male           1     4                     The Ring           fear   \n",
       "4    22   male           1     5                          300     excitement   \n",
       "..   ..    ...         ...   ...                          ...            ...   \n",
       "409  25   male          23    14         Gentlemans Agreement          anger   \n",
       "410  25   male          23    15                       Psycho           fear   \n",
       "411  25   male          23    16         The Bourne Identitiy     excitement   \n",
       "412  25   male          23    17     The Shawshank Redemption        sadness   \n",
       "413  25   male          23    18                 The Departed       surprise   \n",
       "\n",
       "    Valence Arousal Dominance  \n",
       "0         4       3         2  \n",
       "1         3       3         1  \n",
       "2         5       4         4  \n",
       "3         4       3         2  \n",
       "4         4       4         4  \n",
       "..      ...     ...       ...  \n",
       "409       2       2         2  \n",
       "410       2       2         2  \n",
       "411       3       3         2  \n",
       "412       2       2         4  \n",
       "413       2       4         2  \n",
       "\n",
       "[414 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "03ec9e6e-eeaa-43e1-b460-c187d4cd68a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([np.uint8(4), np.uint8(3), np.uint8(5), np.uint8(1), np.uint8(2)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"Valence\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65b64b1-801d-49ed-9a87-0798c18b4d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>video</th>\n",
       "      <th>AF3_raw</th>\n",
       "      <th>F7_raw</th>\n",
       "      <th>F3_raw</th>\n",
       "      <th>FC5_raw</th>\n",
       "      <th>T7_raw</th>\n",
       "      <th>P7_raw</th>\n",
       "      <th>O1_raw</th>\n",
       "      <th>O2_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>AF4_raw</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Video</th>\n",
       "      <th>Video_Name</th>\n",
       "      <th>Target_Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-10.349212474280488, 0.9320963567671932, 4.52...</td>\n",
       "      <td>[-7.336422979280564, -2.7212738534012937, 0.35...</td>\n",
       "      <td>[-6.357392050340919, -46.86718793292361, -64.3...</td>\n",
       "      <td>[27.834342803367694, -39.340612753736416, -63....</td>\n",
       "      <td>[22.051642042275102, -40.510159638368734, -71....</td>\n",
       "      <td>[-25.750224861795058, -41.64657512470267, -48....</td>\n",
       "      <td>[0.3891795592307721, -50.89300581509636, -65.7...</td>\n",
       "      <td>[43.423268402468864, -42.7273519601261, -81.69...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.5554431681218347, -74.90768364976493, -108...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Searching for Bobby Fischer</td>\n",
       "      <td>calmness</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.38710522523616814, 7.818181696969973, 5.25...</td>\n",
       "      <td>[-0.9492062185599682, 8.281459209581069, 2.640...</td>\n",
       "      <td>[-29.84689530067401, -43.18212359451074, -35.4...</td>\n",
       "      <td>[-2.218007780462737, -29.91257528703265, -13.5...</td>\n",
       "      <td>[-29.2927823170884, -56.472712994591255, -43.6...</td>\n",
       "      <td>[-1.1194001362567376, -3.1707960975111944, 0.4...</td>\n",
       "      <td>[-31.946807814430898, -43.741895514175624, -29...</td>\n",
       "      <td>[-55.545831695510955, -85.80257081294347, -68....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-85.11469615543234, -90.75595696563374, -65.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>D.O.A.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.740238271785735, -1.464670313963118, -7.105...</td>\n",
       "      <td>[3.413843732230308, -4.790622767795466, -10.94...</td>\n",
       "      <td>[-47.36015967778116, -42.74643128266957, -39.6...</td>\n",
       "      <td>[-51.68328934853619, -26.555439767001214, -9.1...</td>\n",
       "      <td>[-62.37528256815315, -45.452432926611465, -30....</td>\n",
       "      <td>[-24.54733880679436, -18.391668699835865, -11....</td>\n",
       "      <td>[-77.01211916934554, -62.137931068780205, -46....</td>\n",
       "      <td>[-70.50876618489838, -48.97123494887409, -36.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-117.74840854597883, -110.56750845050888, -11...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The Hangover</td>\n",
       "      <td>amusement</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-7.798221562777997, 4.509400087366941, 9.1247...</td>\n",
       "      <td>[-5.116608810783481, 4.626821600452269, 11.293...</td>\n",
       "      <td>[24.93125629049786, 11.084152372000009, 10.057...</td>\n",
       "      <td>[37.87436013579214, -10.845343361281941, -23.1...</td>\n",
       "      <td>[50.74481684256556, 2.538494081670473, -3.6165...</td>\n",
       "      <td>[9.49379155066421, 1.288462268119328, 2.313902...</td>\n",
       "      <td>[27.807237142070754, -3.4762036998813866, -3.4...</td>\n",
       "      <td>[77.15881665492797, -7.457717821908049, -38.22...</td>\n",
       "      <td>...</td>\n",
       "      <td>[34.00357410810434, -22.402803761126375, -33.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The Ring</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[-1.0540439758948354, 10.2278997652951, 7.6636...</td>\n",
       "      <td>[-2.913624451554164, 7.855432900298843, 3.7526...</td>\n",
       "      <td>[-77.78475320526675, -85.50147614873666, -73.7...</td>\n",
       "      <td>[-7.365409600031402, -40.69691126850421, -29.4...</td>\n",
       "      <td>[8.953908321623967, -76.68765838967207, -62.84...</td>\n",
       "      <td>[-7.614922043039722, -9.66652009112654, -4.025...</td>\n",
       "      <td>[-6.910755146870472, -41.27047808440868, -24.8...</td>\n",
       "      <td>[-30.26654058201691, -98.98452880038919, -47.7...</td>\n",
       "      <td>...</td>\n",
       "      <td>[13.143391845043018, -77.11309307936695, -52.4...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>excitement</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>[-31.704198905027933, -32.72951525295624, -28....</td>\n",
       "      <td>[-15.555043379043072, -17.605404769358245, -20...</td>\n",
       "      <td>[-18.61622651269403, -13.999609198624913, -15....</td>\n",
       "      <td>[-14.698681349777496, -13.672134060956148, -14...</td>\n",
       "      <td>[-23.30060275410491, -22.78753654145279, -17.6...</td>\n",
       "      <td>[-8.816268873573902, -6.252239101327289, -8.81...</td>\n",
       "      <td>[279.79109403670617, 285.42943133898524, 287.4...</td>\n",
       "      <td>[-7.173135811725212, -1.0185719947610834, 2.05...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-37.972611579298515, -38.48291150179311, -40....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>Gentlemans Agreement</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>[5.466920035649326, 2.9008550036569596, -1.203...</td>\n",
       "      <td>[19.824976691430564, 16.74719391801507, 16.233...</td>\n",
       "      <td>[22.984495698257362, 19.393947142874296, 18.36...</td>\n",
       "      <td>[10.522773668263424, 11.035265103820846, 11.03...</td>\n",
       "      <td>[21.95946583143874, 24.009312500894033, 28.110...</td>\n",
       "      <td>[21.238268640320484, 23.80170581178004, 21.236...</td>\n",
       "      <td>[20.594802762840782, 20.593162081299607, 20.59...</td>\n",
       "      <td>[38.0866367831533, 34.49652294398144, 32.44487...</td>\n",
       "      <td>...</td>\n",
       "      <td>[18.966287091329455, 18.45239653087682, 15.374...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>Psycho</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>[-23.62545386161945, -23.62685140441953, -20.0...</td>\n",
       "      <td>[-13.207442040493266, -10.132756016797817, -12...</td>\n",
       "      <td>[5.919693285935381, 5.404129601617996, 6.93984...</td>\n",
       "      <td>[2.4862210644104663, 4.021971582134875, 9.6602...</td>\n",
       "      <td>[-1.9057467428498671, -3.960635285737833, -15....</td>\n",
       "      <td>[4.3931845090082255, 4.390480711103866, 2.3364...</td>\n",
       "      <td>[-2.3892454894785056, -4.442585452677101, -7.0...</td>\n",
       "      <td>[19.635451596486917, 20.658453846575583, 22.70...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-18.03784008673325, -18.03905038031304, -18.5...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>The Bourne Identitiy</td>\n",
       "      <td>excitement</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>[-6.264400004813267, -5.239062339847094, -4.72...</td>\n",
       "      <td>[5.878397163971902, -6.430141588554244, -11.04...</td>\n",
       "      <td>[6.265703153080619, 5.7513450467094, 0.1087818...</td>\n",
       "      <td>[10.77405565695624, 11.28418632089431, 10.2558...</td>\n",
       "      <td>[-8.565089727038279, 7.844667256426551, 16.049...</td>\n",
       "      <td>[1.4333426212944123, 6.0485652422077125, 9.125...</td>\n",
       "      <td>[-36.47451244649021, -37.505569346830654, -39....</td>\n",
       "      <td>[-16.793433465247425, -12.178954818932333, -11...</td>\n",
       "      <td>...</td>\n",
       "      <td>[4.8526302066835605, 7.928167153135863, 4.3370...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>[14.369879486735218, 10.779547738589091, 8.214...</td>\n",
       "      <td>[28.6391787768303, 29.66291739078911, 24.01998...</td>\n",
       "      <td>[-1.66603287039948, -4.744893004886084, -2.695...</td>\n",
       "      <td>[16.369912695275517, 15.856474606888142, 17.90...</td>\n",
       "      <td>[12.222244946599554, 1.9648341686567319, 1.451...</td>\n",
       "      <td>[20.29831967575734, 20.297050489867217, 18.244...</td>\n",
       "      <td>[19.887520662319414, 14.24664059415195, 13.733...</td>\n",
       "      <td>[23.18039318144982, 20.102418744557855, 15.998...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.8823006451415075, -3.4484144085066455, -4....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>The Departed</td>\n",
       "      <td>surprise</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant video                                            AF3_raw  \\\n",
       "0             1     1  [-10.349212474280488, 0.9320963567671932, 4.52...   \n",
       "1             1     2  [-0.38710522523616814, 7.818181696969973, 5.25...   \n",
       "2             1     3  [6.740238271785735, -1.464670313963118, -7.105...   \n",
       "3             1     4  [-7.798221562777997, 4.509400087366941, 9.1247...   \n",
       "4             1     5  [-1.0540439758948354, 10.2278997652951, 7.6636...   \n",
       "..          ...   ...                                                ...   \n",
       "409          23    14  [-31.704198905027933, -32.72951525295624, -28....   \n",
       "410          23    15  [5.466920035649326, 2.9008550036569596, -1.203...   \n",
       "411          23    16  [-23.62545386161945, -23.62685140441953, -20.0...   \n",
       "412          23    17  [-6.264400004813267, -5.239062339847094, -4.72...   \n",
       "413          23    18  [14.369879486735218, 10.779547738589091, 8.214...   \n",
       "\n",
       "                                                F7_raw  \\\n",
       "0    [-7.336422979280564, -2.7212738534012937, 0.35...   \n",
       "1    [-0.9492062185599682, 8.281459209581069, 2.640...   \n",
       "2    [3.413843732230308, -4.790622767795466, -10.94...   \n",
       "3    [-5.116608810783481, 4.626821600452269, 11.293...   \n",
       "4    [-2.913624451554164, 7.855432900298843, 3.7526...   \n",
       "..                                                 ...   \n",
       "409  [-15.555043379043072, -17.605404769358245, -20...   \n",
       "410  [19.824976691430564, 16.74719391801507, 16.233...   \n",
       "411  [-13.207442040493266, -10.132756016797817, -12...   \n",
       "412  [5.878397163971902, -6.430141588554244, -11.04...   \n",
       "413  [28.6391787768303, 29.66291739078911, 24.01998...   \n",
       "\n",
       "                                                F3_raw  \\\n",
       "0    [-6.357392050340919, -46.86718793292361, -64.3...   \n",
       "1    [-29.84689530067401, -43.18212359451074, -35.4...   \n",
       "2    [-47.36015967778116, -42.74643128266957, -39.6...   \n",
       "3    [24.93125629049786, 11.084152372000009, 10.057...   \n",
       "4    [-77.78475320526675, -85.50147614873666, -73.7...   \n",
       "..                                                 ...   \n",
       "409  [-18.61622651269403, -13.999609198624913, -15....   \n",
       "410  [22.984495698257362, 19.393947142874296, 18.36...   \n",
       "411  [5.919693285935381, 5.404129601617996, 6.93984...   \n",
       "412  [6.265703153080619, 5.7513450467094, 0.1087818...   \n",
       "413  [-1.66603287039948, -4.744893004886084, -2.695...   \n",
       "\n",
       "                                               FC5_raw  \\\n",
       "0    [27.834342803367694, -39.340612753736416, -63....   \n",
       "1    [-2.218007780462737, -29.91257528703265, -13.5...   \n",
       "2    [-51.68328934853619, -26.555439767001214, -9.1...   \n",
       "3    [37.87436013579214, -10.845343361281941, -23.1...   \n",
       "4    [-7.365409600031402, -40.69691126850421, -29.4...   \n",
       "..                                                 ...   \n",
       "409  [-14.698681349777496, -13.672134060956148, -14...   \n",
       "410  [10.522773668263424, 11.035265103820846, 11.03...   \n",
       "411  [2.4862210644104663, 4.021971582134875, 9.6602...   \n",
       "412  [10.77405565695624, 11.28418632089431, 10.2558...   \n",
       "413  [16.369912695275517, 15.856474606888142, 17.90...   \n",
       "\n",
       "                                                T7_raw  \\\n",
       "0    [22.051642042275102, -40.510159638368734, -71....   \n",
       "1    [-29.2927823170884, -56.472712994591255, -43.6...   \n",
       "2    [-62.37528256815315, -45.452432926611465, -30....   \n",
       "3    [50.74481684256556, 2.538494081670473, -3.6165...   \n",
       "4    [8.953908321623967, -76.68765838967207, -62.84...   \n",
       "..                                                 ...   \n",
       "409  [-23.30060275410491, -22.78753654145279, -17.6...   \n",
       "410  [21.95946583143874, 24.009312500894033, 28.110...   \n",
       "411  [-1.9057467428498671, -3.960635285737833, -15....   \n",
       "412  [-8.565089727038279, 7.844667256426551, 16.049...   \n",
       "413  [12.222244946599554, 1.9648341686567319, 1.451...   \n",
       "\n",
       "                                                P7_raw  \\\n",
       "0    [-25.750224861795058, -41.64657512470267, -48....   \n",
       "1    [-1.1194001362567376, -3.1707960975111944, 0.4...   \n",
       "2    [-24.54733880679436, -18.391668699835865, -11....   \n",
       "3    [9.49379155066421, 1.288462268119328, 2.313902...   \n",
       "4    [-7.614922043039722, -9.66652009112654, -4.025...   \n",
       "..                                                 ...   \n",
       "409  [-8.816268873573902, -6.252239101327289, -8.81...   \n",
       "410  [21.238268640320484, 23.80170581178004, 21.236...   \n",
       "411  [4.3931845090082255, 4.390480711103866, 2.3364...   \n",
       "412  [1.4333426212944123, 6.0485652422077125, 9.125...   \n",
       "413  [20.29831967575734, 20.297050489867217, 18.244...   \n",
       "\n",
       "                                                O1_raw  \\\n",
       "0    [0.3891795592307721, -50.89300581509636, -65.7...   \n",
       "1    [-31.946807814430898, -43.741895514175624, -29...   \n",
       "2    [-77.01211916934554, -62.137931068780205, -46....   \n",
       "3    [27.807237142070754, -3.4762036998813866, -3.4...   \n",
       "4    [-6.910755146870472, -41.27047808440868, -24.8...   \n",
       "..                                                 ...   \n",
       "409  [279.79109403670617, 285.42943133898524, 287.4...   \n",
       "410  [20.594802762840782, 20.593162081299607, 20.59...   \n",
       "411  [-2.3892454894785056, -4.442585452677101, -7.0...   \n",
       "412  [-36.47451244649021, -37.505569346830654, -39....   \n",
       "413  [19.887520662319414, 14.24664059415195, 13.733...   \n",
       "\n",
       "                                                O2_raw  ...  \\\n",
       "0    [43.423268402468864, -42.7273519601261, -81.69...  ...   \n",
       "1    [-55.545831695510955, -85.80257081294347, -68....  ...   \n",
       "2    [-70.50876618489838, -48.97123494887409, -36.6...  ...   \n",
       "3    [77.15881665492797, -7.457717821908049, -38.22...  ...   \n",
       "4    [-30.26654058201691, -98.98452880038919, -47.7...  ...   \n",
       "..                                                 ...  ...   \n",
       "409  [-7.173135811725212, -1.0185719947610834, 2.05...  ...   \n",
       "410  [38.0866367831533, 34.49652294398144, 32.44487...  ...   \n",
       "411  [19.635451596486917, 20.658453846575583, 22.70...  ...   \n",
       "412  [-16.793433465247425, -12.178954818932333, -11...  ...   \n",
       "413  [23.18039318144982, 20.102418744557855, 15.998...  ...   \n",
       "\n",
       "                                               AF4_raw Age Gender Participant  \\\n",
       "0    [-0.5554431681218347, -74.90768364976493, -108...  22   male           1   \n",
       "1    [-85.11469615543234, -90.75595696563374, -65.6...  22   male           1   \n",
       "2    [-117.74840854597883, -110.56750845050888, -11...  22   male           1   \n",
       "3    [34.00357410810434, -22.402803761126375, -33.6...  22   male           1   \n",
       "4    [13.143391845043018, -77.11309307936695, -52.4...  22   male           1   \n",
       "..                                                 ...  ..    ...         ...   \n",
       "409  [-37.972611579298515, -38.48291150179311, -40....  25   male          23   \n",
       "410  [18.966287091329455, 18.45239653087682, 15.374...  25   male          23   \n",
       "411  [-18.03784008673325, -18.03905038031304, -18.5...  25   male          23   \n",
       "412  [4.8526302066835605, 7.928167153135863, 4.3370...  25   male          23   \n",
       "413  [-0.8823006451415075, -3.4484144085066455, -4....  25   male          23   \n",
       "\n",
       "    Video                   Video_Name Target_Emotion Valence Arousal  \\\n",
       "0       1  Searching for Bobby Fischer       calmness       4       3   \n",
       "1       2                       D.O.A.       surprise       3       3   \n",
       "2       3                 The Hangover      amusement       5       4   \n",
       "3       4                     The Ring           fear       4       3   \n",
       "4       5                          300     excitement       4       4   \n",
       "..    ...                          ...            ...     ...     ...   \n",
       "409    14         Gentlemans Agreement          anger       2       2   \n",
       "410    15                       Psycho           fear       2       2   \n",
       "411    16         The Bourne Identitiy     excitement       3       3   \n",
       "412    17     The Shawshank Redemption        sadness       2       2   \n",
       "413    18                 The Departed       surprise       2       4   \n",
       "\n",
       "    Dominance  \n",
       "0           2  \n",
       "1           1  \n",
       "2           4  \n",
       "3           2  \n",
       "4           4  \n",
       "..        ...  \n",
       "409         2  \n",
       "410         2  \n",
       "411         2  \n",
       "412         4  \n",
       "413         2  \n",
       "\n",
       "[414 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the feature DataFrame with metadata DataFrame\n",
    "final_df = pd.merge(df, b, left_on=['participant', 'video'], right_on=['Participant', 'Video'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0bc2d8a-654c-4863-a721-9d2904948a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>video</th>\n",
       "      <th>AF3_raw</th>\n",
       "      <th>F7_raw</th>\n",
       "      <th>F3_raw</th>\n",
       "      <th>FC5_raw</th>\n",
       "      <th>T7_raw</th>\n",
       "      <th>P7_raw</th>\n",
       "      <th>O1_raw</th>\n",
       "      <th>O2_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>F4_raw</th>\n",
       "      <th>F8_raw</th>\n",
       "      <th>AF4_raw</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Video_Name</th>\n",
       "      <th>Target_Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-10.349212474280488, 0.9320963567671932, 4.52...</td>\n",
       "      <td>[-7.336422979280564, -2.7212738534012937, 0.35...</td>\n",
       "      <td>[-6.357392050340919, -46.86718793292361, -64.3...</td>\n",
       "      <td>[27.834342803367694, -39.340612753736416, -63....</td>\n",
       "      <td>[22.051642042275102, -40.510159638368734, -71....</td>\n",
       "      <td>[-25.750224861795058, -41.64657512470267, -48....</td>\n",
       "      <td>[0.3891795592307721, -50.89300581509636, -65.7...</td>\n",
       "      <td>[43.423268402468864, -42.7273519601261, -81.69...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-10.364532668578148, -26.259576176464076, -32...</td>\n",
       "      <td>[-2.7922149552880873, -51.50648426600064, -69....</td>\n",
       "      <td>[-0.5554431681218347, -74.90768364976493, -108...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>Searching for Bobby Fischer</td>\n",
       "      <td>calmness</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.38710522523616814, 7.818181696969973, 5.25...</td>\n",
       "      <td>[-0.9492062185599682, 8.281459209581069, 2.640...</td>\n",
       "      <td>[-29.84689530067401, -43.18212359451074, -35.4...</td>\n",
       "      <td>[-2.218007780462737, -29.91257528703265, -13.5...</td>\n",
       "      <td>[-29.2927823170884, -56.472712994591255, -43.6...</td>\n",
       "      <td>[-1.1194001362567376, -3.1707960975111944, 0.4...</td>\n",
       "      <td>[-31.946807814430898, -43.741895514175624, -29...</td>\n",
       "      <td>[-55.545831695510955, -85.80257081294347, -68....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-17.208695619364846, -16.19140531780937, -11....</td>\n",
       "      <td>[-122.12733116987303, -138.02650317995787, -12...</td>\n",
       "      <td>[-85.11469615543234, -90.75595696563374, -65.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>D.O.A.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.740238271785735, -1.464670313963118, -7.105...</td>\n",
       "      <td>[3.413843732230308, -4.790622767795466, -10.94...</td>\n",
       "      <td>[-47.36015967778116, -42.74643128266957, -39.6...</td>\n",
       "      <td>[-51.68328934853619, -26.555439767001214, -9.1...</td>\n",
       "      <td>[-62.37528256815315, -45.452432926611465, -30....</td>\n",
       "      <td>[-24.54733880679436, -18.391668699835865, -11....</td>\n",
       "      <td>[-77.01211916934554, -62.137931068780205, -46....</td>\n",
       "      <td>[-70.50876618489838, -48.97123494887409, -36.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-21.93647870702945, -27.57728772842303, -25.5...</td>\n",
       "      <td>[-92.89575464126565, -81.61575420193722, -68.7...</td>\n",
       "      <td>[-117.74840854597883, -110.56750845050888, -11...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>The Hangover</td>\n",
       "      <td>amusement</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-7.798221562777997, 4.509400087366941, 9.1247...</td>\n",
       "      <td>[-5.116608810783481, 4.626821600452269, 11.293...</td>\n",
       "      <td>[24.93125629049786, 11.084152372000009, 10.057...</td>\n",
       "      <td>[37.87436013579214, -10.845343361281941, -23.1...</td>\n",
       "      <td>[50.74481684256556, 2.538494081670473, -3.6165...</td>\n",
       "      <td>[9.49379155066421, 1.288462268119328, 2.313902...</td>\n",
       "      <td>[27.807237142070754, -3.4762036998813866, -3.4...</td>\n",
       "      <td>[77.15881665492797, -7.457717821908049, -38.22...</td>\n",
       "      <td>...</td>\n",
       "      <td>[5.750397059373909, 2.6719938533138072, 4.2089...</td>\n",
       "      <td>[44.528006638508316, 13.75661411855691, 10.164...</td>\n",
       "      <td>[34.00357410810434, -22.402803761126375, -33.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>The Ring</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[-1.0540439758948354, 10.2278997652951, 7.6636...</td>\n",
       "      <td>[-2.913624451554164, 7.855432900298843, 3.7526...</td>\n",
       "      <td>[-77.78475320526675, -85.50147614873666, -73.7...</td>\n",
       "      <td>[-7.365409600031402, -40.69691126850421, -29.4...</td>\n",
       "      <td>[8.953908321623967, -76.68765838967207, -62.84...</td>\n",
       "      <td>[-7.614922043039722, -9.66652009112654, -4.025...</td>\n",
       "      <td>[-6.910755146870472, -41.27047808440868, -24.8...</td>\n",
       "      <td>[-30.26654058201691, -98.98452880038919, -47.7...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-22.956039179678584, -22.95605209016051, -15....</td>\n",
       "      <td>[5.610392693926274, -68.23502592613708, -65.67...</td>\n",
       "      <td>[13.143391845043018, -77.11309307936695, -52.4...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>300</td>\n",
       "      <td>excitement</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>[-31.704198905027933, -32.72951525295624, -28....</td>\n",
       "      <td>[-15.555043379043072, -17.605404769358245, -20...</td>\n",
       "      <td>[-18.61622651269403, -13.999609198624913, -15....</td>\n",
       "      <td>[-14.698681349777496, -13.672134060956148, -14...</td>\n",
       "      <td>[-23.30060275410491, -22.78753654145279, -17.6...</td>\n",
       "      <td>[-8.816268873573902, -6.252239101327289, -8.81...</td>\n",
       "      <td>[279.79109403670617, 285.42943133898524, 287.4...</td>\n",
       "      <td>[-7.173135811725212, -1.0185719947610834, 2.05...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1240.0690002671954, -1196.9337449674017, -93...</td>\n",
       "      <td>[-15.402448839378042, -15.401729520758739, -17...</td>\n",
       "      <td>[-37.972611579298515, -38.48291150179311, -40....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>Gentlemans Agreement</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>[5.466920035649326, 2.9008550036569596, -1.203...</td>\n",
       "      <td>[19.824976691430564, 16.74719391801507, 16.233...</td>\n",
       "      <td>[22.984495698257362, 19.393947142874296, 18.36...</td>\n",
       "      <td>[10.522773668263424, 11.035265103820846, 11.03...</td>\n",
       "      <td>[21.95946583143874, 24.009312500894033, 28.110...</td>\n",
       "      <td>[21.238268640320484, 23.80170581178004, 21.236...</td>\n",
       "      <td>[20.594802762840782, 20.593162081299607, 20.59...</td>\n",
       "      <td>[38.0866367831533, 34.49652294398144, 32.44487...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-256.4677886910428, -450.34520134403937, -462...</td>\n",
       "      <td>[21.75392584043096, 20.215181211643205, 16.112...</td>\n",
       "      <td>[18.966287091329455, 18.45239653087682, 15.374...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>Psycho</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>[-23.62545386161945, -23.62685140441953, -20.0...</td>\n",
       "      <td>[-13.207442040493266, -10.132756016797817, -12...</td>\n",
       "      <td>[5.919693285935381, 5.404129601617996, 6.93984...</td>\n",
       "      <td>[2.4862210644104663, 4.021971582134875, 9.6602...</td>\n",
       "      <td>[-1.9057467428498671, -3.960635285737833, -15....</td>\n",
       "      <td>[4.3931845090082255, 4.390480711103866, 2.3364...</td>\n",
       "      <td>[-2.3892454894785056, -4.442585452677101, -7.0...</td>\n",
       "      <td>[19.635451596486917, 20.658453846575583, 22.70...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-82.28162249200449, -125.86801543132987, -127...</td>\n",
       "      <td>[-9.257969718958025, -6.69583084101619, -3.620...</td>\n",
       "      <td>[-18.03784008673325, -18.03905038031304, -18.5...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Bourne Identitiy</td>\n",
       "      <td>excitement</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>[-6.264400004813267, -5.239062339847094, -4.72...</td>\n",
       "      <td>[5.878397163971902, -6.430141588554244, -11.04...</td>\n",
       "      <td>[6.265703153080619, 5.7513450467094, 0.1087818...</td>\n",
       "      <td>[10.77405565695624, 11.28418632089431, 10.2558...</td>\n",
       "      <td>[-8.565089727038279, 7.844667256426551, 16.049...</td>\n",
       "      <td>[1.4333426212944123, 6.0485652422077125, 9.125...</td>\n",
       "      <td>[-36.47451244649021, -37.505569346830654, -39....</td>\n",
       "      <td>[-16.793433465247425, -12.178954818932333, -11...</td>\n",
       "      <td>...</td>\n",
       "      <td>[11.40135085381238, -8.08989651508042, -39.888...</td>\n",
       "      <td>[-9.197435699224982, 1.0585177042776617, 4.134...</td>\n",
       "      <td>[4.8526302066835605, 7.928167153135863, 4.3370...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>[14.369879486735218, 10.779547738589091, 8.214...</td>\n",
       "      <td>[28.6391787768303, 29.66291739078911, 24.01998...</td>\n",
       "      <td>[-1.66603287039948, -4.744893004886084, -2.695...</td>\n",
       "      <td>[16.369912695275517, 15.856474606888142, 17.90...</td>\n",
       "      <td>[12.222244946599554, 1.9648341686567319, 1.451...</td>\n",
       "      <td>[20.29831967575734, 20.297050489867217, 18.244...</td>\n",
       "      <td>[19.887520662319414, 14.24664059415195, 13.733...</td>\n",
       "      <td>[23.18039318144982, 20.102418744557855, 15.998...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-57.52073108233021, -58.03694740968917, -66.7...</td>\n",
       "      <td>[-42.15346714183702, -45.2312856859056, -48.30...</td>\n",
       "      <td>[-0.8823006451415075, -3.4484144085066455, -4....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Departed</td>\n",
       "      <td>surprise</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant video                                            AF3_raw  \\\n",
       "0             1     1  [-10.349212474280488, 0.9320963567671932, 4.52...   \n",
       "1             1     2  [-0.38710522523616814, 7.818181696969973, 5.25...   \n",
       "2             1     3  [6.740238271785735, -1.464670313963118, -7.105...   \n",
       "3             1     4  [-7.798221562777997, 4.509400087366941, 9.1247...   \n",
       "4             1     5  [-1.0540439758948354, 10.2278997652951, 7.6636...   \n",
       "..          ...   ...                                                ...   \n",
       "409          23    14  [-31.704198905027933, -32.72951525295624, -28....   \n",
       "410          23    15  [5.466920035649326, 2.9008550036569596, -1.203...   \n",
       "411          23    16  [-23.62545386161945, -23.62685140441953, -20.0...   \n",
       "412          23    17  [-6.264400004813267, -5.239062339847094, -4.72...   \n",
       "413          23    18  [14.369879486735218, 10.779547738589091, 8.214...   \n",
       "\n",
       "                                                F7_raw  \\\n",
       "0    [-7.336422979280564, -2.7212738534012937, 0.35...   \n",
       "1    [-0.9492062185599682, 8.281459209581069, 2.640...   \n",
       "2    [3.413843732230308, -4.790622767795466, -10.94...   \n",
       "3    [-5.116608810783481, 4.626821600452269, 11.293...   \n",
       "4    [-2.913624451554164, 7.855432900298843, 3.7526...   \n",
       "..                                                 ...   \n",
       "409  [-15.555043379043072, -17.605404769358245, -20...   \n",
       "410  [19.824976691430564, 16.74719391801507, 16.233...   \n",
       "411  [-13.207442040493266, -10.132756016797817, -12...   \n",
       "412  [5.878397163971902, -6.430141588554244, -11.04...   \n",
       "413  [28.6391787768303, 29.66291739078911, 24.01998...   \n",
       "\n",
       "                                                F3_raw  \\\n",
       "0    [-6.357392050340919, -46.86718793292361, -64.3...   \n",
       "1    [-29.84689530067401, -43.18212359451074, -35.4...   \n",
       "2    [-47.36015967778116, -42.74643128266957, -39.6...   \n",
       "3    [24.93125629049786, 11.084152372000009, 10.057...   \n",
       "4    [-77.78475320526675, -85.50147614873666, -73.7...   \n",
       "..                                                 ...   \n",
       "409  [-18.61622651269403, -13.999609198624913, -15....   \n",
       "410  [22.984495698257362, 19.393947142874296, 18.36...   \n",
       "411  [5.919693285935381, 5.404129601617996, 6.93984...   \n",
       "412  [6.265703153080619, 5.7513450467094, 0.1087818...   \n",
       "413  [-1.66603287039948, -4.744893004886084, -2.695...   \n",
       "\n",
       "                                               FC5_raw  \\\n",
       "0    [27.834342803367694, -39.340612753736416, -63....   \n",
       "1    [-2.218007780462737, -29.91257528703265, -13.5...   \n",
       "2    [-51.68328934853619, -26.555439767001214, -9.1...   \n",
       "3    [37.87436013579214, -10.845343361281941, -23.1...   \n",
       "4    [-7.365409600031402, -40.69691126850421, -29.4...   \n",
       "..                                                 ...   \n",
       "409  [-14.698681349777496, -13.672134060956148, -14...   \n",
       "410  [10.522773668263424, 11.035265103820846, 11.03...   \n",
       "411  [2.4862210644104663, 4.021971582134875, 9.6602...   \n",
       "412  [10.77405565695624, 11.28418632089431, 10.2558...   \n",
       "413  [16.369912695275517, 15.856474606888142, 17.90...   \n",
       "\n",
       "                                                T7_raw  \\\n",
       "0    [22.051642042275102, -40.510159638368734, -71....   \n",
       "1    [-29.2927823170884, -56.472712994591255, -43.6...   \n",
       "2    [-62.37528256815315, -45.452432926611465, -30....   \n",
       "3    [50.74481684256556, 2.538494081670473, -3.6165...   \n",
       "4    [8.953908321623967, -76.68765838967207, -62.84...   \n",
       "..                                                 ...   \n",
       "409  [-23.30060275410491, -22.78753654145279, -17.6...   \n",
       "410  [21.95946583143874, 24.009312500894033, 28.110...   \n",
       "411  [-1.9057467428498671, -3.960635285737833, -15....   \n",
       "412  [-8.565089727038279, 7.844667256426551, 16.049...   \n",
       "413  [12.222244946599554, 1.9648341686567319, 1.451...   \n",
       "\n",
       "                                                P7_raw  \\\n",
       "0    [-25.750224861795058, -41.64657512470267, -48....   \n",
       "1    [-1.1194001362567376, -3.1707960975111944, 0.4...   \n",
       "2    [-24.54733880679436, -18.391668699835865, -11....   \n",
       "3    [9.49379155066421, 1.288462268119328, 2.313902...   \n",
       "4    [-7.614922043039722, -9.66652009112654, -4.025...   \n",
       "..                                                 ...   \n",
       "409  [-8.816268873573902, -6.252239101327289, -8.81...   \n",
       "410  [21.238268640320484, 23.80170581178004, 21.236...   \n",
       "411  [4.3931845090082255, 4.390480711103866, 2.3364...   \n",
       "412  [1.4333426212944123, 6.0485652422077125, 9.125...   \n",
       "413  [20.29831967575734, 20.297050489867217, 18.244...   \n",
       "\n",
       "                                                O1_raw  \\\n",
       "0    [0.3891795592307721, -50.89300581509636, -65.7...   \n",
       "1    [-31.946807814430898, -43.741895514175624, -29...   \n",
       "2    [-77.01211916934554, -62.137931068780205, -46....   \n",
       "3    [27.807237142070754, -3.4762036998813866, -3.4...   \n",
       "4    [-6.910755146870472, -41.27047808440868, -24.8...   \n",
       "..                                                 ...   \n",
       "409  [279.79109403670617, 285.42943133898524, 287.4...   \n",
       "410  [20.594802762840782, 20.593162081299607, 20.59...   \n",
       "411  [-2.3892454894785056, -4.442585452677101, -7.0...   \n",
       "412  [-36.47451244649021, -37.505569346830654, -39....   \n",
       "413  [19.887520662319414, 14.24664059415195, 13.733...   \n",
       "\n",
       "                                                O2_raw  ...  \\\n",
       "0    [43.423268402468864, -42.7273519601261, -81.69...  ...   \n",
       "1    [-55.545831695510955, -85.80257081294347, -68....  ...   \n",
       "2    [-70.50876618489838, -48.97123494887409, -36.6...  ...   \n",
       "3    [77.15881665492797, -7.457717821908049, -38.22...  ...   \n",
       "4    [-30.26654058201691, -98.98452880038919, -47.7...  ...   \n",
       "..                                                 ...  ...   \n",
       "409  [-7.173135811725212, -1.0185719947610834, 2.05...  ...   \n",
       "410  [38.0866367831533, 34.49652294398144, 32.44487...  ...   \n",
       "411  [19.635451596486917, 20.658453846575583, 22.70...  ...   \n",
       "412  [-16.793433465247425, -12.178954818932333, -11...  ...   \n",
       "413  [23.18039318144982, 20.102418744557855, 15.998...  ...   \n",
       "\n",
       "                                                F4_raw  \\\n",
       "0    [-10.364532668578148, -26.259576176464076, -32...   \n",
       "1    [-17.208695619364846, -16.19140531780937, -11....   \n",
       "2    [-21.93647870702945, -27.57728772842303, -25.5...   \n",
       "3    [5.750397059373909, 2.6719938533138072, 4.2089...   \n",
       "4    [-22.956039179678584, -22.95605209016051, -15....   \n",
       "..                                                 ...   \n",
       "409  [-1240.0690002671954, -1196.9337449674017, -93...   \n",
       "410  [-256.4677886910428, -450.34520134403937, -462...   \n",
       "411  [-82.28162249200449, -125.86801543132987, -127...   \n",
       "412  [11.40135085381238, -8.08989651508042, -39.888...   \n",
       "413  [-57.52073108233021, -58.03694740968917, -66.7...   \n",
       "\n",
       "                                                F8_raw  \\\n",
       "0    [-2.7922149552880873, -51.50648426600064, -69....   \n",
       "1    [-122.12733116987303, -138.02650317995787, -12...   \n",
       "2    [-92.89575464126565, -81.61575420193722, -68.7...   \n",
       "3    [44.528006638508316, 13.75661411855691, 10.164...   \n",
       "4    [5.610392693926274, -68.23502592613708, -65.67...   \n",
       "..                                                 ...   \n",
       "409  [-15.402448839378042, -15.401729520758739, -17...   \n",
       "410  [21.75392584043096, 20.215181211643205, 16.112...   \n",
       "411  [-9.257969718958025, -6.69583084101619, -3.620...   \n",
       "412  [-9.197435699224982, 1.0585177042776617, 4.134...   \n",
       "413  [-42.15346714183702, -45.2312856859056, -48.30...   \n",
       "\n",
       "                                               AF4_raw Age Gender  \\\n",
       "0    [-0.5554431681218347, -74.90768364976493, -108...  22   male   \n",
       "1    [-85.11469615543234, -90.75595696563374, -65.6...  22   male   \n",
       "2    [-117.74840854597883, -110.56750845050888, -11...  22   male   \n",
       "3    [34.00357410810434, -22.402803761126375, -33.6...  22   male   \n",
       "4    [13.143391845043018, -77.11309307936695, -52.4...  22   male   \n",
       "..                                                 ...  ..    ...   \n",
       "409  [-37.972611579298515, -38.48291150179311, -40....  25   male   \n",
       "410  [18.966287091329455, 18.45239653087682, 15.374...  25   male   \n",
       "411  [-18.03784008673325, -18.03905038031304, -18.5...  25   male   \n",
       "412  [4.8526302066835605, 7.928167153135863, 4.3370...  25   male   \n",
       "413  [-0.8823006451415075, -3.4484144085066455, -4....  25   male   \n",
       "\n",
       "                      Video_Name Target_Emotion Valence Arousal Dominance  \n",
       "0    Searching for Bobby Fischer       calmness       4       3         2  \n",
       "1                         D.O.A.       surprise       3       3         1  \n",
       "2                   The Hangover      amusement       5       4         4  \n",
       "3                       The Ring           fear       4       3         2  \n",
       "4                            300     excitement       4       4         4  \n",
       "..                           ...            ...     ...     ...       ...  \n",
       "409         Gentlemans Agreement          anger       2       2         2  \n",
       "410                       Psycho           fear       2       2         2  \n",
       "411         The Bourne Identitiy     excitement       3       3         2  \n",
       "412     The Shawshank Redemption        sadness       2       2         4  \n",
       "413                 The Departed       surprise       2       4         2  \n",
       "\n",
       "[414 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df.drop([\"Participant\", \"Video\"], axis=1)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74455d82-b44d-4beb-a928-a19df3db29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Extracted_Features/Final_Raw_Data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(final_df, f, pickle.HIGHEST_PROTOCOL)\n",
    "# The following example reads the resulting pickled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc2dcd4-5e68-4f4d-a61b-0345d1279de7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Extracted_Features/Final_Raw_Data.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# The protocol version used is detected automatically, so we do not\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# have to specify it.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "with open('../Extracted_Features/Final_Raw_Data.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6228e2-359e-4a99-b2c8-a2084a8a6b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>video</th>\n",
       "      <th>AF3_raw</th>\n",
       "      <th>F7_raw</th>\n",
       "      <th>F3_raw</th>\n",
       "      <th>FC5_raw</th>\n",
       "      <th>T7_raw</th>\n",
       "      <th>P7_raw</th>\n",
       "      <th>O1_raw</th>\n",
       "      <th>O2_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>F4_raw</th>\n",
       "      <th>F8_raw</th>\n",
       "      <th>AF4_raw</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Video_Name</th>\n",
       "      <th>Target_Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-10.349212474280488, 0.9320963567671932, 4.52...</td>\n",
       "      <td>[-7.336422979280564, -2.7212738534012937, 0.35...</td>\n",
       "      <td>[-6.357392050340919, -46.86718793292361, -64.3...</td>\n",
       "      <td>[27.834342803367694, -39.340612753736416, -63....</td>\n",
       "      <td>[22.051642042275102, -40.510159638368734, -71....</td>\n",
       "      <td>[-25.750224861795058, -41.64657512470267, -48....</td>\n",
       "      <td>[0.3891795592307721, -50.89300581509636, -65.7...</td>\n",
       "      <td>[43.423268402468864, -42.7273519601261, -81.69...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-10.364532668578148, -26.259576176464076, -32...</td>\n",
       "      <td>[-2.7922149552880873, -51.50648426600064, -69....</td>\n",
       "      <td>[-0.5554431681218347, -74.90768364976493, -108...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>Searching for Bobby Fischer</td>\n",
       "      <td>calmness</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.38710522523616814, 7.818181696969973, 5.25...</td>\n",
       "      <td>[-0.9492062185599682, 8.281459209581069, 2.640...</td>\n",
       "      <td>[-29.84689530067401, -43.18212359451074, -35.4...</td>\n",
       "      <td>[-2.218007780462737, -29.91257528703265, -13.5...</td>\n",
       "      <td>[-29.2927823170884, -56.472712994591255, -43.6...</td>\n",
       "      <td>[-1.1194001362567376, -3.1707960975111944, 0.4...</td>\n",
       "      <td>[-31.946807814430898, -43.741895514175624, -29...</td>\n",
       "      <td>[-55.545831695510955, -85.80257081294347, -68....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-17.208695619364846, -16.19140531780937, -11....</td>\n",
       "      <td>[-122.12733116987303, -138.02650317995787, -12...</td>\n",
       "      <td>[-85.11469615543234, -90.75595696563374, -65.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>D.O.A.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.740238271785735, -1.464670313963118, -7.105...</td>\n",
       "      <td>[3.413843732230308, -4.790622767795466, -10.94...</td>\n",
       "      <td>[-47.36015967778116, -42.74643128266957, -39.6...</td>\n",
       "      <td>[-51.68328934853619, -26.555439767001214, -9.1...</td>\n",
       "      <td>[-62.37528256815315, -45.452432926611465, -30....</td>\n",
       "      <td>[-24.54733880679436, -18.391668699835865, -11....</td>\n",
       "      <td>[-77.01211916934554, -62.137931068780205, -46....</td>\n",
       "      <td>[-70.50876618489838, -48.97123494887409, -36.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-21.93647870702945, -27.57728772842303, -25.5...</td>\n",
       "      <td>[-92.89575464126565, -81.61575420193722, -68.7...</td>\n",
       "      <td>[-117.74840854597883, -110.56750845050888, -11...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>The Hangover</td>\n",
       "      <td>amusement</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-7.798221562777997, 4.509400087366941, 9.1247...</td>\n",
       "      <td>[-5.116608810783481, 4.626821600452269, 11.293...</td>\n",
       "      <td>[24.93125629049786, 11.084152372000009, 10.057...</td>\n",
       "      <td>[37.87436013579214, -10.845343361281941, -23.1...</td>\n",
       "      <td>[50.74481684256556, 2.538494081670473, -3.6165...</td>\n",
       "      <td>[9.49379155066421, 1.288462268119328, 2.313902...</td>\n",
       "      <td>[27.807237142070754, -3.4762036998813866, -3.4...</td>\n",
       "      <td>[77.15881665492797, -7.457717821908049, -38.22...</td>\n",
       "      <td>...</td>\n",
       "      <td>[5.750397059373909, 2.6719938533138072, 4.2089...</td>\n",
       "      <td>[44.528006638508316, 13.75661411855691, 10.164...</td>\n",
       "      <td>[34.00357410810434, -22.402803761126375, -33.6...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>The Ring</td>\n",
       "      <td>fear</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[-1.0540439758948354, 10.2278997652951, 7.6636...</td>\n",
       "      <td>[-2.913624451554164, 7.855432900298843, 3.7526...</td>\n",
       "      <td>[-77.78475320526675, -85.50147614873666, -73.7...</td>\n",
       "      <td>[-7.365409600031402, -40.69691126850421, -29.4...</td>\n",
       "      <td>[8.953908321623967, -76.68765838967207, -62.84...</td>\n",
       "      <td>[-7.614922043039722, -9.66652009112654, -4.025...</td>\n",
       "      <td>[-6.910755146870472, -41.27047808440868, -24.8...</td>\n",
       "      <td>[-30.26654058201691, -98.98452880038919, -47.7...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-22.956039179678584, -22.95605209016051, -15....</td>\n",
       "      <td>[5.610392693926274, -68.23502592613708, -65.67...</td>\n",
       "      <td>[13.143391845043018, -77.11309307936695, -52.4...</td>\n",
       "      <td>22</td>\n",
       "      <td>male</td>\n",
       "      <td>300</td>\n",
       "      <td>excitement</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>[-31.704198905027933, -32.72951525295624, -28....</td>\n",
       "      <td>[-15.555043379043072, -17.605404769358245, -20...</td>\n",
       "      <td>[-18.61622651269403, -13.999609198624913, -15....</td>\n",
       "      <td>[-14.698681349777496, -13.672134060956148, -14...</td>\n",
       "      <td>[-23.30060275410491, -22.78753654145279, -17.6...</td>\n",
       "      <td>[-8.816268873573902, -6.252239101327289, -8.81...</td>\n",
       "      <td>[279.79109403670617, 285.42943133898524, 287.4...</td>\n",
       "      <td>[-7.173135811725212, -1.0185719947610834, 2.05...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1240.0690002671954, -1196.9337449674017, -93...</td>\n",
       "      <td>[-15.402448839378042, -15.401729520758739, -17...</td>\n",
       "      <td>[-37.972611579298515, -38.48291150179311, -40....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>Gentlemans Agreement</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>[5.466920035649326, 2.9008550036569596, -1.203...</td>\n",
       "      <td>[19.824976691430564, 16.74719391801507, 16.233...</td>\n",
       "      <td>[22.984495698257362, 19.393947142874296, 18.36...</td>\n",
       "      <td>[10.522773668263424, 11.035265103820846, 11.03...</td>\n",
       "      <td>[21.95946583143874, 24.009312500894033, 28.110...</td>\n",
       "      <td>[21.238268640320484, 23.80170581178004, 21.236...</td>\n",
       "      <td>[20.594802762840782, 20.593162081299607, 20.59...</td>\n",
       "      <td>[38.0866367831533, 34.49652294398144, 32.44487...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-256.4677886910428, -450.34520134403937, -462...</td>\n",
       "      <td>[21.75392584043096, 20.215181211643205, 16.112...</td>\n",
       "      <td>[18.966287091329455, 18.45239653087682, 15.374...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>Psycho</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>[-23.62545386161945, -23.62685140441953, -20.0...</td>\n",
       "      <td>[-13.207442040493266, -10.132756016797817, -12...</td>\n",
       "      <td>[5.919693285935381, 5.404129601617996, 6.93984...</td>\n",
       "      <td>[2.4862210644104663, 4.021971582134875, 9.6602...</td>\n",
       "      <td>[-1.9057467428498671, -3.960635285737833, -15....</td>\n",
       "      <td>[4.3931845090082255, 4.390480711103866, 2.3364...</td>\n",
       "      <td>[-2.3892454894785056, -4.442585452677101, -7.0...</td>\n",
       "      <td>[19.635451596486917, 20.658453846575583, 22.70...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-82.28162249200449, -125.86801543132987, -127...</td>\n",
       "      <td>[-9.257969718958025, -6.69583084101619, -3.620...</td>\n",
       "      <td>[-18.03784008673325, -18.03905038031304, -18.5...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Bourne Identitiy</td>\n",
       "      <td>excitement</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>[-6.264400004813267, -5.239062339847094, -4.72...</td>\n",
       "      <td>[5.878397163971902, -6.430141588554244, -11.04...</td>\n",
       "      <td>[6.265703153080619, 5.7513450467094, 0.1087818...</td>\n",
       "      <td>[10.77405565695624, 11.28418632089431, 10.2558...</td>\n",
       "      <td>[-8.565089727038279, 7.844667256426551, 16.049...</td>\n",
       "      <td>[1.4333426212944123, 6.0485652422077125, 9.125...</td>\n",
       "      <td>[-36.47451244649021, -37.505569346830654, -39....</td>\n",
       "      <td>[-16.793433465247425, -12.178954818932333, -11...</td>\n",
       "      <td>...</td>\n",
       "      <td>[11.40135085381238, -8.08989651508042, -39.888...</td>\n",
       "      <td>[-9.197435699224982, 1.0585177042776617, 4.134...</td>\n",
       "      <td>[4.8526302066835605, 7.928167153135863, 4.3370...</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>[14.369879486735218, 10.779547738589091, 8.214...</td>\n",
       "      <td>[28.6391787768303, 29.66291739078911, 24.01998...</td>\n",
       "      <td>[-1.66603287039948, -4.744893004886084, -2.695...</td>\n",
       "      <td>[16.369912695275517, 15.856474606888142, 17.90...</td>\n",
       "      <td>[12.222244946599554, 1.9648341686567319, 1.451...</td>\n",
       "      <td>[20.29831967575734, 20.297050489867217, 18.244...</td>\n",
       "      <td>[19.887520662319414, 14.24664059415195, 13.733...</td>\n",
       "      <td>[23.18039318144982, 20.102418744557855, 15.998...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-57.52073108233021, -58.03694740968917, -66.7...</td>\n",
       "      <td>[-42.15346714183702, -45.2312856859056, -48.30...</td>\n",
       "      <td>[-0.8823006451415075, -3.4484144085066455, -4....</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>The Departed</td>\n",
       "      <td>surprise</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant video                                            AF3_raw  \\\n",
       "0             1     1  [-10.349212474280488, 0.9320963567671932, 4.52...   \n",
       "1             1     2  [-0.38710522523616814, 7.818181696969973, 5.25...   \n",
       "2             1     3  [6.740238271785735, -1.464670313963118, -7.105...   \n",
       "3             1     4  [-7.798221562777997, 4.509400087366941, 9.1247...   \n",
       "4             1     5  [-1.0540439758948354, 10.2278997652951, 7.6636...   \n",
       "..          ...   ...                                                ...   \n",
       "409          23    14  [-31.704198905027933, -32.72951525295624, -28....   \n",
       "410          23    15  [5.466920035649326, 2.9008550036569596, -1.203...   \n",
       "411          23    16  [-23.62545386161945, -23.62685140441953, -20.0...   \n",
       "412          23    17  [-6.264400004813267, -5.239062339847094, -4.72...   \n",
       "413          23    18  [14.369879486735218, 10.779547738589091, 8.214...   \n",
       "\n",
       "                                                F7_raw  \\\n",
       "0    [-7.336422979280564, -2.7212738534012937, 0.35...   \n",
       "1    [-0.9492062185599682, 8.281459209581069, 2.640...   \n",
       "2    [3.413843732230308, -4.790622767795466, -10.94...   \n",
       "3    [-5.116608810783481, 4.626821600452269, 11.293...   \n",
       "4    [-2.913624451554164, 7.855432900298843, 3.7526...   \n",
       "..                                                 ...   \n",
       "409  [-15.555043379043072, -17.605404769358245, -20...   \n",
       "410  [19.824976691430564, 16.74719391801507, 16.233...   \n",
       "411  [-13.207442040493266, -10.132756016797817, -12...   \n",
       "412  [5.878397163971902, -6.430141588554244, -11.04...   \n",
       "413  [28.6391787768303, 29.66291739078911, 24.01998...   \n",
       "\n",
       "                                                F3_raw  \\\n",
       "0    [-6.357392050340919, -46.86718793292361, -64.3...   \n",
       "1    [-29.84689530067401, -43.18212359451074, -35.4...   \n",
       "2    [-47.36015967778116, -42.74643128266957, -39.6...   \n",
       "3    [24.93125629049786, 11.084152372000009, 10.057...   \n",
       "4    [-77.78475320526675, -85.50147614873666, -73.7...   \n",
       "..                                                 ...   \n",
       "409  [-18.61622651269403, -13.999609198624913, -15....   \n",
       "410  [22.984495698257362, 19.393947142874296, 18.36...   \n",
       "411  [5.919693285935381, 5.404129601617996, 6.93984...   \n",
       "412  [6.265703153080619, 5.7513450467094, 0.1087818...   \n",
       "413  [-1.66603287039948, -4.744893004886084, -2.695...   \n",
       "\n",
       "                                               FC5_raw  \\\n",
       "0    [27.834342803367694, -39.340612753736416, -63....   \n",
       "1    [-2.218007780462737, -29.91257528703265, -13.5...   \n",
       "2    [-51.68328934853619, -26.555439767001214, -9.1...   \n",
       "3    [37.87436013579214, -10.845343361281941, -23.1...   \n",
       "4    [-7.365409600031402, -40.69691126850421, -29.4...   \n",
       "..                                                 ...   \n",
       "409  [-14.698681349777496, -13.672134060956148, -14...   \n",
       "410  [10.522773668263424, 11.035265103820846, 11.03...   \n",
       "411  [2.4862210644104663, 4.021971582134875, 9.6602...   \n",
       "412  [10.77405565695624, 11.28418632089431, 10.2558...   \n",
       "413  [16.369912695275517, 15.856474606888142, 17.90...   \n",
       "\n",
       "                                                T7_raw  \\\n",
       "0    [22.051642042275102, -40.510159638368734, -71....   \n",
       "1    [-29.2927823170884, -56.472712994591255, -43.6...   \n",
       "2    [-62.37528256815315, -45.452432926611465, -30....   \n",
       "3    [50.74481684256556, 2.538494081670473, -3.6165...   \n",
       "4    [8.953908321623967, -76.68765838967207, -62.84...   \n",
       "..                                                 ...   \n",
       "409  [-23.30060275410491, -22.78753654145279, -17.6...   \n",
       "410  [21.95946583143874, 24.009312500894033, 28.110...   \n",
       "411  [-1.9057467428498671, -3.960635285737833, -15....   \n",
       "412  [-8.565089727038279, 7.844667256426551, 16.049...   \n",
       "413  [12.222244946599554, 1.9648341686567319, 1.451...   \n",
       "\n",
       "                                                P7_raw  \\\n",
       "0    [-25.750224861795058, -41.64657512470267, -48....   \n",
       "1    [-1.1194001362567376, -3.1707960975111944, 0.4...   \n",
       "2    [-24.54733880679436, -18.391668699835865, -11....   \n",
       "3    [9.49379155066421, 1.288462268119328, 2.313902...   \n",
       "4    [-7.614922043039722, -9.66652009112654, -4.025...   \n",
       "..                                                 ...   \n",
       "409  [-8.816268873573902, -6.252239101327289, -8.81...   \n",
       "410  [21.238268640320484, 23.80170581178004, 21.236...   \n",
       "411  [4.3931845090082255, 4.390480711103866, 2.3364...   \n",
       "412  [1.4333426212944123, 6.0485652422077125, 9.125...   \n",
       "413  [20.29831967575734, 20.297050489867217, 18.244...   \n",
       "\n",
       "                                                O1_raw  \\\n",
       "0    [0.3891795592307721, -50.89300581509636, -65.7...   \n",
       "1    [-31.946807814430898, -43.741895514175624, -29...   \n",
       "2    [-77.01211916934554, -62.137931068780205, -46....   \n",
       "3    [27.807237142070754, -3.4762036998813866, -3.4...   \n",
       "4    [-6.910755146870472, -41.27047808440868, -24.8...   \n",
       "..                                                 ...   \n",
       "409  [279.79109403670617, 285.42943133898524, 287.4...   \n",
       "410  [20.594802762840782, 20.593162081299607, 20.59...   \n",
       "411  [-2.3892454894785056, -4.442585452677101, -7.0...   \n",
       "412  [-36.47451244649021, -37.505569346830654, -39....   \n",
       "413  [19.887520662319414, 14.24664059415195, 13.733...   \n",
       "\n",
       "                                                O2_raw  ...  \\\n",
       "0    [43.423268402468864, -42.7273519601261, -81.69...  ...   \n",
       "1    [-55.545831695510955, -85.80257081294347, -68....  ...   \n",
       "2    [-70.50876618489838, -48.97123494887409, -36.6...  ...   \n",
       "3    [77.15881665492797, -7.457717821908049, -38.22...  ...   \n",
       "4    [-30.26654058201691, -98.98452880038919, -47.7...  ...   \n",
       "..                                                 ...  ...   \n",
       "409  [-7.173135811725212, -1.0185719947610834, 2.05...  ...   \n",
       "410  [38.0866367831533, 34.49652294398144, 32.44487...  ...   \n",
       "411  [19.635451596486917, 20.658453846575583, 22.70...  ...   \n",
       "412  [-16.793433465247425, -12.178954818932333, -11...  ...   \n",
       "413  [23.18039318144982, 20.102418744557855, 15.998...  ...   \n",
       "\n",
       "                                                F4_raw  \\\n",
       "0    [-10.364532668578148, -26.259576176464076, -32...   \n",
       "1    [-17.208695619364846, -16.19140531780937, -11....   \n",
       "2    [-21.93647870702945, -27.57728772842303, -25.5...   \n",
       "3    [5.750397059373909, 2.6719938533138072, 4.2089...   \n",
       "4    [-22.956039179678584, -22.95605209016051, -15....   \n",
       "..                                                 ...   \n",
       "409  [-1240.0690002671954, -1196.9337449674017, -93...   \n",
       "410  [-256.4677886910428, -450.34520134403937, -462...   \n",
       "411  [-82.28162249200449, -125.86801543132987, -127...   \n",
       "412  [11.40135085381238, -8.08989651508042, -39.888...   \n",
       "413  [-57.52073108233021, -58.03694740968917, -66.7...   \n",
       "\n",
       "                                                F8_raw  \\\n",
       "0    [-2.7922149552880873, -51.50648426600064, -69....   \n",
       "1    [-122.12733116987303, -138.02650317995787, -12...   \n",
       "2    [-92.89575464126565, -81.61575420193722, -68.7...   \n",
       "3    [44.528006638508316, 13.75661411855691, 10.164...   \n",
       "4    [5.610392693926274, -68.23502592613708, -65.67...   \n",
       "..                                                 ...   \n",
       "409  [-15.402448839378042, -15.401729520758739, -17...   \n",
       "410  [21.75392584043096, 20.215181211643205, 16.112...   \n",
       "411  [-9.257969718958025, -6.69583084101619, -3.620...   \n",
       "412  [-9.197435699224982, 1.0585177042776617, 4.134...   \n",
       "413  [-42.15346714183702, -45.2312856859056, -48.30...   \n",
       "\n",
       "                                               AF4_raw Age Gender  \\\n",
       "0    [-0.5554431681218347, -74.90768364976493, -108...  22   male   \n",
       "1    [-85.11469615543234, -90.75595696563374, -65.6...  22   male   \n",
       "2    [-117.74840854597883, -110.56750845050888, -11...  22   male   \n",
       "3    [34.00357410810434, -22.402803761126375, -33.6...  22   male   \n",
       "4    [13.143391845043018, -77.11309307936695, -52.4...  22   male   \n",
       "..                                                 ...  ..    ...   \n",
       "409  [-37.972611579298515, -38.48291150179311, -40....  25   male   \n",
       "410  [18.966287091329455, 18.45239653087682, 15.374...  25   male   \n",
       "411  [-18.03784008673325, -18.03905038031304, -18.5...  25   male   \n",
       "412  [4.8526302066835605, 7.928167153135863, 4.3370...  25   male   \n",
       "413  [-0.8823006451415075, -3.4484144085066455, -4....  25   male   \n",
       "\n",
       "                      Video_Name Target_Emotion Valence Arousal Dominance  \n",
       "0    Searching for Bobby Fischer       calmness       4       3         2  \n",
       "1                         D.O.A.       surprise       3       3         1  \n",
       "2                   The Hangover      amusement       5       4         4  \n",
       "3                       The Ring           fear       4       3         2  \n",
       "4                            300     excitement       4       4         4  \n",
       "..                           ...            ...     ...     ...       ...  \n",
       "409         Gentlemans Agreement          anger       2       2         2  \n",
       "410                       Psycho           fear       2       2         2  \n",
       "411         The Bourne Identitiy     excitement       3       3         2  \n",
       "412     The Shawshank Redemption        sadness       2       2         4  \n",
       "413                 The Departed       surprise       2       4         2  \n",
       "\n",
       "[414 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c74985-dfd9-432a-ab08-329875ea54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming 'final_df' is already loaded\n",
    "\n",
    "# Define the target variable\n",
    "target = 'Target_Emotion'\n",
    "\n",
    "# Extract DWT features\n",
    "eeg_features = [col for col in final_df.columns if col.endswith(('raw'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c451880-d634-4349-8b28-66af0cce15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "final_df[target] = label_encoder.fit_transform(final_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e106cf-3aaf-467c-bfb3-6cf6c61e78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for EEG model\n",
    "X = final_df[eeg_features]\n",
    "y = final_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b17140b-bbcc-4037-8dd6-bc5115d45ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A2000 12GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bc1a54-d0ca-4340-be6b-b0a2657a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = X.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 14, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(14):\n",
    "        reshaped_data[i, j, :] = X_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# # Normalize the data\n",
    "# mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "# std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "# standardized_data = (standardized_data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b768f89-698f-42a9-bad2-cf88dc16e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_indices = torch.tensor(y.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b130e-7a25-4a84-85a3-590f49644231",
   "metadata": {},
   "source": [
    "#### CNN Model on Raw EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4258385-3944-431b-b39f-c0761ddab3ba",
   "metadata": {},
   "source": [
    "#### For classifying emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a24e11fd-17b4-40c3-859f-4ecba6b2da94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/30], Training Loss: 2.2869\n",
      "Accuracy on training set: 17.52%\n",
      "Validation Loss: 2.2508\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [2/30], Training Loss: 2.1278\n",
      "Accuracy on training set: 20.85%\n",
      "Validation Loss: 2.1989\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [3/30], Training Loss: 1.8732\n",
      "Accuracy on training set: 39.88%\n",
      "Validation Loss: 2.1500\n",
      "Accuracy on validation set: 13.25%\n",
      "\n",
      "Epoch [4/30], Training Loss: 1.4333\n",
      "Accuracy on training set: 60.73%\n",
      "Validation Loss: 1.9942\n",
      "Accuracy on validation set: 30.12%\n",
      "\n",
      "Epoch [5/30], Training Loss: 0.9086\n",
      "Accuracy on training set: 93.66%\n",
      "Validation Loss: 1.6627\n",
      "Accuracy on validation set: 50.60%\n",
      "\n",
      "Epoch [6/30], Training Loss: 0.5948\n",
      "Accuracy on training set: 97.58%\n",
      "Validation Loss: 1.5425\n",
      "Accuracy on validation set: 57.83%\n",
      "\n",
      "Epoch [7/30], Training Loss: 0.4246\n",
      "Accuracy on training set: 99.40%\n",
      "Validation Loss: 1.5773\n",
      "Accuracy on validation set: 49.40%\n",
      "\n",
      "Epoch [8/30], Training Loss: 0.3181\n",
      "Accuracy on training set: 99.70%\n",
      "Validation Loss: 1.5256\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [9/30], Training Loss: 0.2337\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.5648\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [10/30], Training Loss: 0.2217\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.5262\n",
      "Accuracy on validation set: 53.01%\n",
      "\n",
      "Epoch [11/30], Training Loss: 0.1385\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.5164\n",
      "Accuracy on validation set: 50.60%\n",
      "\n",
      "Epoch [12/30], Training Loss: 0.1201\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4998\n",
      "Accuracy on validation set: 55.42%\n",
      "\n",
      "Epoch [13/30], Training Loss: 0.0943\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4604\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [14/30], Training Loss: 0.0841\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4529\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [15/30], Training Loss: 0.0742\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4630\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [16/30], Training Loss: 0.0674\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4486\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [17/30], Training Loss: 0.0650\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4605\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [18/30], Training Loss: 0.0576\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4384\n",
      "Accuracy on validation set: 53.01%\n",
      "\n",
      "Epoch [19/30], Training Loss: 0.0615\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4380\n",
      "Accuracy on validation set: 49.40%\n",
      "\n",
      "Epoch [20/30], Training Loss: 0.0459\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4453\n",
      "Accuracy on validation set: 53.01%\n",
      "\n",
      "Epoch [21/30], Training Loss: 0.0553\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4250\n",
      "Accuracy on validation set: 53.01%\n",
      "\n",
      "Epoch [22/30], Training Loss: 0.0569\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4421\n",
      "Accuracy on validation set: 55.42%\n",
      "\n",
      "Epoch [23/30], Training Loss: 0.0686\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4482\n",
      "Accuracy on validation set: 50.60%\n",
      "\n",
      "Epoch [24/30], Training Loss: 0.0551\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4374\n",
      "Accuracy on validation set: 54.22%\n",
      "\n",
      "Epoch [25/30], Training Loss: 0.0483\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4292\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [26/30], Training Loss: 0.0451\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4373\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [27/30], Training Loss: 0.0472\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4323\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [28/30], Training Loss: 0.0497\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4500\n",
      "Accuracy on validation set: 51.81%\n",
      "\n",
      "Epoch [29/30], Training Loss: 0.0458\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4286\n",
      "Accuracy on validation set: 49.40%\n",
      "\n",
      "Epoch [30/30], Training Loss: 0.0439\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.4293\n",
      "Accuracy on validation set: 50.60%\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [1/30], Training Loss: 2.2317\n",
      "Accuracy on training set: 20.54%\n",
      "Validation Loss: 2.2282\n",
      "Accuracy on validation set: 7.23%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/30], Training Loss: 2.2687\n",
      "Accuracy on training set: 19.94%\n",
      "Validation Loss: 2.2560\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/30], Training Loss: 2.2999\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 2.2175\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/30], Training Loss: 2.2754\n",
      "Accuracy on training set: 26.20%\n",
      "Validation Loss: 2.2403\n",
      "Accuracy on validation set: 14.63%\n",
      "\n",
      "Early stopping\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define a more complex CNN model\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 480, 512)  # Adjusted input size\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch size is preserved\n",
    "        x = torch.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ComplexCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "num_epochs = 30  # Increased number of epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn2.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = ComplexCNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b1db1-61b7-4648-8b0f-d33d4ca55237",
   "metadata": {},
   "source": [
    "### Evaluating the model (Cross fold Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "74c60776-e35c-4a21-9512-2a91f5e3a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Final Model Mean Validation Accuracy: 90.58%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Function to evaluate the final model\n",
    "def evaluate_final_model():\n",
    "    k_folds = 5\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    val_accuracies = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "        \n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(val_accuracies)\n",
    "    print(f'Final Model Mean Validation Accuracy: {mean_accuracy:.2f}%')\n",
    "\n",
    "evaluate_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a31f66f0-e635-4999-8f3b-350418025ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Final Model Mean Validation Accuracy for Class amusement: 96.79%\n",
      "Final Model Mean Validation Accuracy for Class anger: 92.14%\n",
      "Final Model Mean Validation Accuracy for Class calmness: 88.33%\n",
      "Final Model Mean Validation Accuracy for Class disgust: 90.91%\n",
      "Final Model Mean Validation Accuracy for Class excitement: 95.00%\n",
      "Final Model Mean Validation Accuracy for Class fear: 89.17%\n",
      "Final Model Mean Validation Accuracy for Class happiness: 90.73%\n",
      "Final Model Mean Validation Accuracy for Class sadness: 90.95%\n",
      "Final Model Mean Validation Accuracy for Class surprise: 91.46%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to evaluate the final model\n",
    "def evaluate_model_per_class():\n",
    "    k_folds = 5\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    num_classes = 9\n",
    "    val_accuracies_per_class = np.zeros((k_folds, num_classes))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "        \n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        correct_predictions_per_class = np.zeros(num_classes)\n",
    "        total_predictions_per_class = np.zeros(num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                for i in range(num_classes):\n",
    "                    correct_predictions_per_class[i] += ((predicted_classes == i) & (batch_y == i)).sum().item()\n",
    "                    total_predictions_per_class[i] += (batch_y == i).sum().item()\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            if total_predictions_per_class[i] > 0:\n",
    "                val_accuracies_per_class[fold, i] = correct_predictions_per_class[i] / total_predictions_per_class[i] * 100\n",
    "            else:\n",
    "                val_accuracies_per_class[fold, i] = float('nan')  # Handle case where there are no samples for a class\n",
    "    \n",
    "    mean_accuracies_per_class = np.nanmean(val_accuracies_per_class, axis=0)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        print(f'Final Model Mean Validation Accuracy for Class {label_encoder.inverse_transform([i])[0]}: {mean_accuracies_per_class[i]:.2f}%')\n",
    "\n",
    "evaluate_model_per_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e53beefa-3495-4c10-8350-336b47a42d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on random part of dataset just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c5ccdd-f272-4bcb-88ff-f72278122fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "# Define a more complex CNN model\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 480, 512)  # Adjusted input size\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch size is preserved\n",
    "        x = torch.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ComplexCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn2.pth'\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12a1cd4-efab-42b8-aa3c-f683f603d5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming standardized_data and y_one_hot are already defined and converted to tensors\u001b[39;00m\n\u001b[0;32m      9\u001b[0m standardized_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(standardized_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m414\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m7680\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined and converted to tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Split the dataset into training and test sets (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "y = model(test_dataset)\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9cfc9381-f29f-4e00-aa2b-d4b91f98d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 93.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined and converted to tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Split the dataset into training and test sets (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Accuracy on test dataset: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14303de5-48e1-48ab-a45a-ca38a9a53324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555973a4-a3f5-4d0a-bb31-440f3463e457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "828cb2a4-28fd-4ae4-b781-d08cad0087b0",
   "metadata": {},
   "source": [
    "### The below model gave less accuracy than the one we have gotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e6c9416-b1aa-4a49-9b4e-75fa57d79644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/30], Training Loss: 2.2645\n",
      "Accuracy on training set: 22.66%\n",
      "Validation Loss: 2.2252\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [2/30], Training Loss: 2.0877\n",
      "Accuracy on training set: 23.87%\n",
      "Validation Loss: 2.2175\n",
      "Accuracy on validation set: 7.23%\n",
      "\n",
      "Epoch [3/30], Training Loss: 1.7780\n",
      "Accuracy on training set: 40.48%\n",
      "Validation Loss: 2.1957\n",
      "Accuracy on validation set: 13.25%\n",
      "\n",
      "Epoch [4/30], Training Loss: 1.3975\n",
      "Accuracy on training set: 71.90%\n",
      "Validation Loss: 2.1185\n",
      "Accuracy on validation set: 19.28%\n",
      "\n",
      "Epoch [5/30], Training Loss: 1.0109\n",
      "Accuracy on training set: 94.26%\n",
      "Validation Loss: 1.9609\n",
      "Accuracy on validation set: 28.92%\n",
      "\n",
      "Epoch [6/30], Training Loss: 0.7349\n",
      "Accuracy on training set: 98.49%\n",
      "Validation Loss: 1.8874\n",
      "Accuracy on validation set: 33.73%\n",
      "\n",
      "Epoch [7/30], Training Loss: 0.5506\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9100\n",
      "Accuracy on validation set: 31.33%\n",
      "\n",
      "Epoch [8/30], Training Loss: 0.4617\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.8587\n",
      "Accuracy on validation set: 39.76%\n",
      "\n",
      "Epoch [9/30], Training Loss: 0.3558\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7815\n",
      "Accuracy on validation set: 40.96%\n",
      "\n",
      "Epoch [10/30], Training Loss: 0.2885\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7792\n",
      "Accuracy on validation set: 44.58%\n",
      "\n",
      "Epoch [11/30], Training Loss: 0.2499\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7526\n",
      "Accuracy on validation set: 43.37%\n",
      "\n",
      "Epoch [12/30], Training Loss: 0.2605\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7527\n",
      "Accuracy on validation set: 45.78%\n",
      "\n",
      "Epoch [13/30], Training Loss: 0.2018\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7279\n",
      "Accuracy on validation set: 43.37%\n",
      "\n",
      "Epoch [14/30], Training Loss: 0.1570\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7036\n",
      "Accuracy on validation set: 46.99%\n",
      "\n",
      "Epoch [15/30], Training Loss: 0.1686\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7385\n",
      "Accuracy on validation set: 43.37%\n",
      "\n",
      "Epoch [16/30], Training Loss: 0.1450\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7299\n",
      "Accuracy on validation set: 45.78%\n",
      "\n",
      "Epoch [17/30], Training Loss: 0.1241\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7307\n",
      "Accuracy on validation set: 45.78%\n",
      "\n",
      "Epoch [18/30], Training Loss: 0.1236\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7357\n",
      "Accuracy on validation set: 44.58%\n",
      "\n",
      "Epoch [19/30], Training Loss: 0.1278\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.7631\n",
      "Accuracy on validation set: 42.17%\n",
      "\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Epoch [1/30], Training Loss: 2.2653\n",
      "Accuracy on training set: 18.43%\n",
      "Validation Loss: 2.1918\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/30], Training Loss: 2.2696\n",
      "Accuracy on training set: 22.36%\n",
      "Validation Loss: 2.2241\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/30], Training Loss: 2.2721\n",
      "Accuracy on training set: 21.15%\n",
      "Validation Loss: 2.2260\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/30], Training Loss: 2.2483\n",
      "Accuracy on training set: 16.27%\n",
      "Validation Loss: 2.2508\n",
      "Accuracy on validation set: 9.76%\n",
      "\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 31.07%\n",
      "Standard Deviation of Validation Accuracy: 14.92%\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define a more complex CNN model\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 480, 512)  # Adjusted input size\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch size is preserved\n",
    "        x = torch.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ComplexCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "num_epochs = 30  # Increased number of epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = ComplexCNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Calculate mean and standard deviation of validation accuracies\n",
    "mean_accuracy = np.mean(val_accuracies)\n",
    "std_accuracy = np.std(val_accuracies)\n",
    "\n",
    "print(f'Mean Validation Accuracy: {mean_accuracy:.2f}%')\n",
    "print(f'Standard Deviation of Validation Accuracy: {std_accuracy:.2f}%')\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38ce1f-c6e8-48bf-94eb-c34ae4d157f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02fc5a40-e4b5-4abe-acb8-7e47ac7f51b2",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "14748ce4-b9cf-47ad-a055-0657cf051eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning with lr=0.001, wd=0.0001, dr=0.5\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 133\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    134\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    135\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define a more complex CNN model\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 480, 512)  # Adjusted input size\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn6 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch size is preserved\n",
    "        x = torch.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.0005]\n",
    "weight_decays = [1e-4, 1e-5]\n",
    "dropout_rates = [0.5, 0.3]\n",
    "\n",
    "best_mean_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for dr in dropout_rates:\n",
    "            print(f'Tuning with lr={lr}, wd={wd}, dr={dr}')\n",
    "            \n",
    "            # Initialize lists to store loss and accuracy\n",
    "            val_accuracies = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "                print(f'Fold {fold+1}/{k_folds}')\n",
    "                \n",
    "                train_subset = Subset(dataset, train_idx)\n",
    "                val_subset = Subset(dataset, val_idx)\n",
    "                train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "                val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "                \n",
    "                model = ComplexCNNModel(dropout_rate=dr).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "                \n",
    "                early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "                num_epochs = 30\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for batch_x, batch_y in train_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(batch_x)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    model.eval()\n",
    "                    val_loss = 0\n",
    "                    correct_predictions = 0\n",
    "                    total_predictions = 0\n",
    "                    with torch.no_grad():\n",
    "                        for batch_x, batch_y in val_loader:\n",
    "                            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                            outputs = model(batch_x)\n",
    "                            loss = criterion(outputs, batch_y)\n",
    "                            val_loss += loss.item()\n",
    "                            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                            correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                            total_predictions += batch_x.size(0)\n",
    "                    \n",
    "                    accuracy = correct_predictions / total_predictions * 100\n",
    "                    val_accuracies.append(accuracy)\n",
    "                    \n",
    "                    early_stopping(val_loss, model, model_path)\n",
    "                    \n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "\n",
    "            mean_accuracy = np.mean(val_accuracies)\n",
    "            print(f'Mean Validation Accuracy: {mean_accuracy:.2f}%')\n",
    "\n",
    "            if mean_accuracy > best_mean_accuracy:\n",
    "                best_mean_accuracy = mean_accuracy\n",
    "                best_hyperparams = {'lr': lr, 'wd': wd, 'dr': dr}\n",
    "\n",
    "print(f'Best Hyperparameters: {best_hyperparams}')\n",
    "print(f'Best Mean Validation Accuracy: {best_mean_accuracy:.2f}%')\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40d812-408c-4cea-999a-52dffd9ba582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5df59238-bd45-4df1-81de-65708786c248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning with lr=0.001, wd=0.0001, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 41.58%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 40.18%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.06%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.86%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 39.74%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.63%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.01%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.91%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 41.29%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.39%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.55%\n",
      "Tuning with lr=0.001, wd=0.0001, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 38.18%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.84%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 44.61%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 34.18%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.53%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 41.01%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.78%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.13%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 38.98%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.52%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.48%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.46%\n",
      "Tuning with lr=0.001, wd=1e-05, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.02%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 41.25%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.90%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 34.61%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.24%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 38.29%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 39.00%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.01%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.89%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 38.49%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 39.66%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.48%\n",
      "Tuning with lr=0.001, wd=1e-06, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.40%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 37.67%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 40.38%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 33.84%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.03%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 39.55%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 38.22%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 31.29%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 36.88%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.50%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.85%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 33.22%\n",
      "Tuning with lr=0.0005, wd=0.0001, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.05%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 38.19%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 41.48%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 32.60%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 33.14%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.42%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 40.83%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.76%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 31.26%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.18%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.49%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.44%\n",
      "Tuning with lr=0.0005, wd=1e-05, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.49%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 38.33%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 40.19%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 30.99%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.85%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 40.47%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 39.26%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 32.06%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.02%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 36.59%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 37.73%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 33.82%\n",
      "Tuning with lr=0.0005, wd=1e-06, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 34.49%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 27.47%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 32.36%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 24.24%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 24.91%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 32.25%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 35.48%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 25.07%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 26.42%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 30.01%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 31.56%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 25.04%\n",
      "Tuning with lr=0.0001, wd=0.0001, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 23.32%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 29.52%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 32.80%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 25.73%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 24.81%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 32.28%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 31.18%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 28.06%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 30.89%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 30.20%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 27.45%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 27.63%\n",
      "Tuning with lr=0.0001, wd=1e-05, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 27.70%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.5, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 30.50%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.5, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 33.54%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.5, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 28.50%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.5, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 26.83%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.3, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 33.71%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.3, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 33.09%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.3, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 23.27%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.3, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 28.10%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.2, batch_size=32, num_epochs=30\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 33.51%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.2, batch_size=32, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 29.98%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.2, batch_size=64, num_epochs=30\n",
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Mean Validation Accuracy: 27.34%\n",
      "Tuning with lr=0.0001, wd=1e-06, dr=0.2, batch_size=64, num_epochs=50\n",
      "Fold 1/5\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Early stopping\n",
      "Mean Validation Accuracy: 25.53%\n",
      "Best Hyperparameters: {'lr': 0.001, 'wd': 1e-05, 'dr': 0.5, 'batch_size': 32, 'num_epochs': 50}\n",
      "Best Mean Validation Accuracy: 44.61%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "weight_decays = [1e-4, 1e-5, 1e-6]\n",
    "dropout_rates = [0.5, 0.3, 0.2]\n",
    "batch_sizes = [32, 64]\n",
    "num_epochs_list = [30, 50]\n",
    "\n",
    "# Function to perform hyperparameter tuning\n",
    "def hyperparameter_tuning():\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    for lr, wd, dr, batch_size, num_epochs in itertools.product(learning_rates, weight_decays, dropout_rates, batch_sizes, num_epochs_list):\n",
    "        print(f\"Tuning with lr={lr}, wd={wd}, dr={dr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "        \n",
    "        k_folds = 5\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "        \n",
    "        val_accuracies = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            print(f'Fold {fold+1}/{k_folds}')\n",
    "            \n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = ComplexCNNModel(dropout_rate=dr).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "            early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                correct_predictions = 0\n",
    "                total_predictions = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_x, batch_y in val_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_x)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "                        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                        total_predictions += batch_x.size(0)\n",
    "                \n",
    "                accuracy = correct_predictions / total_predictions * 100\n",
    "                val_accuracies.append(accuracy)\n",
    "                \n",
    "                early_stopping(val_loss, model, model_path)\n",
    "                \n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "        \n",
    "        mean_accuracy = np.mean(val_accuracies)\n",
    "        print(f'Mean Validation Accuracy: {mean_accuracy:.2f}%')\n",
    "        \n",
    "        if mean_accuracy > best_accuracy:\n",
    "            best_accuracy = mean_accuracy\n",
    "            best_hyperparams = {'lr': lr, 'wd': wd, 'dr': dr, 'batch_size': batch_size, 'num_epochs': num_epochs}\n",
    "    \n",
    "    print(f'Best Hyperparameters: {best_hyperparams}')\n",
    "    print(f'Best Mean Validation Accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d89303-db59-49c3-b790-a4f1fe62d3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d219a42-5f49-419c-a4fe-91183df2c79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0757945-2173-4572-a918-71f41df06a6d",
   "metadata": {},
   "source": [
    "### Evaluating the best model's mean validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "94669b8e-2f06-45c0-9b4c-3bf78b63bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = {'lr': 0.001, 'wd': 1e-05, 'dr': 0.5, 'batch_size': 32, 'num_epochs': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d8b15652-4628-4580-9cb1-bf80095d78a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "Final Model Mean Validation Accuracy: 85.27%\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the final model\n",
    "def evaluate_final_model(best_hyperparams):\n",
    "    lr = best_hyperparams['lr']\n",
    "    wd = best_hyperparams['wd']\n",
    "    dr = best_hyperparams['dr']\n",
    "    batch_size = best_hyperparams['batch_size']\n",
    "    \n",
    "    k_folds = 5\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    val_accuracies = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "        \n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = ComplexCNNModel(dropout_rate=dr).to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(val_accuracies)\n",
    "    print(f'Final Model Mean Validation Accuracy: {mean_accuracy:.2f}%')\n",
    "\n",
    "evaluate_final_model(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e494587-97e9-450b-af98-8d16cad13583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca20d9d-0fc4-4d42-808d-6f337a69c34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d735d5-084c-4962-9be2-933334a9131d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9eebb-7e54-409d-96b6-e8c53e09e187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7e44f-5001-4ecd-b3fb-a66152f5ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6ce75b-ac81-474b-815c-e618334dd969",
   "metadata": {},
   "source": [
    "### The below model is simplified version of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e0083ff2-cac5-4f34-a4ae-354bf8fead92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/30], Training Loss: 2.3254\n",
      "Accuracy on training set: 61.33%\n",
      "Validation Loss: 2.2622\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [2/30], Training Loss: 1.9431\n",
      "Accuracy on training set: 48.34%\n",
      "Validation Loss: 2.2311\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [3/30], Training Loss: 1.6677\n",
      "Accuracy on training set: 79.76%\n",
      "Validation Loss: 2.2393\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Epoch [4/30], Training Loss: 1.3339\n",
      "Accuracy on training set: 92.15%\n",
      "Validation Loss: 2.2123\n",
      "Accuracy on validation set: 16.87%\n",
      "\n",
      "Epoch [5/30], Training Loss: 0.9659\n",
      "Accuracy on training set: 97.28%\n",
      "Validation Loss: 2.1491\n",
      "Accuracy on validation set: 20.48%\n",
      "\n",
      "Epoch [6/30], Training Loss: 0.6004\n",
      "Accuracy on training set: 99.40%\n",
      "Validation Loss: 2.0496\n",
      "Accuracy on validation set: 28.92%\n",
      "\n",
      "Epoch [7/30], Training Loss: 0.3396\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9714\n",
      "Accuracy on validation set: 30.12%\n",
      "\n",
      "Epoch [8/30], Training Loss: 0.1768\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9820\n",
      "Accuracy on validation set: 36.14%\n",
      "\n",
      "Epoch [9/30], Training Loss: 0.1323\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 2.0305\n",
      "Accuracy on validation set: 34.94%\n",
      "\n",
      "Epoch [10/30], Training Loss: 0.0830\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9165\n",
      "Accuracy on validation set: 32.53%\n",
      "\n",
      "Epoch [11/30], Training Loss: 0.0735\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9899\n",
      "Accuracy on validation set: 28.92%\n",
      "\n",
      "Epoch [12/30], Training Loss: 0.0510\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9638\n",
      "Accuracy on validation set: 34.94%\n",
      "\n",
      "Epoch [13/30], Training Loss: 0.0465\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9090\n",
      "Accuracy on validation set: 32.53%\n",
      "\n",
      "Epoch [14/30], Training Loss: 0.0423\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.8700\n",
      "Accuracy on validation set: 42.17%\n",
      "\n",
      "Epoch [15/30], Training Loss: 0.0321\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.8992\n",
      "Accuracy on validation set: 31.33%\n",
      "\n",
      "Epoch [16/30], Training Loss: 0.0267\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.8000\n",
      "Accuracy on validation set: 36.14%\n",
      "\n",
      "Epoch [17/30], Training Loss: 0.0285\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.8459\n",
      "Accuracy on validation set: 38.55%\n",
      "\n",
      "Epoch [18/30], Training Loss: 0.0253\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9580\n",
      "Accuracy on validation set: 33.73%\n",
      "\n",
      "Epoch [19/30], Training Loss: 0.0216\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9272\n",
      "Accuracy on validation set: 33.73%\n",
      "\n",
      "Epoch [20/30], Training Loss: 0.0256\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9097\n",
      "Accuracy on validation set: 31.33%\n",
      "\n",
      "Epoch [21/30], Training Loss: 0.0348\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 2.1480\n",
      "Accuracy on validation set: 24.10%\n",
      "\n",
      "Epoch [22/30], Training Loss: 0.0371\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 2.0160\n",
      "Accuracy on validation set: 33.73%\n",
      "\n",
      "Epoch [23/30], Training Loss: 0.0632\n",
      "Accuracy on training set: 96.07%\n",
      "Validation Loss: 2.5380\n",
      "Accuracy on validation set: 25.30%\n",
      "\n",
      "Epoch [24/30], Training Loss: 0.0684\n",
      "Accuracy on training set: 99.70%\n",
      "Validation Loss: 2.2819\n",
      "Accuracy on validation set: 25.30%\n",
      "\n",
      "Epoch [25/30], Training Loss: 0.0796\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9900\n",
      "Accuracy on validation set: 38.55%\n",
      "\n",
      "Epoch [26/30], Training Loss: 0.0558\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 1.9745\n",
      "Accuracy on validation set: 28.92%\n",
      "\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Epoch [1/30], Training Loss: 2.3354\n",
      "Accuracy on training set: 41.69%\n",
      "Validation Loss: 2.3350\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/30], Training Loss: 2.3255\n",
      "Accuracy on training set: 35.65%\n",
      "Validation Loss: 2.4445\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/30], Training Loss: 2.2427\n",
      "Accuracy on training set: 41.69%\n",
      "Validation Loss: 2.5263\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/30], Training Loss: 2.4507\n",
      "Accuracy on training set: 32.23%\n",
      "Validation Loss: 2.2951\n",
      "Accuracy on validation set: 14.63%\n",
      "\n",
      "Early stopping\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define a more complex CNN model\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc1 = nn.Linear(32 * 1920, 256)  # Adjusted input size\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 9)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ComplexCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "num_epochs = 30  # Increased number of epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn_simplified.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = ComplexCNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec50b27-06f3-4574-b3af-0566de38f5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7751e59-1a66-4ab6-b322-0fc93a26c8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ddd72-6bea-483a-a500-a68dcf02e257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27454f-7389-4b8d-85f2-e49b128ad67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee274204-be48-4b3e-be6a-77e25786a69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719ffa7-5606-439a-8313-54de0f102eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c60013-4742-4d6e-b5ee-beaf93bdeb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcdd8f-f95c-4f42-9af3-e40339a892f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7e8bc6-5d36-4820-87df-66ed98bbab44",
   "metadata": {},
   "source": [
    "### Old Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1296adfb-bd23-4124-907c-3261c24c726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/20], Training Loss: 2.3695\n",
      "Accuracy on training set: 35.65%\n",
      "Validation Loss: 2.4258\n",
      "Accuracy on validation set: 18.07%\n",
      "\n",
      "Epoch [2/20], Training Loss: 2.1110\n",
      "Accuracy on training set: 42.60%\n",
      "Validation Loss: 2.2312\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Epoch [3/20], Training Loss: 1.9401\n",
      "Accuracy on training set: 60.12%\n",
      "Validation Loss: 2.1821\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Epoch [4/20], Training Loss: 1.8425\n",
      "Accuracy on training set: 58.31%\n",
      "Validation Loss: 2.1835\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Epoch [5/20], Training Loss: 1.7187\n",
      "Accuracy on training set: 71.30%\n",
      "Validation Loss: 2.2092\n",
      "Accuracy on validation set: 16.87%\n",
      "\n",
      "Epoch [6/20], Training Loss: 1.6205\n",
      "Accuracy on training set: 83.69%\n",
      "Validation Loss: 2.1933\n",
      "Accuracy on validation set: 18.07%\n",
      "\n",
      "Epoch [7/20], Training Loss: 1.4883\n",
      "Accuracy on training set: 87.31%\n",
      "Validation Loss: 2.1846\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Epoch [8/20], Training Loss: 1.4174\n",
      "Accuracy on training set: 88.82%\n",
      "Validation Loss: 2.1852\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Epoch [9/20], Training Loss: 1.3117\n",
      "Accuracy on training set: 94.86%\n",
      "Validation Loss: 2.1731\n",
      "Accuracy on validation set: 16.87%\n",
      "\n",
      "Epoch [10/20], Training Loss: 1.2378\n",
      "Accuracy on training set: 95.47%\n",
      "Validation Loss: 2.1712\n",
      "Accuracy on validation set: 18.07%\n",
      "\n",
      "Epoch [11/20], Training Loss: 1.1545\n",
      "Accuracy on training set: 97.58%\n",
      "Validation Loss: 2.1393\n",
      "Accuracy on validation set: 21.69%\n",
      "\n",
      "Epoch [12/20], Training Loss: 1.1234\n",
      "Accuracy on training set: 97.89%\n",
      "Validation Loss: 2.1273\n",
      "Accuracy on validation set: 24.10%\n",
      "\n",
      "Epoch [13/20], Training Loss: 1.0686\n",
      "Accuracy on training set: 98.79%\n",
      "Validation Loss: 2.1216\n",
      "Accuracy on validation set: 28.92%\n",
      "\n",
      "Epoch [14/20], Training Loss: 0.9813\n",
      "Accuracy on training set: 98.49%\n",
      "Validation Loss: 2.1199\n",
      "Accuracy on validation set: 24.10%\n",
      "\n",
      "Epoch [15/20], Training Loss: 0.9446\n",
      "Accuracy on training set: 99.09%\n",
      "Validation Loss: 2.1360\n",
      "Accuracy on validation set: 22.89%\n",
      "\n",
      "Epoch [16/20], Training Loss: 0.9229\n",
      "Accuracy on training set: 98.79%\n",
      "Validation Loss: 2.1072\n",
      "Accuracy on validation set: 21.69%\n",
      "\n",
      "Epoch [17/20], Training Loss: 0.8929\n",
      "Accuracy on training set: 99.40%\n",
      "Validation Loss: 2.1052\n",
      "Accuracy on validation set: 22.89%\n",
      "\n",
      "Epoch [18/20], Training Loss: 0.8820\n",
      "Accuracy on training set: 99.70%\n",
      "Validation Loss: 2.0968\n",
      "Accuracy on validation set: 27.71%\n",
      "\n",
      "Epoch [19/20], Training Loss: 0.8338\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 2.1072\n",
      "Accuracy on validation set: 24.10%\n",
      "\n",
      "Epoch [20/20], Training Loss: 0.8010\n",
      "Accuracy on training set: 100.00%\n",
      "Validation Loss: 2.0956\n",
      "Accuracy on validation set: 25.30%\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [1/20], Training Loss: 2.3262\n",
      "Accuracy on training set: 32.33%\n",
      "Validation Loss: 2.1872\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Epoch [2/20], Training Loss: 2.0696\n",
      "Accuracy on training set: 48.64%\n",
      "Validation Loss: 2.1708\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Epoch [3/20], Training Loss: 1.8918\n",
      "Accuracy on training set: 50.45%\n",
      "Validation Loss: 2.1811\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [4/20], Training Loss: 1.8160\n",
      "Accuracy on training set: 69.79%\n",
      "Validation Loss: 2.2009\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [5/20], Training Loss: 1.6566\n",
      "Accuracy on training set: 74.02%\n",
      "Validation Loss: 2.1810\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Epoch [6/20], Training Loss: 1.5850\n",
      "Accuracy on training set: 73.41%\n",
      "Validation Loss: 2.1751\n",
      "Accuracy on validation set: 13.25%\n",
      "\n",
      "Epoch [7/20], Training Loss: 1.4985\n",
      "Accuracy on training set: 80.06%\n",
      "Validation Loss: 2.1544\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Epoch [8/20], Training Loss: 1.4355\n",
      "Accuracy on training set: 86.71%\n",
      "Validation Loss: 2.1500\n",
      "Accuracy on validation set: 14.46%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/20], Training Loss: 2.3850\n",
      "Accuracy on training set: 22.05%\n",
      "Validation Loss: 2.2574\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/20], Training Loss: 2.3629\n",
      "Accuracy on training set: 32.02%\n",
      "Validation Loss: 2.3081\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/20], Training Loss: 2.3540\n",
      "Accuracy on training set: 23.49%\n",
      "Validation Loss: 2.3264\n",
      "Accuracy on validation set: 12.20%\n",
      "\n",
      "Early stopping\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot is already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = standardized_data.view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define a simpler CNN model\n",
    "class SimpleCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc1 = nn.Linear(32 * 1920, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.5)  # Moderate dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 32 * 1920)\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "num_epochs = 20\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = SimpleCNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd152893-8d1c-4c40-a09e-8614c4231b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/20], Training Loss: 2.2689\n",
      "Accuracy on training set: 25.68%\n",
      "Validation Loss: 2.2302\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [2/20], Training Loss: 2.0621\n",
      "Accuracy on training set: 33.84%\n",
      "Validation Loss: 2.2448\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [3/20], Training Loss: 1.9216\n",
      "Accuracy on training set: 48.04%\n",
      "Validation Loss: 2.2373\n",
      "Accuracy on validation set: 6.02%\n",
      "\n",
      "Epoch [4/20], Training Loss: 1.7779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 146\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 146\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m    148\u001b[0m         train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 46\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m960\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn5(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)))\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming y_one_hot is already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = standardized_data.view(414, 14, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the CNN model with increased dropout\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=14, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc1 = nn.Linear(128 * 960, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 960)\n",
    "        x = torch.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "num_epochs = 20\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = '../Models/raw_data_cnn.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = CNNModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcd6cc-ff09-4d0f-b14f-7506b551029f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a08a5-4a4a-4b8d-983e-9e65235cd6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d793a-b86a-4ce8-be7b-16cbda9a3c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fbc3a-8938-41b9-a475-0b0966cb23e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a21696-89b6-453b-8527-e4ae61e62192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4f0b2-e1ff-4442-a053-18c091be6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda81f9d-6ccc-4e08-b023-81a54975b899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48847a88-eef7-4c07-8736-86be2861b4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ce2f3-f38e-47c7-b495-6c1d77d12a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d115cd-7003-49ee-b5f1-4d23d834cea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a052f-b5ae-4eee-a187-30882eb26d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237fde0-8edc-42cb-918c-9aa3ae634759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77ff8b00-98db-4484-ae82-f45cd69ddc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened data shape: (414, 107520)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data creation for illustration\n",
    "# X_imf should be replaced with your actual dataframe\n",
    "# Assuming X_imf is a dataframe with lists of 7680 elements\n",
    "\n",
    "# Function to flatten columns containing lists into a 2D array\n",
    "def flatten_columns(df):\n",
    "    return np.array(df.iloc[:, :len(df.columns)].values.tolist()).reshape(df.shape[0], -1)\n",
    "\n",
    "flattened_data = flatten_columns(X)\n",
    "\n",
    "# Verify the shape of the flattened data\n",
    "print(f'Flattened data shape: {flattened_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc3b59dd-95f9-414e-bba4-cbd23fb3e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized data shape: (414, 107520)\n",
      "Mean of standardized data: [ 8.91664627e-18 -3.82141983e-18  2.18558397e-17 ...  6.10086324e-18\n",
      "  3.23144624e-17 -4.62592927e-18]\n",
      "Standard deviation of standardized data: [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the flattened data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(flattened_data)\n",
    "\n",
    "# Verify the shape and mean/std of standardized data\n",
    "print(f'Standardized data shape: {standardized_data.shape}')\n",
    "print(f'Mean of standardized data: {np.mean(standardized_data, axis=0)}')\n",
    "print(f'Standard deviation of standardized data: {np.std(standardized_data, axis=0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07c1eeb5-4d5e-41f4-9aec-47cac451e62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06130696,  0.04392667,  0.14352059, ...,  0.08859186,\n",
       "         0.25883853,  0.14559242],\n",
       "       [ 0.01684903,  0.10069246,  0.15104969, ...,  0.01624005,\n",
       "        -0.09056394, -0.05986159],\n",
       "       [ 0.07276537,  0.02416881,  0.02411968, ..., -0.90405031,\n",
       "        -0.70633722, -0.67074667],\n",
       "       ...,\n",
       "       [-0.16546342, -0.15852624, -0.10869809, ..., -0.10660791,\n",
       "        -0.08593933, -0.0496164 ],\n",
       "       [-0.02926027, -0.00694558,  0.04855048, ...,  0.00433739,\n",
       "        -0.02707823, -0.05355139],\n",
       "       [ 0.1326224 ,  0.12510463,  0.18145423, ..., -0.2165923 ,\n",
       "        -0.24471244, -0.21426181]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fffe2-01d8-46a4-99f0-b3dc43b77e66",
   "metadata": {},
   "source": [
    "## Creating model using raw EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77351f57-2807-48b3-ab96-70539a8d1f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best parameters found: {'bootstrap': False, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Accuracy: 0.22\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   amusement       0.27      0.38      0.32         8\n",
      "       anger       0.33      0.22      0.27         9\n",
      "    calmness       0.20      0.09      0.12        11\n",
      "     disgust       0.44      0.33      0.38        12\n",
      "  excitement       0.33      0.27      0.30        11\n",
      "        fear       0.29      0.18      0.22        11\n",
      "   happiness       0.12      0.12      0.12         8\n",
      "     sadness       0.00      0.00      0.00         8\n",
      "    surprise       0.12      0.40      0.19         5\n",
      "\n",
      "    accuracy                           0.22        83\n",
      "   macro avg       0.24      0.22      0.21        83\n",
      "weighted avg       0.25      0.22      0.22        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f'Best parameters found: {grid_search.best_params_}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_classifier = grid_search.best_estimator_\n",
    "best_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Detailed classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89e1eedf-e8ce-40be-9cd3-740c056d855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A2000 12GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "757d5601-3ef8-4da3-bb50-556d8dab24ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.9654\n",
      "Epoch [2/10], Loss: 1.7230\n",
      "Epoch [3/10], Loss: 1.0408\n",
      "Epoch [4/10], Loss: 0.2875\n",
      "Epoch [5/10], Loss: 0.1479\n",
      "Epoch [6/10], Loss: 0.1144\n",
      "Epoch [7/10], Loss: 0.0419\n",
      "Epoch [8/10], Loss: 0.0483\n",
      "Epoch [9/10], Loss: 0.0321\n",
      "Epoch [10/10], Loss: 0.0272\n",
      "tensor([[ -2.4766,   0.3382,   7.9686,  ...,  -4.3074,  -2.1992,  -6.1776],\n",
      "        [  1.6820,  -4.5621,   2.1664,  ...,   1.6001,   0.1249,   8.0359],\n",
      "        [ 12.0344,  -4.2796, -11.4532,  ...,  -4.5454,  -1.0308,  -2.1525],\n",
      "        ...,\n",
      "        [ -0.4063,  -1.0266,  -0.9628,  ...,   1.9722,  -2.7729,   1.6038],\n",
      "        [  1.2529,  -1.2286,  -2.4018,  ...,  -3.3884,   7.7119,   1.0661],\n",
      "        [  0.1546,  -3.0245,   0.3837,  ...,  -0.8957,   1.1648,   6.4990]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have standardized_data and y\n",
    "# Convert standardized_data and y to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(107520, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(standardized_data.size()[0])\n",
    "    \n",
    "    for i in range(0, standardized_data.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = standardized_data[indices], y_one_hot[indices]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(standardized_data)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5febe584-c5fc-4f74-90eb-2154a205b273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4766,  0.3382,  7.9686,  2.0732, -1.0617,  0.3879, -4.3074, -2.1992,\n",
       "        -6.1776])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c5a8f0c-0cb3-442d-b8d4-413ba033816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: tensor([2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8])\n",
      "True Classes: tensor([2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8,\n",
      "        2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3,\n",
      "        6, 1, 7, 3, 2, 0, 6, 1, 5, 4, 7, 8, 2, 8, 0, 5, 4, 3, 6, 1, 7, 3, 2, 0,\n",
      "        6, 1, 5, 4, 7, 8])\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Convert model outputs to class predictions\n",
    "predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "true_classes = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "correct_predictions = (predicted_classes == true_classes).sum().item()\n",
    "accuracy = correct_predictions / len(true_classes) * 100\n",
    "\n",
    "# Print the predicted classes, true classes, and accuracy\n",
    "print(\"Predicted Classes:\", predicted_classes)\n",
    "print(\"True Classes:\", true_classes)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d7868-ccf5-4ed1-8b31-3397ea9fa97d",
   "metadata": {},
   "source": [
    "- The model is trained on whole dataset and the accuracy for the whole training dataset is found to be 100%\n",
    "- We would want to divide the dataset into training and test data and find the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6709b966-9a0a-47b6-a2fe-04898e422f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.6666\n",
      "Epoch [2/20], Loss: 1.9852\n",
      "Epoch [3/20], Loss: 2.1209\n",
      "Epoch [4/20], Loss: 2.1303\n",
      "Epoch [5/20], Loss: 4.2271\n",
      "Epoch [6/20], Loss: 1.8788\n",
      "Epoch [7/20], Loss: 2.5143\n",
      "Epoch [8/20], Loss: 2.6892\n",
      "Epoch [9/20], Loss: 1.8031\n",
      "Epoch [10/20], Loss: 1.4404\n",
      "Epoch [11/20], Loss: 2.5674\n",
      "Epoch [12/20], Loss: 1.6803\n",
      "Epoch [13/20], Loss: 1.2023\n",
      "Epoch [14/20], Loss: 1.4562\n",
      "Epoch [15/20], Loss: 1.4216\n",
      "Epoch [16/20], Loss: 1.7338\n",
      "Epoch [17/20], Loss: 1.1633\n",
      "Epoch [18/20], Loss: 1.3994\n",
      "Epoch [19/20], Loss: 1.3968\n",
      "Epoch [20/20], Loss: 1.1915\n",
      "Accuracy on test set: 13.25%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming you have standardized_data and y_one_hot\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(107520, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        true_classes = torch.argmax(batch_y, dim=1)\n",
    "        correct_predictions += (predicted_classes == true_classes).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebd2ab-f9b3-47ca-9d90-68e674aad6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming you have standardized_data and y_one_hot\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(107520, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        true_classes = torch.argmax(batch_y, dim=1)\n",
    "        correct_predictions += (predicted_classes == true_classes).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "864f42e4-3bca-432c-ad66-1b22f0214141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.1557\n",
      "Epoch [2/20], Loss: 2.5185\n",
      "Epoch [3/20], Loss: 2.1721\n",
      "Epoch [4/20], Loss: 2.0175\n",
      "Epoch [5/20], Loss: 2.0842\n",
      "Epoch [6/20], Loss: 2.1184\n",
      "Epoch [7/20], Loss: 1.9912\n",
      "Epoch [8/20], Loss: 2.1052\n",
      "Epoch [9/20], Loss: 2.0485\n",
      "Epoch [10/20], Loss: 2.0790\n",
      "Epoch [11/20], Loss: 1.9696\n",
      "Epoch [12/20], Loss: 2.1117\n",
      "Epoch [13/20], Loss: 2.1083\n",
      "Epoch [14/20], Loss: 2.0458\n",
      "Epoch [15/20], Loss: 2.1421\n",
      "Epoch [16/20], Loss: 2.2228\n",
      "Epoch [17/20], Loss: 2.0158\n",
      "Epoch [18/20], Loss: 1.8224\n",
      "Epoch [19/20], Loss: 2.1615\n",
      "Epoch [20/20], Loss: 2.1206\n",
      "Accuracy on test set: 13.25%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming you have standardized_data and y_one_hot\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model with batch normalization\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(107520, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        true_classes = torch.argmax(batch_y, dim=1)\n",
    "        correct_predictions += (predicted_classes == true_classes).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "46248aa0-6eb3-4df4-b5c6-d9917f335c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.2467\n",
      "Epoch [2/20], Loss: 2.0136\n",
      "Epoch [3/20], Loss: 2.0043\n",
      "Epoch [4/20], Loss: 2.0542\n",
      "Epoch [5/20], Loss: 2.2275\n",
      "Epoch [6/20], Loss: 2.1046\n",
      "Epoch [7/20], Loss: 2.0746\n",
      "Epoch [8/20], Loss: 2.2253\n",
      "Epoch [9/20], Loss: 2.1439\n",
      "Epoch [10/20], Loss: 2.0780\n",
      "Epoch [11/20], Loss: 2.0387\n",
      "Epoch [12/20], Loss: 2.1048\n",
      "Epoch [13/20], Loss: 1.9629\n",
      "Epoch [14/20], Loss: 1.9249\n",
      "Epoch [15/20], Loss: 2.2909\n",
      "Epoch [16/20], Loss: 2.0124\n",
      "Epoch [17/20], Loss: 1.8794\n",
      "Epoch [18/20], Loss: 2.1400\n",
      "Epoch [19/20], Loss: 2.3873\n",
      "Epoch [20/20], Loss: 2.0613\n",
      "Accuracy on test set: 10.84%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming you have standardized_data and y_one_hot\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model with batch normalization\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(107520, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(batch_y, 1)[1])\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        true_classes = torch.argmax(batch_y, dim=1)\n",
    "        correct_predictions += (predicted_classes == true_classes).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2030306-b835-4ff8-acf4-bcef9ecd2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have standardized_data and y_one_hot\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1bb51c18-bf1e-45eb-bfdf-45edbfdc8716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([414, 1, 224, 480])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(standardized_data, dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7bcc7d8d-8e06-4751-ae4c-908f005aefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Assuming the data is 1D and needs to be reshaped to 2D (e.g., 224x480)\n",
    "standardized_data = standardized_data.view(-1, 1, 224, 480)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 112 * 240, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 112 * 240)\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Debug prints to check the shapes\n",
    "        print(f'batch_x shape: {batch_x.shape}')\n",
    "        print(f'batch_y shape: {batch_y.shape}')\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bf61ab23-adc6-4029-b884-1a907620d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1 and pool: torch.Size([32, 32, 112, 240])\n",
      "After conv2 and pool: torch.Size([32, 64, 56, 120])\n",
      "After view: torch.Size([32, 430080])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x430080 and 1720320x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 46\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m56\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m120\u001b[39m)  \u001b[38;5;66;03m# Adjust this based on the output shape\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter view: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)))\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x430080 and 1720320x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Assuming the data is 1D and needs to be reshaped to 2D (e.g., 224x480)\n",
    "standardized_data = standardized_data.view(-1, 1, 224, 480)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 112 * 240, 128)  # Adjust this based on the output shape\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        print(f'After conv1 and pool: {x.shape}')\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        print(f'After conv2 and pool: {x.shape}')\n",
    "        x = x.view(-1, 64 * 56 * 120)  # Adjust this based on the output shape\n",
    "        print(f'After view: {x.shape}')\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b409b4c-6b8d-4264-8731-1798e099b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [1/20], Loss: 2.1431\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [2/20], Loss: 2.1206\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [3/20], Loss: 2.2390\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [4/20], Loss: 1.9069\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [5/20], Loss: 2.1098\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [6/20], Loss: 1.7928\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [7/20], Loss: 2.0534\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [8/20], Loss: 1.5493\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [9/20], Loss: 1.6817\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [10/20], Loss: 1.7166\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [11/20], Loss: 1.3832\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [12/20], Loss: 1.5281\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [13/20], Loss: 1.6297\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [14/20], Loss: 1.5545\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [15/20], Loss: 1.6196\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [16/20], Loss: 1.5088\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [17/20], Loss: 1.5421\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [18/20], Loss: 1.2538\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [19/20], Loss: 1.6364\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([32, 1, 224, 480])\n",
      "batch_y shape: torch.Size([32])\n",
      "batch_x shape: torch.Size([11, 1, 224, 480])\n",
      "batch_y shape: torch.Size([11])\n",
      "Epoch [20/20], Loss: 1.4248\n",
      "Accuracy on test set: 19.28%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Assuming the data is 1D and needs to be reshaped to 2D (e.g., 224x480)\n",
    "standardized_data = standardized_data.view(-1, 1, 224, 480)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 120, 128)  # Updated input size\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 56 * 120)  # Updated view size\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Debug prints to check the shapes\n",
    "        print(f'batch_x shape: {batch_x.shape}')\n",
    "        print(f'batch_y shape: {batch_y.shape}')\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "37e2e8ea-2c62-4554-8e74-9a4c2ec47130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2999\n",
      "Epoch [2/10], Loss: 2.1989\n",
      "Epoch [3/10], Loss: 2.1102\n",
      "Epoch [4/10], Loss: 2.0589\n",
      "Epoch [5/10], Loss: 2.0783\n",
      "Epoch [6/10], Loss: 1.9289\n",
      "Epoch [7/10], Loss: 1.9378\n",
      "Epoch [8/10], Loss: 1.9206\n",
      "Epoch [9/10], Loss: 1.8483\n",
      "Epoch [10/10], Loss: 1.7778\n",
      "Accuracy on test set: 9.64%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Assuming the data is 1D and needs to be reshaped to 2D (e.g., 224x480)\n",
    "standardized_data = standardized_data.view(-1, 1, 224, 480)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Increased batch size\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 120, 128)  # Updated input size\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 56 * 120)  # Updated view size\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Reduced learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecebd31-6580-4e3c-a070-366bdbc2f147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion-Recognition-kuwpk_hL",
   "language": "python",
   "name": "emotion-recognition-kuwpk_hl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
