{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12ee539-ae2c-4ed2-a8fb-cb145aab0dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is:---> (1, 2, 24)\n",
      "Shape of W is: -------> (24, 1)\n",
      "et value is [[-1. -1.]]\n",
      "Step 1: Calculate tanh(dot(x, W) + b)\n",
      "Result shape: (1, 2)\n",
      "Weights (W):\n",
      "[[ 0.98674273]\n",
      " [-0.17824568]\n",
      " [-2.0293376 ]\n",
      " [ 0.19803815]\n",
      " [ 0.79331344]\n",
      " [ 0.25545767]\n",
      " [-1.774217  ]\n",
      " [ 1.3488773 ]\n",
      " [-0.09669409]\n",
      " [-1.4060137 ]\n",
      " [ 3.1828892 ]\n",
      " [ 0.5232729 ]\n",
      " [-1.8090416 ]\n",
      " [ 0.31025654]\n",
      " [-1.567651  ]\n",
      " [-2.2930984 ]\n",
      " [ 2.0591543 ]\n",
      " [-0.25074744]\n",
      " [-1.0286402 ]\n",
      " [-0.28456938]\n",
      " [ 0.49673414]\n",
      " [-0.05148069]\n",
      " [ 1.0968968 ]\n",
      " [ 0.2238658 ]]\n",
      "Bias (b):\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "Step 2: Calculate softmax\n",
      "Result shape: (1, 2)\n",
      "Attention weights:\n",
      "[[0.5 0.5]]\n",
      "\n",
      "Step 3: Expand dimensions of attention weights\n",
      "Result shape: (1, 2, 1)\n",
      "\n",
      "Step 4: Multiply input by attention weights\n",
      "Result shape: (1, 2, 24)\n",
      "\n",
      "Step 5: Sum along axis 1\n",
      "Final output shape: (1, 24)\n",
      "\n",
      "Input data:\n",
      "[[[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "   24]\n",
      "  [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "   24]]]\n",
      "\n",
      "Output data:\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      "  19. 20. 21. 22. 23. 24.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.backend import squeeze, tanh, dot, expand_dims, sum, softmax\n",
    "\n",
    "# Define a more distinctive input\n",
    "input_data = np.array([[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
    "                        ]])\n",
    "\n",
    "# Define the Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=RandomNormal(mean=0.0, stddev=1.0))\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        print(\"Shape of X is:--->\", x.shape)\n",
    "        print(\"Shape of W is: ------->\", self.W.shape)\n",
    "        et = squeeze(tanh(dot(x, self.W) + self.b), axis=-1)\n",
    "        print(\"et value is\", et.numpy())\n",
    "        print(\"Step 1: Calculate tanh(dot(x, W) + b)\")\n",
    "        print(\"Result shape:\", et.shape)\n",
    "        print(\"Weights (W):\")\n",
    "        print(self.W.numpy())\n",
    "        print(\"Bias (b):\")\n",
    "        print(self.b.numpy())\n",
    "        \n",
    "        at = softmax(et)\n",
    "        print(\"\\nStep 2: Calculate softmax\")\n",
    "        print(\"Result shape:\", at.shape)\n",
    "        print(\"Attention weights:\")\n",
    "        print(at.numpy())\n",
    "        \n",
    "        at = expand_dims(at, axis=-1)\n",
    "        print(\"\\nStep 3: Expand dimensions of attention weights\")\n",
    "        print(\"Result shape:\", at.shape)\n",
    "        \n",
    "        output = x * at\n",
    "        print(\"\\nStep 4: Multiply input by attention weights\")\n",
    "        print(\"Result shape:\", output.shape)\n",
    "        \n",
    "        output = sum(output, axis=1)\n",
    "        print(\"\\nStep 5: Sum along axis 1\")\n",
    "        print(\"Final output shape:\", output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "# Create an instance of the AttentionLayer\n",
    "attention_layer = AttentionLayer()\n",
    "\n",
    "# Apply the AttentionLayer to the input data\n",
    "output_data = attention_layer(input_data)\n",
    "\n",
    "print(\"\\nInput data:\")\n",
    "print(input_data)\n",
    "print(\"\\nOutput data:\")\n",
    "print(output_data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975b1d13-5a01-44e5-bb5a-87ea2be110f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (59, 1, 637) (59,)\n",
      "Testing data shape :  (60, 1, 637) (60,)\n",
      "Total number of outputs :  2\n",
      "Output classes :  [0. 1.]\n",
      "(59, 1, 637, 1) (60, 1, 637, 1)\n",
      "(47, 1, 637, 1)\n",
      "(12, 1, 637, 1)\n",
      "(47, 2)\n",
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">660</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_3               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">637</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">62,400</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7644</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)              │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7668</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ attention_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">92,028</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │              \u001b[38;5;34m60\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │             \u001b[38;5;34m660\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_3               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m637\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │ max_pooling2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │          \u001b[38;5;34m62,400\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7644\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                │              \u001b[38;5;34m25\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)              │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7668\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ attention_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                │          \u001b[38;5;34m92,028\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                │             \u001b[38;5;34m156\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │              \u001b[38;5;34m26\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,355</span> (606.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m155,355\u001b[0m (606.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,355</span> (606.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m155,355\u001b[0m (606.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Train...\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Exception encountered when calling AttentionLayer.call().\n\n\u001b[1mname 'K' is not defined\u001b[0m\n\nArguments received by AttentionLayer.call():\n  • x=tf.Tensor(shape=(None, 1, 24), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 151\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    154\u001b[0m test_eval \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([test_X, test_X], Y_test_one_hot, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Workspace\\my_venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mAttentionLayer.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 20\u001b[0m     et \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(K\u001b[38;5;241m.\u001b[39mtanh(K\u001b[38;5;241m.\u001b[39mdot(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m     at \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msoftmax(et)\n\u001b[0;32m     22\u001b[0m     at \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mexpand_dims(at, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: Exception encountered when calling AttentionLayer.call().\n\n\u001b[1mname 'K' is not defined\u001b[0m\n\nArguments received by AttentionLayer.call():\n  • x=tf.Tensor(shape=(None, 1, 24), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, LSTM, Bidirectional, concatenate, Reshape, Layer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Custom Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def readData(filename, readX, readY):\n",
    "    file = open(filename, \"r\")\n",
    "    for line in file:\n",
    "        line.rstrip()\n",
    "        temp = line.split(\",\")\n",
    "        readX.append(float(temp[0]))\n",
    "        dataSet = [float(t.rstrip()) for t in temp[1:]]\n",
    "        readY.append(dataSet)\n",
    "    file.close()\n",
    "\n",
    "def initiate(a, c, d):\n",
    "    s = len(a)\n",
    "    t = len(a[1])\n",
    "    i = 1\n",
    "    var = []\n",
    "    x = s - 1\n",
    "    shape = (c, d)\n",
    "    for x in range(0, x + 1):\n",
    "        iarray = np.empty([1, t])\n",
    "        iarray = np.array(a[x])\n",
    "        imatrix = np.reshape(iarray, shape)\n",
    "        var.append(imatrix)\n",
    "    varar = np.asarray(var)\n",
    "    return varar\n",
    "\n",
    "def initiatec(list):\n",
    "    narray = np.asarray(list)\n",
    "    return narray\n",
    "\n",
    "trainingX = []\n",
    "trainingY = []\n",
    "testX = []\n",
    "testY = []\n",
    "\n",
    "readData(\"Datasets/Lightning2_NOISE/Lightning2_TRAIN_NOISE50.csv\", trainingX, trainingY)\n",
    "readData(\"Datasets/Lightning2_NOISE/Lightning2_TEST_NOISE50.csv\", testX, testY)\n",
    "\n",
    "X_train = initiate(trainingY, 1, 637)\n",
    "Y_train = initiatec(trainingX)\n",
    "\n",
    "X_test = initiate(testY, 1, 637)\n",
    "Y_test = initiatec(testX)\n",
    "\n",
    "print('Training data shape : ', X_train.shape, Y_train.shape)\n",
    "print('Testing data shape : ', X_test.shape, Y_test.shape)\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "nClasses = len(classes)\n",
    "\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    "\n",
    "train_X = X_train.reshape(-1, 1, 637, 1)\n",
    "test_X = X_test.reshape(-1, 1, 637, 1)\n",
    "\n",
    "print(train_X.shape, test_X.shape)\n",
    "\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "\n",
    "Y_train_one_hot = pd.get_dummies(Y_train)\n",
    "Y_test_one_hot = pd.get_dummies(Y_test)\n",
    "\n",
    "train_X, valid_X, train_label, valid_label = train_test_split(train_X, Y_train_one_hot, test_size=0.2, random_state=13)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(valid_X.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "\n",
    "batchsize = 2\n",
    "epochs = 220\n",
    "num_classes = 2\n",
    "\n",
    "# Define the input shape for Conv2D layers\n",
    "conv_input_shape = (1, 637, 1)\n",
    "\n",
    "# Define the input layer for Conv2D layers\n",
    "conv_input_layer = Input(shape=conv_input_shape)\n",
    "\n",
    "# Adding Conv2D and MaxPooling2D layers for spatial features\n",
    "conv_layer = Conv2D(6, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(conv_input_layer)\n",
    "conv_layer = MaxPooling2D((2, 2), padding='same', strides=(1,1))(conv_layer)\n",
    "conv_layer = Dropout(0.5)(conv_layer)\n",
    "conv_layer = Conv2D(12, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')(conv_layer)\n",
    "conv_layer = MaxPooling2D(pool_size=(2, 2), padding='same', strides=(1,1))(conv_layer)\n",
    "conv_layer = Dropout(0.6)(conv_layer)\n",
    "conv_output = Flatten()(conv_layer)\n",
    "\n",
    "# Define the input shape for BiLSTM layer\n",
    "lstm_input_shape = (1, 637)\n",
    "\n",
    "# Define the input layer for BiLSTM layer\n",
    "lstm_input_layer = Input(shape=lstm_input_shape)\n",
    "\n",
    "# Adding BiLSTM layer with attention\n",
    "lstm_layer = Bidirectional(LSTM(12, return_sequences=True))(lstm_input_layer)\n",
    "\n",
    "attention_output = AttentionLayer()(lstm_layer)\n",
    "\n",
    "# Concatenate the output of Conv2D and attention layer\n",
    "concatenated_layers = concatenate([conv_output, attention_output])\n",
    "\n",
    "# Dense layers\n",
    "dense_layer = Dense(12, activation='sigmoid')(concatenated_layers)\n",
    "dense_layer = Dense(12, activation='sigmoid')(dense_layer)\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Define the model with both input branches and output\n",
    "model = Model(inputs=[conv_input_layer, lstm_input_layer], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, verbose=1)\n",
    "\n",
    "print('Train...')\n",
    "# Train the model with early stopping\n",
    "train = model.fit([train_X, train_X], train_label, batch_size=batchsize, epochs=epochs, verbose=1, validation_data=([valid_X, valid_X], valid_label),callbacks=[early_stopping] )\n",
    "\n",
    "# Evaluate the model\n",
    "test_eval = model.evaluate([test_X, test_X], Y_test_one_hot, verbose=1)\n",
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])\n",
    "\n",
    "# Predict the classes for test data\n",
    "predicted_classes = model.predict([test_X, test_X])\n",
    "\n",
    "# Convert predictions to one-hot encoded format\n",
    "predicted_classes_one_hot = pd.get_dummies(np.argmax(predicted_classes, axis=1))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_test_one_hot.values.argmax(axis=1), predicted_classes_one_hot.values.argmax(axis=1))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mtx)\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "print(classification_report(Y_test_one_hot, predicted_classes_one_hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953e72e-6b4c-4f93-9c53-6c1780b6e9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-KERNEL",
   "language": "python",
   "name": "dl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
