{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense\n",
    "import sys\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense, Dropout, Reshape, Attention, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense, Dropout, Reshape, Flatten, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense, Dropout, Attention, Concatenate, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Extracted_Features/Final_Arousal_Features.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    final_arousal_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Extracted_Features/Final_Valence_Features.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    final_valence_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_valence_df.drop([\"Arousal\",\"Gender\", \"participant\", \"video\", \"Video_Name\", \"Age\"], inplace=True, axis=1)\n",
    "final_arousal_df.drop([\"Valence\", \"Gender\", \"participant\", \"video\", \"Video_Name\", \"Age\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_valence_df.drop(columns=['Valence', 'Target_Emotion'])  # Drop the target columns\n",
    "y = final_valence_df['Valence']  # Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13380\\1935069952.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Model training with Random Forest Regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         raise ValueError(\n\u001b[0;32m   1298\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m requires y to be passed, but the target y is None\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training with Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.8003421686746985\n",
      "R^2 Score: -0.03752360716078251\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_valence_df.drop(columns=['Valence', 'Target_Emotion'])  # Drop the target columns\n",
    "y = final_valence_df['Target_Emotion']  # Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_flattened' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the data into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX_flattened\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Model training with Random Forest Regressor\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_flattened' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training with Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Conv1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the features (X) and target (y)\n",
    "X = final_valence_df.drop(columns=['Valence', 'Target_Emotion'])  # Drop the target columns\n",
    "y = final_valence_df['Valence']  # Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_valence_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns = [col for col in final_valence_df.columns if isinstance(final_valence_df[col].iloc[0], list)]\n",
    "len(list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_len(x):\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    else:\n",
    "        return 0  # Or any other default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of lists: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5334/1240711617.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  max_length = final_valence_df[list_columns].applymap(list_len).max().max()\n"
     ]
    }
   ],
   "source": [
    "max_length = final_valence_df[list_columns].applymap(list_len).max().max()\n",
    "print(\"Maximum length of lists:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(lst, max_length):\n",
    "    return np.pad(lst, (0, max_length - len(lst)), 'constant')\n",
    "\n",
    "for col in list_columns:\n",
    "    final_valence_df[col] = final_valence_df[col].apply(lambda x: pad_list(x, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_3d_array(df, columns):\n",
    "    return np.stack(df[columns].apply(lambda row: np.stack(row.values), axis=1).values)\n",
    "\n",
    "# Prepare the data\n",
    "X = convert_to_3d_array(final_valence_df, list_columns)\n",
    "y = final_valence_df['Valence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 140, 640)\n",
      "(414,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)  # Should be (number_of_samples, number_of_columns, max_length)\n",
    "print(y.shape)  # Should be (number_of_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_time_steps, num_features, _rnn_nb, _fc_nb, dropout_rate=0.5, l2_reg=1e-4):\n",
    "    spec_start = Input((num_time_steps, num_features))\n",
    "\n",
    "    spec_x = spec_start\n",
    "\n",
    "    for _r in _rnn_nb:\n",
    "        spec_x = GRU(_r, activation='tanh', dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True, kernel_regularizer=l2(l2_reg))(spec_x)\n",
    "        spec_x = Dropout(dropout_rate)(spec_x)\n",
    "\n",
    "    attention = Attention()([spec_x, spec_x])\n",
    "\n",
    "    attended = Attention()([spec_x, attention])\n",
    "\n",
    "    spec_x = Concatenate(axis=-1)([spec_x, attended])\n",
    "\n",
    "    for _f in _fc_nb:\n",
    "        spec_x = TimeDistributed(Dense(_f, activation='relu', kernel_regularizer=l2(l2_reg)))(spec_x)\n",
    "        spec_x = Dropout(dropout_rate)(spec_x)\n",
    "\n",
    "    out = TimeDistributed(Dense(1, activation='linear'))(spec_x)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(1, activation='linear')(out)\n",
    "\n",
    "    _model = Model(inputs=spec_start, outputs=out)\n",
    "    _model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    _model.summary()\n",
    "\n",
    "    return _model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">135,552</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">74,496</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">141</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m640\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │    \u001b[38;5;34m135,552\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m74,496\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m16,448\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │         \u001b[38;5;34m65\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m141\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,702</span> (885.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m226,702\u001b[0m (885.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,702</span> (885.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m226,702\u001b[0m (885.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 14:34:22.328973: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 148377600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 360ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 351ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 350ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 346ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 348ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 349ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 343ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 346ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 342ms/step - loss: nan - mean_squared_error: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 344ms/step - loss: nan - mean_squared_error: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x718db6b8d4f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "num_time_steps = len(list_columns)  # Number of feature columns\n",
    "num_features = max_length  # Length of each list after padding\n",
    "model = get_model(num_time_steps, num_features, _rnn_nb=[64, 128], _fc_nb=[64])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ML models and CNN model - working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('./Extracted_Features/Final_Valence_Features.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming 'final_df' is already loaded\n",
    "\n",
    "# Define the target variable\n",
    "target = 'Target_Emotion'\n",
    "\n",
    "# Extract DWT features\n",
    "dwt_features = [col for col in final_df.columns if col.endswith(('_alpha', '_beta', '_gamma', '_theta'))]\n",
    "\n",
    "# Extract IMF features\n",
    "imf_features = [col for col in final_df.columns if col.endswith(tuple([f'_IMF_{i}' for i in range(1, 7)]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "final_df[target] = label_encoder.fit_transform(final_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for DWT model\n",
    "X_dwt = final_df[dwt_features]\n",
    "y_dwt = final_df[target]\n",
    "\n",
    "# Prepare the data for IMF model\n",
    "X_imf = final_df[imf_features]\n",
    "y_imf = final_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_imf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_imf\n",
    "y = y_imf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened data shape: (414, 645120)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data creation for illustration\n",
    "# X_imf should be replaced with your actual dataframe\n",
    "# Assuming X_imf is a dataframe with lists of 7680 elements\n",
    "\n",
    "# Function to flatten columns containing lists into a 2D array\n",
    "def flatten_columns(df):\n",
    "    return np.array(df.iloc[:, :len(df.columns)].values.tolist()).reshape(df.shape[0], -1)\n",
    "\n",
    "flattened_data = flatten_columns(X)\n",
    "\n",
    "# Verify the shape of the flattened data\n",
    "print(f'Flattened data shape: {flattened_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = flattened_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier - doesn't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m     18\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mclassifier, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Print the best parameters found by GridSearchCV\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f'Best parameters found: {grid_search.best_params_}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_classifier = grid_search.best_estimator_\n",
    "best_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Detailed classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A2000 12GB\n",
      "Memory Usage:\n",
      "Allocated: 9.5 GB\n",
      "Cached:    13.2 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = np.eye(9)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: actually standardize the data instead of directly giving the flattened_data as input\n",
    "#### If required maybe try with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized data shape: (414, 645120)\n",
      "Mean of standardized data: [ 2.54426110e-17 -1.30732784e-18  6.13438447e-18 ... -4.34770309e-17\n",
      "  2.98338917e-18 -5.19076194e-17]\n",
      "Standard deviation of standardized data: [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the flattened data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(flattened_data)\n",
    "\n",
    "# Verify the shape and mean/std of standardized data\n",
    "print(f'Standardized data shape: {standardized_data.shape}')\n",
    "print(f'Mean of standardized data: {np.mean(standardized_data, axis=0)}')\n",
    "print(f'Standard deviation of standardized data: {np.std(standardized_data, axis=0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 645120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardized_data = flattened_data\n",
    "standardized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([414, 645120])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming imf_data is a 2D array with shape (num_samples, 480 * 1344)\n",
    "imf_data = flattened_data.reshape(-1, 480, 1344)\n",
    "\n",
    "# Standardize the reshaped imf_data\n",
    "mean = np.mean(imf_data, axis=(1, 2), keepdims=True)\n",
    "std = np.std(imf_data, axis=(1, 2), keepdims=True)\n",
    "standardized_data = (imf_data - mean) / std\n",
    "\n",
    "# print(mean)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.14285791e-02, -9.20818087e-03,  9.57107101e-04, ...,\n",
       "         -3.83366686e-03, -1.81870793e-02, -4.25896066e-03],\n",
       "        [ 8.81835499e-03, -4.38215242e-03, -1.78676451e-02, ...,\n",
       "         -4.33882614e-03,  5.99797174e-03, -1.16609015e-03],\n",
       "        [-1.54028476e-02, -1.54133683e-02, -9.49912479e-04, ...,\n",
       "         -5.25398297e-03, -1.66127335e-02, -9.42453598e-03],\n",
       "        ...,\n",
       "        [ 3.40660250e-02,  3.75104981e-02,  4.05853182e-02, ...,\n",
       "         -2.76282933e-02, -3.16879660e-02, -3.55028474e-02],\n",
       "        [-3.90182830e-02, -4.21775001e-02, -4.49213103e-02, ...,\n",
       "          4.10069033e-02,  4.02491925e-02,  3.94094679e-02],\n",
       "        [ 3.84925122e-02,  3.75031079e-02,  3.64460373e-02, ...,\n",
       "          1.00913464e+01,  1.02461678e+01,  1.04025547e+01]],\n",
       "\n",
       "       [[ 5.90709983e-03,  2.20047229e-02,  1.13996116e-02, ...,\n",
       "         -3.99006010e-03, -2.28751864e-02, -7.95115460e-03],\n",
       "        [ 1.81754181e-02,  1.45953765e-02, -1.17362244e-02, ...,\n",
       "         -1.17520268e-02, -2.36947974e-02,  3.97269691e-03],\n",
       "        [ 1.84812875e-02, -4.90122885e-03, -2.34742837e-02, ...,\n",
       "         -1.05926164e-02,  1.79700523e-02,  1.27923336e-02],\n",
       "        ...,\n",
       "        [ 8.01551319e-02,  7.23383297e-02,  6.33219687e-02, ...,\n",
       "          1.13983309e-01,  1.07133085e-01,  9.77776509e-02],\n",
       "        [ 8.63072532e-02,  7.31121400e-02,  5.85825589e-02, ...,\n",
       "         -9.74947892e-02, -9.09260745e-02, -8.39808299e-02],\n",
       "        [-7.66863973e-02, -6.90701182e-02, -6.11593344e-02, ...,\n",
       "         -1.61139047e+01, -1.63139783e+01, -1.65156817e+01]],\n",
       "\n",
       "       [[-1.07516634e-02, -1.80910250e-02, -2.30802065e-02, ...,\n",
       "         -2.36210072e-02, -1.12888800e-02, -5.71061150e-03],\n",
       "        [-1.60271688e-02, -2.46468306e-02, -1.70619105e-02, ...,\n",
       "         -6.28183210e-03, -1.13887330e-02, -2.33433344e-02],\n",
       "        [-2.41109564e-02, -1.08939015e-02, -6.35980262e-03, ...,\n",
       "         -2.43589055e-02, -1.50446045e-02, -6.17362661e-03],\n",
       "        ...,\n",
       "        [ 1.68741923e-02,  1.35036978e-02,  8.93339023e-03, ...,\n",
       "         -9.08553311e-02, -1.10695554e-01, -1.29192825e-01],\n",
       "        [-1.45983643e-01, -1.60704508e-01, -1.72991918e-01, ...,\n",
       "         -3.04365091e-02, -2.97650070e-02, -2.90505178e-02],\n",
       "        [-2.82935262e-02, -2.74945171e-02, -2.66539751e-02, ...,\n",
       "         -4.21900335e+00, -4.29414552e+00, -4.37018357e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.86730607e-02, -2.92042839e-02, -2.58973846e-03, ...,\n",
       "         -1.92091295e-03, -3.51451554e-03,  4.90611856e-03],\n",
       "        [ 4.94427548e-03, -2.35611464e-03, -3.03986671e-05, ...,\n",
       "          7.62290295e-04, -1.65148201e-03,  3.05302818e-03],\n",
       "        [ 4.06795180e-03, -2.05662012e-03,  1.70535584e-03, ...,\n",
       "          2.40153338e-03, -3.10173758e-03, -1.00485947e-03],\n",
       "        ...,\n",
       "        [ 6.14363453e-03,  4.90604196e-03,  3.48009755e-03, ...,\n",
       "          5.87668277e-03,  5.42360564e-03,  4.91203002e-03],\n",
       "        [ 4.35403724e-03,  3.76170860e-03,  3.14712541e-03, ...,\n",
       "          1.76117143e-03,  1.17964993e-03,  5.96077053e-04],\n",
       "        [ 2.54432055e-05, -5.17261225e-04, -1.01704584e-03, ...,\n",
       "          7.44537364e+00,  7.56520369e+00,  7.68623784e+00]],\n",
       "\n",
       "       [[-1.41757637e-01, -7.46183504e-02, -2.65161816e-02, ...,\n",
       "          1.11986887e-02, -2.61639474e-02, -1.27947517e-02],\n",
       "        [ 1.20119786e-02, -1.17055627e-02, -2.61017627e-02, ...,\n",
       "          6.50646421e-03, -1.82973250e-02, -2.12526875e-02],\n",
       "        [ 6.33935282e-03,  6.99749655e-03, -2.10641884e-02, ...,\n",
       "         -6.83514025e-03, -1.42385601e-02, -5.07854614e-03],\n",
       "        ...,\n",
       "        [-2.47409724e-02, -2.18611788e-02, -1.82021383e-02, ...,\n",
       "          3.44827240e-02,  3.17746695e-02,  2.84680343e-02],\n",
       "        [ 2.45733710e-02,  2.01181102e-02,  1.51574323e-02, ...,\n",
       "         -2.11468342e-02, -2.18244791e-02, -2.20473674e-02],\n",
       "        [-2.18302617e-02, -2.11865908e-02, -2.01295912e-02, ...,\n",
       "         -2.17854315e-01, -2.12750084e-01, -2.07279759e-01]],\n",
       "\n",
       "       [[-8.17604466e-02, -5.40907762e-02, -3.37298177e-02, ...,\n",
       "          8.13598015e-03,  9.61247368e-03, -4.41094578e-03],\n",
       "        [-1.92373532e-03,  8.53614998e-03,  7.79957422e-03, ...,\n",
       "         -7.40829816e-03,  7.58369102e-03,  1.19812847e-02],\n",
       "        [-6.90961638e-03, -3.43132118e-03,  1.11601040e-02, ...,\n",
       "          4.53504579e-03,  3.92989113e-03,  8.42144666e-04],\n",
       "        ...,\n",
       "        [ 6.17217620e-03,  7.23658575e-03,  8.03638338e-03, ...,\n",
       "          1.26280382e-02,  1.12224062e-02,  9.51775244e-03],\n",
       "        [ 7.51718792e-03,  5.22382347e-03,  2.66119110e-03, ...,\n",
       "         -7.86716053e-03, -8.62624111e-03, -9.28515169e-03],\n",
       "        [-9.83555501e-03, -1.02613351e-02, -1.05436890e-02, ...,\n",
       "         -4.13662134e+00, -4.26321527e+00, -4.39233111e+00]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = standardized_data.reshape(standardized_data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 645120)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct code for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_3d: (414, 84, 7680)\n"
     ]
    }
   ],
   "source": [
    "num_samples = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "list_length = len(X.iloc[0, 0])\n",
    "\n",
    "# Initialize a 3D NumPy array\n",
    "X_3d = np.zeros((num_samples, num_features, list_length))\n",
    "\n",
    "# Fill the 3D array with data from the DataFrame\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_features):\n",
    "        X_3d[i, j, :] = X.iloc[i, j]\n",
    "print(\"Shape of X_3d:\", X_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7680"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_3d[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(X_3d, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is no need to run the below cells for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_standardized: (414, 84, 7680)\n"
     ]
    }
   ],
   "source": [
    "# Standardize each feature (each sub-list independently)\n",
    "scalers = []\n",
    "data_standardized = np.zeros_like(X_3d)\n",
    "\n",
    "for i in range(num_features):  # Iterate over features\n",
    "    for j in range(list_length):  # Iterate over the list length\n",
    "        scaler = StandardScaler()\n",
    "        data_standardized[:, i, j] = scaler.fit_transform(X_3d[:, i, j].reshape(-1, 1)).flatten()\n",
    "        scalers.append(scaler)\n",
    "\n",
    "print(\"Shape of data_standardized:\", data_standardized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of standardized_flattened_data: (414, 645120)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the standardized data to 2D array\n",
    "standardized_data = data_standardized.reshape(num_samples, -1)\n",
    "\n",
    "print(\"Shape of standardized_flattened_data:\", standardized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor\n",
    "standardized_data = torch.tensor(standardized_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0402, -0.0142,  0.0117,  ...,  0.0693,  0.0695,  0.0697],\n",
       "        [-0.0016,  0.0111,  0.0142,  ...,  0.0307,  0.0306,  0.0304],\n",
       "        [ 0.0005, -0.0130, -0.0172,  ...,  0.0361,  0.0359,  0.0357],\n",
       "        ...,\n",
       "        [-0.0636, -0.0419, -0.0035,  ...,  0.0599,  0.0600,  0.0601],\n",
       "        [-0.0586, -0.0385, -0.0092,  ...,  0.0477,  0.0477,  0.0476],\n",
       "        [-0.0661, -0.0552, -0.0354,  ...,  0.0428,  0.0427,  0.0425]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = standardized_data.view(-1, 1, 480, 1344)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_one_hot.shape)\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([414, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot_tensor = torch.tensor(y_one_hot)\n",
    "print(y_one_hot_tensor.shape)\n",
    "y_one_hot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = y_one_hot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\1385617319.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model parameters.\n",
      "Epoch [1/20], Loss: 1.9674\n",
      "Accuracy on test set: 53.01%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Loss: 1.8193\n",
      "Accuracy on test set: 46.99%\n",
      "Epoch [3/20], Loss: 1.7001\n",
      "Accuracy on test set: 53.01%\n",
      "Epoch [4/20], Loss: 1.5849\n",
      "Accuracy on test set: 69.88%\n",
      "Saved the best model parameters.\n",
      "Epoch [5/20], Loss: 1.4314\n",
      "Accuracy on test set: 61.45%\n",
      "Epoch [6/20], Loss: 1.3666\n",
      "Accuracy on test set: 66.27%\n",
      "Epoch [7/20], Loss: 1.2555\n",
      "Accuracy on test set: 49.40%\n",
      "Epoch [8/20], Loss: 1.2036\n",
      "Accuracy on test set: 45.78%\n",
      "Epoch [9/20], Loss: 1.0853\n",
      "Accuracy on test set: 71.08%\n",
      "Saved the best model parameters.\n",
      "Epoch [10/20], Loss: 1.0539\n",
      "Accuracy on test set: 60.24%\n",
      "Epoch [11/20], Loss: 1.0039\n",
      "Accuracy on test set: 69.88%\n",
      "Epoch [12/20], Loss: 0.9066\n",
      "Accuracy on test set: 69.88%\n",
      "Epoch [13/20], Loss: 0.9255\n",
      "Accuracy on test set: 69.88%\n",
      "Epoch [14/20], Loss: 0.8829\n",
      "Accuracy on test set: 69.88%\n",
      "Epoch [15/20], Loss: 0.8493\n",
      "Accuracy on test set: 71.08%\n",
      "Epoch [16/20], Loss: 0.8100\n",
      "Accuracy on test set: 67.47%\n",
      "Epoch [17/20], Loss: 0.7775\n",
      "Accuracy on test set: 66.27%\n",
      "Epoch [18/20], Loss: 0.7754\n",
      "Accuracy on test set: 68.67%\n",
      "Epoch [19/20], Loss: 0.7561\n",
      "Accuracy on test set: 60.24%\n",
      "Epoch [20/20], Loss: 0.7318\n",
      "Accuracy on test set: 77.11%\n",
      "Saved the best model parameters.\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to 2D (e.g., 480x1344)\n",
    "# standardized_data = standardized_data.view(-1, 1, 480, 1344)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 120 * 336, 128)  # Updated input size\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 120 * 336)  # Updated view size\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # Further reduced learning rate and added L2 regularization\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/best_model.pth'\n",
    "\n",
    "# Load the best model parameters if available\n",
    "try:\n",
    "    load_model(model, model_path)\n",
    "    print(\"Loaded the best model parameters.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved model found. Starting training from scratch.\")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "            total_predictions += batch_x.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Save the model if it has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        save_model(model, model_path)\n",
    "        print(\"Saved the best model parameters.\")\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check if the model is overfitting or underfitting - Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\1993488471.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model parameters.\n",
      "Epoch [1/20], Loss: 0.9226\n",
      "Accuracy on test set: 74.70%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Loss: 0.8658\n",
      "Accuracy on test set: 83.13%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/20], Loss: 0.7320\n",
      "Accuracy on test set: 93.98%\n",
      "Saved the best model parameters.\n",
      "Epoch [4/20], Loss: 0.6738\n",
      "Accuracy on test set: 93.98%\n",
      "Epoch [5/20], Loss: 0.5753\n",
      "Accuracy on test set: 89.16%\n",
      "Epoch [6/20], Loss: 0.5235\n",
      "Accuracy on test set: 91.57%\n",
      "Epoch [7/20], Loss: 0.4850\n",
      "Accuracy on test set: 93.98%\n",
      "Epoch [8/20], Loss: 0.4939\n",
      "Accuracy on test set: 91.57%\n",
      "Epoch [9/20], Loss: 0.4383\n",
      "Accuracy on test set: 90.36%\n",
      "Epoch [10/20], Loss: 0.4091\n",
      "Accuracy on test set: 91.57%\n",
      "Epoch [11/20], Loss: 0.3864\n",
      "Accuracy on test set: 90.36%\n",
      "Epoch [12/20], Loss: 0.3822\n",
      "Accuracy on test set: 95.18%\n",
      "Saved the best model parameters.\n",
      "Epoch [13/20], Loss: 0.3436\n",
      "Accuracy on test set: 95.18%\n",
      "Epoch [14/20], Loss: 0.3226\n",
      "Accuracy on test set: 92.77%\n",
      "Epoch [15/20], Loss: 0.2940\n",
      "Accuracy on test set: 92.77%\n",
      "Epoch [16/20], Loss: 0.3079\n",
      "Accuracy on test set: 95.18%\n",
      "Epoch [17/20], Loss: 0.3218\n",
      "Accuracy on test set: 96.39%\n",
      "Saved the best model parameters.\n",
      "Epoch [18/20], Loss: 0.3194\n",
      "Accuracy on test set: 93.98%\n",
      "Epoch [19/20], Loss: 0.3197\n",
      "Accuracy on test set: 92.77%\n",
      "Epoch [20/20], Loss: 0.2921\n",
      "Accuracy on test set: 93.98%\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAGJCAYAAAD2VnIMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAACup0lEQVR4nOzdd1gUVxcG8HdBqggoIkUREbsiGluU2Av2rlgiqKiJvUeNvfdeo1Gx9xITo4iosXexd7CLWEEEQWC+P+63CwjiAguzC+/vefZhd3Z25uwAO3vm3nuuQpIkCUREREREREQkOz25AyAiIiIiIiIigUk6ERERERERkZZgkk5ERERERESkJZikExEREREREWkJJulEREREREREWoJJOhEREREREZGWYJJOREREREREpCWYpBMRERERERFpCSbpRERERERERFqCSTplC127dkWhQoXS9NoJEyZAoVBoNiAt8+jRIygUCvj4+GTqfo8dOwaFQoFjx46plqn7u8qomAsVKoSuXbtqdJvq8PHxgUKhwKNHjzJ930RElDrJnYNS831BoVBgwoQJGo2pVq1aqFWrlka3SUTyYJJOslIoFGrdEiZxROl1+vRpTJgwAR8+fJA7FCIiymDNmzeHqakpPn78+M11OnfuDENDQ7x9+zYTI0u9W7duYcKECbygS5TF5ZA7AMreNmzYkOjx+vXr4efnl2R5yZIl07WfVatWIS4uLk2vHTNmDEaOHJmu/ZP60vO7Utfp06cxceJEdO3aFZaWlomeu3v3LvT0eP2SiCir6Ny5M/7++2/s2bMHnp6eSZ6PiIjAX3/9hYYNG8LKyirN+8mM7wu3bt3CxIkTUatWrSS9zg4dOpSh+yaizMMknWT1888/J3p89uxZ+Pn5JVn+tYiICJiamqq9HwMDgzTFBwA5cuRAjhz8V8ks6fldaYKRkZGs+yciIs1q3rw5cuXKhc2bNyebpP/111/49OkTOnfunK79yP19wdDQULZ965JPnz4hZ86ccodBlCI2F5HWq1WrFsqUKYNLly6hRo0aMDU1xe+//w5AnFibNGkCe3t7GBkZwdnZGZMnT0ZsbGyibXw9zlk5lmzOnDlYuXIlnJ2dYWRkhEqVKuHChQuJXpvcGDOFQoF+/fph7969KFOmDIyMjFC6dGkcPHgwSfzHjh1DxYoVYWxsDGdnZ/zxxx9qj1s7ceIE2rVrh4IFC8LIyAgODg4YPHgwIiMjk7w/MzMzPH/+HC1btoSZmRmsra0xbNiwJMfiw4cP6Nq1KywsLGBpaQkvLy+1un1fvHgRCoUC69atS/Kcr68vFAoF/vnnHwDA48eP0adPHxQvXhwmJiawsrJCu3bt1Oqel9yYdHVjvnbtGrp27YrChQvD2NgYtra26N69e6LuixMmTMDw4cMBAE5OTqohFcrYkhuTHhgYiHbt2iFPnjwwNTXFjz/+iP379ydaRzm+fvv27Zg6dSoKFCgAY2Nj1K1bFw8ePPju+/6WZcuWoXTp0jAyMoK9vT369u2b5L3fv38fbdq0ga2tLYyNjVGgQAF06NABoaGhqnX8/Pzw008/wdLSEmZmZihevLjq/4iIKCszMTFB69at4e/vj5CQkCTPb968Gbly5ULz5s3x7t07DBs2DC4uLjAzM4O5uTkaNWqEq1evfnc/yZ3bo6KiMHjwYFhbW6v28ezZsySvVee86ePjg3bt2gEAateunWRIYHJj0kNCQuDt7Q0bGxsYGxvD1dU1yXk8Nd+JkpOaY/b582dMmDABxYoVg7GxMezs7NC6dWs8fPhQtU5cXBwWLlwIFxcXGBsbw9raGg0bNsTFixcTxZtcTZqvx/orfye3bt1Cp06dkDt3bvz0008A1PvOoPT8+XN4e3urvm86OTmhd+/eiI6ORmBgIBQKBebPn5/kdadPn4ZCocCWLVu+exyJEmLzIOmEt2/folGjRujQoQN+/vln2NjYABAnLDMzMwwZMgRmZmY4cuQIxo0bh7CwMMyePfu72928eTM+fvyIX375BQqFArNmzULr1q0RGBj43RbdkydPYvfu3ejTpw9y5cqFRYsWoU2bNnjy5Imqu9yVK1fQsGFD2NnZYeLEiYiNjcWkSZNgbW2t1vvesWMHIiIi0Lt3b1hZWeH8+fNYvHgxnj17hh07diRaNzY2Fu7u7qhSpQrmzJmDw4cPY+7cuXB2dkbv3r0BAJIkoUWLFjh58iR+/fVXlCxZEnv27IGXl9d3Y6lYsSIKFy6M7du3J1l/27ZtyJ07N9zd3QEAFy5cwOnTp9GhQwcUKFAAjx49wvLly1GrVi3cunUrVb0gUhOzn58fAgMD0a1bN9ja2uLmzZtYuXIlbt68ibNnz0KhUKB169a4d+8etmzZgvnz5yNv3rwA8M3fyatXr1CtWjVERERgwIABsLKywrp169C8eXPs3LkTrVq1SrT+jBkzoKenh2HDhiE0NBSzZs1C586dce7cObXfs9KECRMwceJE1KtXD71798bdu3exfPlyXLhwAadOnYKBgQGio6Ph7u6OqKgo9O/fH7a2tnj+/Dn++ecffPjwARYWFrh58yaaNm2KsmXLYtKkSTAyMsKDBw9w6tSpVMdERKSLOnfujHXr1mH79u3o16+favm7d+/g6+uLjh07wsTEBDdv3sTevXvRrl07ODk54dWrV/jjjz9Qs2ZN3Lp1C/b29qnab48ePbBx40Z06tQJ1apVw5EjR9CkSZMk66lz3qxRowYGDBiARYsW4ffff1cNBfzWkMDIyEjUqlULDx48QL9+/eDk5IQdO3aga9eu+PDhAwYOHJho/bR+JwoMDFTrmMXGxqJp06bw9/dHhw4dMHDgQHz8+BF+fn64ceMGnJ2dAQDe3t7w8fFBo0aN0KNHD8TExODEiRM4e/YsKlasmKrjr9SuXTsULVoU06ZNgyRJANT7zgAAL168QOXKlfHhwwf06tULJUqUwPPnz7Fz505ERESgcOHCcHNzw6ZNmzB48OBE+920aRNy5cqFFi1apCluysYkIi3St29f6es/y5o1a0oApBUrViRZPyIiIsmyX375RTI1NZU+f/6sWubl5SU5OjqqHgcFBUkAJCsrK+ndu3eq5X/99ZcEQPr7779Vy8aPH58kJgCSoaGh9ODBA9Wyq1evSgCkxYsXq5Y1a9ZMMjU1lZ4/f65adv/+fSlHjhxJtpmc5N7f9OnTJYVCIT1+/DjR+wMgTZo0KdG65cuXlypUqKB6vHfvXgmANGvWLNWymJgYqXr16hIAae3atSnGM2rUKMnAwCDRMYuKipIsLS2l7t27pxj3mTNnJADS+vXrVcuOHj0qAZCOHj2a6L0k/F2lJubk9rtlyxYJgHT8+HHVstmzZ0sApKCgoCTrOzo6Sl5eXqrHgwYNkgBIJ06cUC37+PGj5OTkJBUqVEiKjY1N9F5KliwpRUVFqdZduHChBEC6fv16kn0ltHbt2kQxhYSESIaGhlKDBg1U+5AkSVqyZIkEQFqzZo0kSZJ05coVCYC0Y8eOb257/vz5EgDp9evXKcZARJRVxcTESHZ2dlLVqlUTLV+xYoUEQPL19ZUkSZI+f/6c6DNXksR3BiMjo0TnWOX3iITnoK+/LwQEBEgApD59+iTaXqdOnSQA0vjx41XL1D1v7tixI8l5U6lmzZpSzZo1VY8XLFggAZA2btyoWhYdHS1VrVpVMjMzk8LCwhK9F3W+EyVH3WO2Zs0aCYA0b968JNuIi4uTJEmSjhw5IgGQBgwY8M11kjv2Sl8fV+XvpGPHjknWVfc7g6enp6SnpydduHDhmzH98ccfEgDp9u3bqueio6OlvHnzJvpOQaQudncnnWBkZIRu3bolWW5iYqK6//HjR7x58wbVq1dHREQE7ty5893tenh4IHfu3KrH1atXByCuCn9PvXr1VFd9AaBs2bIwNzdXvTY2NhaHDx9Gy5YtE115L1KkCBo1avTd7QOJ39+nT5/w5s0bVKtWDZIk4cqVK0nW//XXXxM9rl69eqL38u+//yJHjhyqlnUA0NfXR//+/dWKx8PDA1++fMHu3btVyw4dOoQPHz7Aw8Mj2bi/fPmCt2/fokiRIrC0tMTly5fV2ldaYk6438+fP+PNmzf48ccfASDV+024/8qVK6u6xwGAmZkZevXqhUePHuHWrVuJ1u/WrVuicYGp+ZtK6PDhw4iOjsagQYMSFbLr2bMnzM3NVd3tLSwsAIghBxEREcluS1kc76+//srwonxERNpIX18fHTp0wJkzZxJ1Id+8eTNsbGxQt25dAOL7hvIzNzY2Fm/fvlUNEUrL+QsABgwYkGj5oEGDkqyryfNmwv3b2tqiY8eOqmUGBgYYMGAAwsPD8d9//yVaP63fidQ9Zrt27ULevHmTPX8rW6137doFhUKB8ePHf3OdtPj6+xGg3neGuLg47N27F82aNUu2FV8ZU/v27WFsbIxNmzapnvP19cWbN2++W2eJKDlM0kkn5M+fP9mCKDdv3kSrVq1gYWEBc3NzWFtbqz4ME47H/ZaCBQsmeqw8Ob1//z7Vr1W+XvnakJAQREZGokiRIknWS25Zcp48eYKuXbsiT548qnHmNWvWBJD0/SnHbX0rHkCMebOzs4OZmVmi9YoXL65WPK6urihRogS2bdumWrZt2zbkzZsXderUUS2LjIzEuHHj4ODgACMjI+TNmxfW1tb48OGDWr+XhFIT87t37zBw4EDY2NjAxMQE1tbWcHJyAqDe38O39p/cvpTdCx8/fpxoeXr+pr7eL5D0fRoaGqJw4cKq552cnDBkyBD8+eefyJs3L9zd3bF06dJE79fDwwNubm7o0aMHbGxs0KFDB2zfvp0JOxFlK8rCcJs3bwYAPHv2DCdOnECHDh2gr68PQCRl8+fPR9GiRROdv65du5am85eenl6iC/pA8ucvTZ43E+6/aNGiSWYs0fT5S91j9vDhQxQvXjzF4noPHz6Evb098uTJ8/03mArK7wIJqfOd4fXr1wgLC0OZMmVS3L6lpSWaNWum+tsCRFf3/PnzJ/p+RKQuJumkExJe7VT68OEDatasiatXr2LSpEn4+++/4efnh5kzZwKAWgmI8qT8Nen/45Uy6rXqiI2NRf369bF//36MGDECe/fuhZ+fn6pQytfv71vxaJqHhweOHj2KN2/eICoqCvv27UObNm0SnXT79++PqVOnon379ti+fTsOHToEPz8/WFlZZWhi2L59e6xatQq//vordu/ejUOHDqmK+WVWQprRfxfJmTt3Lq5du4bff/8dkZGRGDBgAEqXLq0qTmRiYoLjx4/j8OHD6NKlC65duwYPDw/Ur18/SWFBIqKsqkKFCihRooSqiNeWLVsgSVKiqu7Tpk3DkCFDUKNGDWzcuBG+vr7w8/ND6dKlM/Q8Itd5M6G0nr8y+5h9q0U9pfNZct8jNf2dwdPTE4GBgTh9+jQ+fvyIffv2oWPHjpzWldKEheNIZx07dgxv377F7t27UaNGDdXyoKAgGaOKly9fPhgbGydb2Vudat/Xr1/HvXv3sG7dukRTxvj5+aU5JkdHR/j7+yM8PDxRy/Tdu3fV3oaHhwcmTpyIXbt2wcbGBmFhYejQoUOidXbu3AkvLy/MnTtXtezz589qVZFPa8zv37+Hv78/Jk6ciHHjxqmW379/P8k2U9NlztHRMdnjoxxO4ejoqPa2UkO53bt376Jw4cKq5dHR0QgKCkK9evUSre/i4gIXFxeMGTMGp0+fhpubG1asWIEpU6YAAPT09FC3bl3UrVsX8+bNw7Rp0zB69GgcPXo0ybaIiLKqzp07Y+zYsbh27Ro2b96MokWLolKlSqrnd+7cidq1a2P16tWJXvfhwwdVoVF1OTo6Ii4uTtWCrJTcOUXd82Zqz1/Xrl1DXFxcokRR0+cvdY+Zs7Mzzp07hy9fvnyzEJ2zszN8fX3x7t27b7amK1v4vz42X/cMSIm63xmsra1hbm6OGzdufHebDRs2hLW1NTZt2oQqVaogIiICXbp0UTsmooR4aYd0lvKKb8IrvNHR0Vi2bJlcISWir6+PevXqYe/evXjx4oVq+YMHD3DgwAG1Xg8kfn+SJGHhwoVpjqlx48aIiYnB8uXLVctiY2OxePFitbdRsmRJuLi4YNu2bdi2bRvs7OwSXSRRxv71lffFixenqdVW3ZiTO14AsGDBgiTbVM6Pqs5Fg8aNG+P8+fM4c+aMatmnT5+wcuVKFCpUCKVKlVL3raRKvXr1YGhoiEWLFiV6T6tXr0ZoaKiqOnBYWBhiYmISvdbFxQV6enqIiooCILr0fa1cuXIAoFqHiCg7ULaajxs3DgEBAUnmRk/u/LVjxw48f/481ftS1p9ZtGhRouXJnZfUPW+m9vwVHBycaIhaTEwMFi9eDDMzM9XwufRS95i1adMGb968wZIlS5JsQ/n6Nm3aQJIkTJw48ZvrmJubI2/evDh+/Hii51Pz/U/d7wx6enpo2bIl/v77b9UUcMnFBAA5cuRAx44dsX37dvj4+MDFxQVly5ZVOyaihNiSTjqrWrVqyJ07N7y8vDBgwAAoFAps2LAhQ7sVp9aECRNw6NAhuLm5oXfv3oiNjcWSJUtQpkwZBAQEpPjaEiVKwNnZGcOGDcPz589hbm6OXbt2pXpsc0LNmjWDm5sbRo4ciUePHqFUqVLYvXt3qse7eXh4YNy4cTA2Noa3t3eSrlxNmzbFhg0bYGFhgVKlSuHMmTM4fPiwamq6jIjZ3NwcNWrUwKxZs/Dlyxfkz58fhw4dSrZnRYUKFQAAo0ePRocOHWBgYIBmzZqpvvwkNHLkSGzZsgWNGjXCgAEDkCdPHqxbtw5BQUHYtWtXhnVjs7a2xqhRozBx4kQ0bNgQzZs3x927d7Fs2TJUqlRJVXvhyJEj6NevH9q1a4dixYohJiYGGzZsgL6+Ptq0aQMAmDRpEo4fP44mTZrA0dERISEhWLZsGQoUKJCoIB4RUVbn5OSEatWq4a+//gKAJEl606ZNMWnSJHTr1g3VqlXD9evXsWnTpkQ9mtRVrlw5dOzYEcuWLUNoaCiqVasGf3//ZHvTqXveLFeuHPT19TFz5kyEhobCyMgIderUQb58+ZJss1evXvjjjz/QtWtXXLp0CYUKFcLOnTtx6tQpLFiwALly5Ur1e0qOusfM09MT69evx5AhQ3D+/HlUr14dnz59wuHDh9GnTx+0aNECtWvXRpcuXbBo0SLcv38fDRs2RFxcHE6cOIHatWurps/r0aMHZsyYgR49eqBixYo4fvw47t27p3bMqfnOMG3aNBw6dAg1a9ZEr169ULJkSbx8+RI7duzAyZMnVcVZle9x0aJFOHr0qGr4JVGaZGoteaLv+NYUbKVLl052/VOnTkk//vijZGJiItnb20u//fab5Ovr+91pvZTTd8yePTvJNvGN6Tu+Xqdv375JXvv19F2SJEn+/v5S+fLlJUNDQ8nZ2Vn6888/paFDh0rGxsbfOArxbt26JdWrV08yMzOT8ubNK/Xs2VM11VvCqUe8vLyknDlzJnl9crG/fftW6tKli2Rubi5ZWFhIXbp0UU3j9b0p2JTu378vAZAASCdPnkzy/Pv376Vu3bpJefPmlczMzCR3d3fpzp07SY6POlOwpSbmZ8+eSa1atZIsLS0lCwsLqV27dtKLFy+S/E4lSZImT54s5c+fX9LT00s09Vlyv8OHDx9Kbdu2lSwtLSVjY2OpcuXK0j///JNoHeV7+XoqtJSmikno6ynYlJYsWSKVKFFCMjAwkGxsbKTevXtL79+/Vz0fGBgode/eXXJ2dpaMjY2lPHnySLVr15YOHz6sWsff319q0aKFZG9vLxkaGkr29vZSx44dpXv37qUYExFRVrR06VIJgFS5cuUkz33+/FkaOnSoZGdnJ5mYmEhubm7SmTNnkkxvps4UbJIkSZGRkdKAAQMkKysrKWfOnFKzZs2kp0+fJjkvqXvelCRJWrVqlVS4cGFJX18/0Tn06xglSZJevXql2q6hoaHk4uKS5HyUmu9EyVH3mEmSmPZs9OjRkpOTk2RgYCDZ2tpKbdu2lR4+fKhaJyYmRpo9e7ZUokQJydDQULK2tpYaNWokXbp0KdF2vL29JQsLCylXrlxS+/btpZCQkG9+h0tuCtLUfGd4/Pix5OnpKVlbW0tGRkZS4cKFpb59+yaaclWpdOnSkp6envTs2bMUjxtRShSSpEXNjkTZRMuWLXHz5s1kx0sTERERkW4qX7488uTJA39/f7lDIR3GMelEGSwyMjLR4/v37+Pff/9FrVq15AmIiIiIiDTu4sWLCAgISFTwlygt2JJOlMHs7OzQtWtX1dzWy5cvR1RUFK5cuYKiRYvKHR4RERERpcONGzdw6dIlzJ07F2/evEFgYCCMjY3lDot0GAvHEWWwhg0bYsuWLQgODoaRkRGqVq2KadOmMUEnIiIiygJ27tyJSZMmoXjx4tiyZQsTdEo3tqQTERERERERaQmOSSciIiIiIiLSEkzSiYiIiIiIiLREthuTHhcXhxcvXiBXrlxQKBRyh0NERARJkvDx40fY29tDT4/XzzWB53siItImqTnXZ7sk/cWLF3BwcJA7DCIioiSePn2KAgUKyB1GlsDzPRERaSN1zvXZLknPlSsXAHFwzM3NZY6GiIgICAsLg4ODg+ocRenH8z0REWmT1Jzrs12SruzyZm5uzpM2ERFpFXbL1hye74mISBupc67nwDciIiIiIiIiLcEknYiIiIiIiEhLMEknIiIiIiIi0hLZbkw6EZG6JElCTEwMYmNj5Q6FdJy+vj5y5MjBMedahP/flJXxM4dItzFJJyJKRnR0NF6+fImIiAi5Q6EswtTUFHZ2djA0NJQ7lGyP/9+UHfAzh0h3yZqkHz9+HLNnz8alS5fw8uVL7NmzBy1btvzm+rt378by5csREBCAqKgolC5dGhMmTIC7u3vmBU1EWV5cXByCgoKgr68Pe3t7GBoasjWC0kySJERHR+P169cICgpC0aJFoafH0WZy4f83ZXX8zCHSfbIm6Z8+fYKrqyu6d++O1q1bf3f948ePo379+pg2bRosLS2xdu1aNGvWDOfOnUP58uUzIWIiyg6io6MRFxcHBwcHmJqayh0OZQEmJiYwMDDA48ePER0dDWNjY7lDyrb4/03ZAT9ziHSbrEl6o0aN0KhRI7XXX7BgQaLH06ZNw19//YW///6bSToRaRxbHkiT+PekXfj7oKyOf+NEukunx6THxcXh48ePyJMnzzfXiYqKQlRUlOpxWFhYZoRGRERERERElGo6fYltzpw5CA8PR/v27b+5zvTp02FhYaG6OTg4aC6AV6+ADRuA27c1t00iIiIiIiIt8+kTcPEiIElyR5L16WySvnnzZkycOBHbt29Hvnz5vrneqFGjEBoaqro9ffpUc0EMGgR4egJbtmhum0REWqZQoUJJhhul5NixY1AoFPjw4UOGxQQAPj4+sLS0zNB9EGU1tWrVwqBBg1SP1fn/VigU2Lt3b7r3rantEFHmu3oVKFcOqFQJmDFD7miyPp1M0rdu3YoePXpg+/btqFevXorrGhkZwdzcPNFNYxo0ED99fTW3TSKiNFIoFCneJkyYkKbtXrhwAb169VJ7/WrVquHly5ewsLBI0/6IKKlmzZqhYcOGyT534sQJKBQKXLt2LdXbTe3/tzomTJiAcuXKJVn+8uXLVNUiIiLt4OMD/Pgj8OCBeDxhAjsSZzSdG5O+ZcsWdO/eHVu3bkWTJk3kDUaZpF+4ALx7B6QwNp6IKKO9fPlSdX/btm0YN24c7t69q1pmZmamui9JEmJjY5Ejx/dPA9bW1qmKw9DQELa2tql6DRGlzNvbG23atMGzZ89QoECBRM+tXbsWFStWRNmyZVO93dT+f6dHdv1ciI6O5lzlpJMiI4EBA4A//xSPGzUCYmOBQ4cAb2/gxAlAX1/eGLMqWVvSw8PDERAQgICAAABAUFAQAgIC8OTJEwCiq7qnp6dq/c2bN8PT0xNz585FlSpVEBwcjODgYISGhsoRPpA/P1C6tBiYcfiwPDEQUaaQJDEWS46bumO/bG1tVTcLCwsoFArV4zt37iBXrlw4cOAAKlSoACMjI5w8eRIPHz5EixYtYGNjAzMzM1SqVAmHv/o8+7o7rEKhwJ9//olWrVrB1NQURYsWxb59+1TPf93dXdkt3dfXFyVLloSZmRkaNmyY6KJCTEwMBgwYAEtLS1hZWWHEiBHw8vJCy5YtU/V7Wr58OZydnWFoaIjixYtjw4YNCX6HEiZMmICCBQvCyMgI9vb2GDBggOr5ZcuWoWjRojA2NoaNjQ3atm2bqn2T7tKF/++mTZvC2toaPj4+iZaHh4djx44d8Pb2xtu3b9GxY0fkz58fpqamcHFxwZbvDMn7+v/7/v37qFGjBoyNjVGqVCn4+fklec2IESNQrFgxmJqaonDhwhg7diy+fPkCQPy/T5w4EVevXlX14lHG/HV39+vXr6NOnTowMTGBlZUVevXqhfDwcNXzXbt2RcuWLTFnzhzY2dnBysoKffv2Ve0rOep8pkVFRWHEiBFwcHCAkZERihQpgtWrV6uev3nzJpo2bQpzc3PkypUL1atXx8OHDwEkHS4AAC1btkTXrl0THdPJkyfD09MT5ubmqp4KKR03pb///huVKlWCsbEx8ubNi1atWgEAJk2ahDJlyiR5v+XKlcPYsWO/eTyI0iowEHBzEwm6QgFMngz88494nCsXcOYMsHSp3FFmXbIm6RcvXkT58uVV06cNGTIE5cuXx7hx4wCIViFlwg4AK1euRExMDPr27Qs7OzvVbeDAgbLEDwBwdxc/Dx2SLwYiynAREYCZmTy3iAjNvY+RI0dixowZuH37NsqWLYvw8HA0btwY/v7+uHLlCho2bIhmzZol+uxNzsSJE9G+fXtcu3YNjRs3RufOnfHu3bsUjl8E5syZgw0bNuD48eN48uQJhg0bpnp+5syZ2LRpE9auXYtTp04hLCws1WNX9+zZg4EDB2Lo0KG4ceMGfvnlF3Tr1g1Hjx4FAOzatQvz58/HH3/8gfv372Pv3r1wcXEBIM5HAwYMwKRJk3D37l0cPHgQNWrUSNX+SXfpwv93jhw54OnpCR8fH0gJMvsdO3YgNjYWHTt2xOfPn1GhQgXs378fN27cQK9evdClSxecP39erX3ExcWhdevWMDQ0xLlz57BixQqMGDEiyXq5cuWCj48Pbt26hYULF2LVqlWYP38+AMDDwwNDhw5F6dKl8fLlS7x8+RIeHh5JtvHp0ye4u7sjd+7cuHDhAnbs2IHDhw+jX79+idY7evQoHj58iKNHj2LdunXw8fFJcqEiIXU+0zw9PbFlyxYsWrQIt2/fxh9//KHqafT8+XPUqFEDRkZGOHLkCC5duoTu3bsjJiZGrWOoNGfOHLi6uuLKlSuqJDql4wYA+/fvR6tWrdC4cWNcuXIF/v7+qFy5MgCge/fuuH37Ni5cuKBa/8qVK7h27Rq6deuWqtiIvmffPuCHH4ArVwBra5HmjBkD6OkBDg7ArFlivVGjgKAgeWPNsqRsJjQ0VAIghYaGamaDBw9KEiBJBQpIUlycZrZJRLKKjIyUbt26JUVGRqqWhYeLf3U5buHhqX8Pa9eulSwsLFSPjx49KgGQ9u7d+93Xli5dWlq8eLHqsaOjozR//nzVYwDSmDFjEhybcAmAdODAgUT7ev/+vSoWANKDBw9Ur1m6dKlkY2OjemxjYyPNnj1b9TgmJkYqWLCg1KJFC7XfY7Vq1aSePXsmWqddu3ZS48aNJUmSpLlz50rFihWToqOjk2xr165dkrm5uRQWFvbN/aVXcn9XSho/N9E3j6ku/3/fvn1bAiAdPXpUtax69erSzz///M3XNGnSRBo6dKjqcc2aNaWBAweqHif8//b19ZVy5MghPX/+XPX8gQMHJADSnj17vrmP2bNnSxUqVFA9Hj9+vOTq6ppkvYTbWblypZQ7d24pPMEB2L9/v6SnpycFBwdLkiRJXl5ekqOjoxQTE6Nap127dpKHh8c3Y0lOws+0u3fvSgAkPz+/ZNcdNWqU5OTklOznhCQlPX6SJEktWrSQvLy8VI8dHR2lli1bfjeur49b1apVpc6dO39z/UaNGkm9e/dWPe7fv79Uq1atb66f0mcOUXK+fJGkESPiP5+qVZOkp0+TrhcbK0k1a4p16tVjCqSu1JzrdbJwnFapXh0wMgKePQPu3JE7GiLKIKamQHi4PDdTU829j4oVKyZ6HB4ejmHDhqFkyZKwtLSEmZkZbt++/d2W9IRjX3PmzAlzc3OEhIR8c31TU1M4OzurHtvZ2anWDw0NxatXr1QtRgCgr6+PChUqpOq93b59G25ubomWubm54fb/q9u0a9cOkZGRKFy4MHr27Ik9e/aoWsfq168PR0dHFC5cGF26dMGmTZsQockuDKTVdOX/u0SJEqhWrRrWrFkDAHjw4AFOnDgBb29vAEBsbCwmT54MFxcX5MmTB2ZmZvD19f3u/7PS7du34eDgAHt7e9WyqlWrJllv27ZtcHNzg62tLczMzDBmzBi195FwX66ursiZM6dqmZubG+Li4hLV0ihdujT0Ewx6TfjZkZzvfaYFBARAX18fNWvWTPb1AQEBqF69OgwMDFL1fr729Wct8P3jFhAQgLp1635zmz179sSWLVvw+fNnREdHY/PmzejevXu64iRSCg4G6tUDZs4UjwcNAo4dA74qgQFAtKivWgUYG4sRv2vXZmak2QOT9PQyNQWUXSLZ5Z0oy1IogJw55bkpFJp7Hwm/EAPAsGHDsGfPHkybNg0nTpxAQEAAXFxcEB0dneJ2vv4Cq1AoEBcXl6r1pUyeaNXBwQF3797FsmXLYGJigj59+qBGjRr48uULcuXKhcuXL2PLli2ws7PDuHHj4OrqmuHTyJF20KX/b29vb+zatQsfP37E2rVr4ezsrEo4Z8+ejYULF2LEiBE4evQoAgIC4O7u/t3/59Q4c+YMOnfujMaNG+Off/7BlStXMHr0aI3uI6HUftZ87zPNxMQkxf1973k9Pb0kn13JjZH/+rNWneP2vX03a9YMRkZG2LNnD/7++298+fKFtTNII06cAMqXB/77TwzD2b4dmD8fSOlaVdGiYpw6AAwZArx4kTmxZhdM0jWBU7ERkY46deoUunbtilatWsHFxQW2trZ49OhRpsZgYWEBGxubRGMtY2Njcfny5VRtp2TJkjh16lSiZadOnUKpUqVUj01MTNCsWTMsWrQIx44dw5kzZ3D9+nUAYsxvvXr1MGvWLFy7dg2PHj3CkSNH0vHOiDSvffv20NPTw+bNm7F+/Xp0794div9n+qdOnUKLFi3w888/w9XVFYULF8a9e/fU3nbJkiXx9OnTREUdz549m2id06dPw9HREaNHj0bFihVRtGhRPH78ONE6hoaGiI2N/e6+rl69ik+fPqmWnTp1Cnp6eihevLjaMX/te59pLi4uiIuLw3///Zfs68uWLYsTJ058szidtbV1ouMTGxuLGzdufDcudY5b2bJl4e/v/81t5MiRA15eXli7di3Wrl2LDh06fDexJ0qJJAFz5gC1a4uW9NKlgYsXgXbt1Hv9oEFAxYpAaCjQt6/6hTDp+5ika4KyeNyxY0BUlKyhEBGlRtGiRbF7924EBATg6tWr6NSpU4qtVBmlf//+mD59Ov766y/cvXsXAwcOxPv371XJhzqGDx8OHx8fLF++HPfv38e8efOwe/duVYE6Hx8frF69Gjdu3EBgYCA2btwIExMTODo64p9//sGiRYsQEBCAx48fY/369YiLi0tXskCUEczMzODh4YFRo0bh5cuXiaqKFy1aFH5+fjh9+jRu376NX375Ba9evVJ72/Xq1UOxYsXg5eWFq1ev4sSJExg9enSidYoWLYonT55g69atePjwIRYtWoQ9e/YkWqdQoUKqGXvevHmDqGS+G3Xu3BnGxsbw8vLCjRs3cPToUfTv3x9dunSBjY1N6g7KV/Gl9JlWqFAheHl5oXv37ti7dy+CgoJw7NgxbN++HQDQr18/hIWFoUOHDrh48SLu37+PDRs2qLrg16lTB/v378f+/ftx584d9O7dW60eN+oct/Hjx2PLli0YP348bt++jevXr2Omsu/x//Xo0QNHjhzBwYMH2dWd0iU0FGjdGhg+XEyr9vPPwLlzQGpOezlyAGvWiJ979wI7d2ZYuNkOk3RNKFMGsLMTkwmePCl3NEREaps3bx5y586NatWqoVmzZnB3d8cPP/yQ6XGMGDECHTt2hKenJ6pWrQozMzO4u7vD2NhY7W20bNkSCxcuxJw5c1C6dGn88ccfWLt2LWrVqgUAsLS0xKpVq+Dm5oayZcvi8OHD+Pvvv2FlZQVLS0vs3r0bderUQcmSJbFixQps2bIFpUuXzqB3TJR23t7eeP/+Pdzd3RONHx8zZgx++OEHuLu7o1atWrC1tU3VNIZ6enrYs2cPIiMjUblyZfTo0QNTp05NtE7z5s0xePBg9OvXD+XKlcPp06eTTAHWpk0bNGzYELVr14a1tXWy08CZmprC19cX7969Q6VKldC2bVvUrVsXS5YsSd3B+Io6n2nLly9H27Zt0adPH5QoUQI9e/ZUtehbWVnhyJEjCA8PR82aNVGhQgWsWrVK1e2+e/fu8PLygqenJ2rWrInChQujdu3a341LneNWq1Yt7NixA/v27UO5cuVQp06dJJX5ixYtimrVqqFEiRKoUqVKeg4VZWNXrwIVKojE2tAQWL4cWL9eDMFJLRcX4Pffxf1+/YC3bzUaaralkDJ7UKDMwsLCYGFhgdDQUJibm2tuw127AuvWAb/9Fl9xgYh00ufPnxEUFAQnJ6dUJYmkOXFxcShZsiTat2+PycpBbzoupb+rDDs3ZWPfOqb8/yZdJkkSihYtij59+mDIkCEprsu/dUrO2rVAnz7A58+Ao6No/U6mzmGqREWJKdtu3QK6dBEJPyWVmnM9W9I1hePSiYjS7PHjx1i1ahXu3buH69evo3fv3ggKCkKnTp3kDo2ISCu8fv0aS5YsQXBwMOdGp1SLjAR69AC6dxcJeuPGwOXL6U/QATHR1Zo1ohDmhg3AgQPp32Z2xyRdU+rXFz+vXhWVF4iISG16enrw8fFBpUqV4ObmhuvXr+Pw4cMoWbKk3KEREWmFfPnyYdKkSVi5ciVy584tdzikQx4+BKpVA1avFon0lCnA338DefJobh9VqohCcgDwyy9AWJjmtp0d5ZA7gCzD2lr087h8GfDzE309iIhILQ4ODkkqsxMRUbxsNkKVNOSvvwAvL1Eoztoa2LxZzIeeESZPFvsLDARGjQKWLs2Y/WQHTNI1yd1dJOmHDjFJJyIi+oaPHz9i7Nix2LNnD0JCQlC+fHksXLgQlSpVAgB07doV69atS/Qad3d3HDx4UI5wiUgGW7cCy5YBmpxwxMQEmD0bKFdOc9vUZtOmAcoJGqpVA7ZtAwoUyLj95cwJrFoF1K0rfncdOgDVq2fc/rIyJuma1KABMH26aEmPiwP0OJqAiIjoaz169MCNGzewYcMG2NvbY+PGjahXrx5u3bqF/PnzAwAaNmyItWvXql5jZGSk0RjYKklZnS7/jcfGAgMHAiEhmt92x47AlStAVq+ld+JEfII+aBAwaxbw/0kKMlSdOkDPniJZ9/YWI4FNTDJ+v1kNk3RNqlZNXEJ69Qq4di37XKYjIiJSU2RkJHbt2oW//voLNWrUAABMmDABf//9N5YvX44pU6YAEEm5ra2txvevnEorIiICJvzmSFlYREQEgPi/eV1y6pRI0C0tgT//FOOo0ysuDujfH7hzB5g6VXTNzqoiI0WCDIhicfPnZ+7+Z80C9u8H7t8HJk4EZszI3P1nBUzSNcnQEKhdG/jnH9HlnUk6ERFRIjExMYiNjU0yJZSJiQlOnjypenzs2DHky5cPuXPnRp06dTBlyhRYWVl9c7tRUVGIiopSPQ77RtUifX19WFpaIuT/TXSmpqZQaCIDINISkiQhIiICISEhsLS0hL6+vtwhpdrOneJn8+ZAmzaa266entjejBlA27aAq6vmtq1NJk4UCbK9vejen9ksLcXc6y1aAHPmAO3aiXnZSX1M0jWtQQORpPv6ijnTiYiISCVXrlyoWrUqJk+ejJIlS8LGxgZbtmzBmTNnUKRIEQCiq3vr1q3h5OSEhw8f4vfff0ejRo1w5syZbyYc06dPx8SJE9WKQdlCH5IRfWmJtISlpWWG9EbJaHFxwO7d4n7btprdduvWIknftUtMRXbuHJAji2VDly6JxBgQibKlpTxxNG8uxqRv3SqO9cWLmdPdPqtQSLo8YCUNUjOJfJrcuwcULy5a1d+9E93fiUinfP78GUFBQXByckrS2keUVin9XWX4uUnLPHz4EN27d8fx48ehr6+PH374AcWKFcOlS5dw+/btJOsHBgbC2dkZhw8fRt26dZPdZnIt6Q4ODike09jYWHz58kUzb4pIixgYGOhkCzoAnD0LVK0KmJkBr19rfux4cDBQqhTw/r1oUR8xQrPbl9OXL0ClSmIcuIeHSJDl9Po1ULIk8PatGF4wZoy88cgtNef6LHbtSAsULQo4OgKPHwP//Qc0bix3REREqVKrVi2UK1cOCxYsAAAUKlQIgwYNwiDlBKjJUCgU2LNnD1q2bJmufWtqOymZMGEC9u7di4CAgAzbB6XM2dkZ//33Hz59+oSwsDDY2dnBw8MDhQsXTnb9woULI2/evHjw4ME3k3QjI6NUF5fT19fX2USGKKvatUv8bNo0Y4q72dqKMdpduwLjxwOtWgHFiml+P3KYNUsk6FZWwKJFckcjpnxbtAjo3Fkk6a1biwsk9H0sP65pCoWYig0Q49KJiDJJs2bN0LBhw2SfO3HiBBQKBa5du5bq7V64cAG9evVKb3iJTJgwAeWSqdvx8uVLNGrUSKP7Iu2VM2dO2NnZ4f379/D19UWLFi2SXe/Zs2d4+/Yt7OzsMjlCIspMkhSfpGtyLPrXPD3F1/WoKFFgTZPTvMnl1i1g0iRxf+FCIF8+eeNR6tgRaNIEiI4Wxzo2Vu6IdAOT9IzQoIH46esrbxxElK14e3vDz88Pz549S/Lc2rVrUbFiRZQtWzbV27W2toapqakmQvwuW1tbjU+1RdrH19cXBw8eRFBQEPz8/FC7dm2UKFEC3bp1Q3h4OIYPH46zZ8/i0aNH8Pf3R4sWLVCkSBG4Ky+CE1GWdOUKEBQkpuzKyOu1CgXwxx9iVOrJk8CKFRm3r8wQGyuquEdHi4S4Uye5I4qnUIjjmyuXGMqwZIncEekGJukZoW5dUT7yzh3gyRO5oyEiTZAk4NMneW5qlg5p2rQprK2t4ePjk2h5eHg4duzYAW9vb7x9+xYdO3ZE/vz5YWpqChcXF2zZsiXF7RYqVEjV9R0A7t+/jxo1asDY2BilSpWCn59fkteMGDECxYoVg6mpKQoXLoyxY8eqxv76+Phg4sSJuHr1KhQKBRQKhSpmhUKBvXv3qrZz/fp11KlTByYmJrCyskKvXr0QHh6uer5r165o2bIl5syZAzs7O1hZWaFv376pGmccFxeHSZMmoUCBAjAyMkK5cuVw8OBB1fPR0dHo168f7OzsYGxsDEdHR0yfPh2AqKI8YcIEFCxYEEZGRrC3t8eAAQPU3nd2FRoair59+6JEiRLw9PTETz/9BF9fX9U42mvXrqF58+YoVqwYvL29UaFCBZw4cYIXcIiyOGUreqNGGV/WydExfmqwESPESFVdtWQJcOaMSISXL9fMlHWaVKBAfJX5338XF2IoZRyTnhEsLYEqVcR/y6FD4tIWEem2iAhRxUYO4eFqfVvJkSMHPD094ePjg9GjR6umldqxYwdiY2PRsWNHhIeHo0KFChgxYgTMzc2xf/9+dOnSBc7OzqhcufJ39xEXF4fWrVvDxsYG586dQ2hoaLJj1XPlygUfHx/Y29vj+vXr6NmzJ3LlyoXffvsNHh4euHHjBg4ePIjDhw8DACwsLJJs49OnT3B3d0fVqlVx4cIFhISEoEePHujXr1+iCxFHjx6FnZ0djh49igcPHsDDwwPlypVDz549v/t+AGDhwoWYO3cu/vjjD5QvXx5r1qxB8+bNcfPmTRQtWhSLFi3Cvn37sH37dhQsWBBPnz7F06dPAQC7du3C/PnzsXXrVpQuXRrBwcG4evWqWvvNztq3b4/27dsn+5yJiQl82RONKNvJrK7uCfXpI4qrnToF/Por8O+/2pfgfk9QkEh8AZEIOzjIG8+39OwJbNkiSnb16iVSJF071plKymZCQ0MlAFJoaGjG7mj8eEkCJKldu4zdDxFpXGRkpHTr1i0pMjIyfmF4uPifluMWHq527Ldv35YASEePHlUtq169uvTzzz9/8zVNmjSRhg4dqnpcs2ZNaeDAgarHjo6O0vz58yVJkiRfX18pR44c0vPnz1XPHzhwQAIg7dmz55v7mD17tlShQgXV4/Hjx0uurq5J1ku4nZUrV0q5c+eWwhO8//3790t6enpScHCwJEmS5OXlJTk6OkoxMTGqddq1ayd5eHh8M5av921vby9NnTo10TqVKlWS+vTpI0mSJPXv31+qU6eOFBcXl2Rbc+fOlYoVKyZFR0d/c39Kyf5d/V+mnZuyER5TIt1y44Y45RkaSlJm/tveuSNJRkZi3+vXZ95+NSEuTpLq1ROx16wpSbGxckeUsvv3JcnYWMS7erXc0WS+1JyX2N09oyjHzR0+zAoJRFmBqalo0Zbjlorx4CVKlEC1atWwZs0aAMCDBw9w4sQJeHt7AxBTTk2ePBkuLi7IkycPzMzM4OvriydqDs25ffs2HBwcYG9vr1pWtWrVJOtt27YNbm5usLW1hZmZGcaMGaP2PhLuy9XVFTkT9CJwc3NDXFwc7t69q1pWunTpRBW67ezs1J7/OiwsDC9evICbm1ui5W5ubqqpwLp27YqAgAAUL14cAwYMwKEERUHbtWuHyMhIFC5cGD179sSePXsQExOTqvdJRETxrej16wOZORNl8eLAhAni/qBBwKtXmbfv9Fq7VqQaxsbAn3+K0bbarEgRUeUdAIYMAV68kDcebablv0odVqkSYGEhJmG8eFHuaIgovRQK0eVcjlsq+4N5e3tj165d+PjxI9auXQtnZ2fUrFkTADB79mwsXLgQI0aMwNGjRxEQEAB3d3dER0dr7FCdOXMGnTt3RuPGjfHPP//gypUrGD16tEb3kZCBgUGixwqFAnEaLNX7ww8/ICgoCJMnT0ZkZCTat2+Ptm3bAgAcHBxw9+5dLFu2DCYmJujTpw9q1KjBubeJiFJp507xM7O6uic0dChQvjzw7h3Qv3/m7z8tXrwQiS4gEt8iReSNR12DBok0KTRUDDdQs+xOtsMkPaPkyAHUqyfucyo2IspE7du3h56eHjZv3oz169eje/fuqvHpp06dQosWLfDzzz/D1dUVhQsXxr1799TedsmSJfH06VO8fPlStezs2bOJ1jl9+jQcHR0xevRoVKxYEUWLFsXjryryGBoaIvY7vYxKliyJq1ev4tOnT6plp06dgp6eHooXL652zCkxNzeHvb09Tp06lWj5qVOnUCrBZK7m5ubw8PDAqlWrsG3bNuzatQvv3r0DIMZQN2vWDIsWLcKxY8dw5swZXL9+XSPxERFlB/fvA9evi6/P35iJMUMZGACrVwP6+sCOHcCePZkfQ2pIEtC3r0h0K1YUia+uyJFDHGsDA+Cvv8TxpqRYOC4jNWgg+u74+gJjx8odDRFlE2ZmZvDw8MCoUaMQFhaGrl27qp4rWrQodu7cidOnTyN37tyYN28eXr16lSghTUm9evVQrFgxeHl5Yfbs2QgLC8Po0aMTrVO0aFE8efIEW7duRaVKlbB//37s+eobT6FChRAUFISAgAAUKFAAuXLlSlK5u3Pnzhg/fjy8vLwwYcIEvH79Gv3790eXLl1gY2OTtoOTjOHDh2P8+PFwdnZGuXLlsHbtWgQEBGDTpk0AgHnz5sHOzg7ly5eHnp4eduzYAVtbW1haWsLHxwexsbGoUqUKTE1NsXHjRpiYmMDR0VFj8RF9T3S0SHA0OdezmRlQokT2Kez08SMQGak9c0tnN8qu7rVrA3nyyBND+fKiyvu0aaKFt1YtIHdueWL5np07gb17RcK7Zo34qUtcXESxu4kTgX79xMRYVlZyR5W8R4/EHPSNG2fufnXsV6pjlPOlnz0rLnUlU72YiCgjeHt7Y/Xq1WjcuHGi8eNjxoxBYGAg3N3dYWpqil69eqFly5YIDQ1Va7t6enrYs2cPvL29UblyZRQqVAiLFi1Cw4YNVes0b94cgwcPRr9+/RAVFYUmTZpg7NixmKAc9AegTZs22L17N2rXro0PHz5g7dq1iS4mAICpqSl8fX0xcOBAVKpUCaampmjTpg3mzZuXrmPztQEDBiA0NBRDhw5FSEgISpUqhX379qFo0aIARKX6WbNm4f79+9DX10elSpXw77//Qk9PD5aWlpgxYwaGDBmC2NhYuLi44O+//4aVtn7boCypXTtg3z7Nb3fNGqBbN81vV9t8+iS63z59Kr6yubjIHVH2k9lV3b9l7Fhg924xi/LQoeJ/QNu8fSsSW0Akurr69/r77+Jiw82bYm73nTvFVG3aZP9+oEsXICoKOH8eKF068/atkKTsNRIgLCwMFhYWCA0NhXlmVKUoXhy4d0/8x7dqlfH7I6J0+/z5M4KCguDk5ARjY2O5w6EsIqW/q0w/N2UD2eWYSpJoefzwAcifXzMtahERwOvXQI0aYrqkrG7IEGD+fHG/UiUxg26CWpSUwR49ApycRK+Nly8BDXaUSpNTp4Dq1cX/1qFDopCdNvH0BDZsEAnjpUvAV53QdMrly6IV/cMHIG9eYPNm7TjesbHA+PHA1KniceXKolt+wYLp225qzksck57RlFXeOS6diIiINOztW/EFFxBtAo8epf+mrHd74oRuVbpOi7NngQULxH0TE+DChfjHlDl27xY/a9SQP0EHADe3+JbqXr3EJCva4sABkaDr6Ylx3bqcoAPADz+ICw0//AC8eSPSpsmTNTt0J7VCQkQcygS9b1/g+PH0J+ipxSQ9oym7vPv6snwhERERadT9++JngQKpmq0xRQULihZlSdL+AlrpERUFeHuL9+npCSxaJJaPHQs8eCBvbNmJtnR1T2jaNMDRUVy0+qrsimzCwoBffhH3Bw0CqlSRNRyNKVxY9F7o2VP8L44bJ7q/v32b+bGcPi0uGPj7i8/TTZuAJUvkuRjCJD2j1aolyhcGBQEPH8odDREREWUhyiT9/yUUNOb/swyqEqisaOpUURAqXz5g3jyRsNepIwrIKRMGylgvXojECABat5Y3loTMzICVK8X9xYvjY5TTqFGibkLhwsCkSXJHo1nGxuJ4+/iI+wcPimT5/PnM2b8kiR40NWsCz5+LopkXLgCdOmXO/pPDJD2jmZmJfjMAu7wTERGRRmVUkq5s1Tx6VJ4WrYx27Rowfbq4v3SpqCytUACrVolu78eOifuUsZQ9NX78UdRU0CYNGgBdu4oEztsb+PxZvliOHweWLRP3V60CcuaUL5aM5OUFnDsn5nx/8gT46SfxvjPygllYGNC+PTB4MBATA3h4iIsDak56k2GYpGeGhF3eiUhnZLO6mpTB+PdEGSGjknRnZ8DVVRRQyojK8XKKiRFJV0yMqOmbsJt14cLxY1GHDweePZMnxuxi507xU5u6uic0bx5gayuqvU+ZIk8MkZFAjx7ifs+eordHVla2rKiL0bo18OWLGBP+888ZUxvg+nUxtGfnTtHxefFiYMsWIFcuze8rtZikZwZl8bgjR8RfGxFpNQMDAwBARESEzJFQVqL8e1L+fRFpQkYl6UB84pTVurzPny+SAEtL0Yr+9VzwAwaI8b5hYUDv3uz2nlFevxYtxID2Jum5c4u/EQCYMQMICMj8GCZMEP/n9vbA7NmZv385WFiIxHnuXDHTwubN4n/y9m3N7WPDBrHNe/cABwfxt9ivX9LPA7lwCrbMEBcnLsO9fi3mMqlRI3P2S0Rp9vLlS3z48AH58uWDqakpFNryqU06R5IkREREICQkBJaWlrCzs0uyTnaZLiwzZYdjKkniy+zHj2KuYU13z7x1S0zzZGAgvsJYWGh2+3K4f1+01H3+nPI88DdvAuXLi7aVzZuBjh0zN87sYNUqUT1dWeFbm7VrJ5LG8uVFV2hNTHWojosXRSIZFyd6tDRrljn71SYnT4ru6C9fim7+q1eLLulp9fmzKLz3xx/icYMGokBc3rwaCTdFqTkvZdKfWDanpycm/du8WYxLZ5JOpPVsbW0BACEhITJHQlmFpaWl6u+KSBNCQkSCrlCIbtqaVqoUULKkaL365x+gc2fN7yMzxcWJbsOfP4uvZV27fnvd0qVFlfdx40TLer16gLV1poWaLWhjVfdvWbxYVPy+ckW07o4YkfH7/PJFDMuIiwM6dMieCTogxqVfuSIulB09Ko7F6dOiV4GhYeq29eiRKIp56ZL43Bw3Tvyf6+tnSOjpwpb0zLJunTgbVKwoygUSkU6IjY3FFw5ToXQyMDCAfgrfArJDq29myw7H9ORJoHr1+KmiMsLYsWIsbqtW8fNZ66oVK0T39Zw5gRs3gEKFUl4/Olp8bbt+XSQImzdnSpjZwvv3oqp+TIwY7128uNwRfd/69aKwmZERcPVqxsc8ZYr4/7OyEhfKsvtFopgYkVQrCz7++COwfbvoqq6O/fuBLl3E316ePKL1vGHDjIs3Oak5LzFJzywvX4rBJAqFuPSdGX0qiIhIJ2SHhDKzZYdjunYt0L27aOX188uYfQQEiC6+Jiaiy7uuVpV++lS0jn/8CCxcKFrH1cHuxhlD2XZVpoy4CKILJAlo1EjUgf7pJzGCVS+DqnvduiX+76KjRTIp51Rg2uaff0Sy/eGDSKc2bxY9Y74lNhYYPz6+IGTlysCOHUDBgpkSbiKpOS+xcFxmsbMDXFzEf/jhw3JHQ0RERDouI4vGKbm6iq70kZHAgQMZt5+MJEnAL7+IBL1aNVEtWl0VKwJDh4r7v/4KhIZmTIzZjS51dVdSKMQ4ZjMz0Ytl+fKM2U9srOjmHh0NNGnCeghfa9oUuHxZ1DJ480bU5540SVxI+1pIiBhzrkzQ+/YVBeLkSNBTi0l6ZlJWeed86URERJROmZGkKxTxiZRyuixds2mTuMBgaAj8+Wfqx59OmCDmbX7xAvjttwwJMVv5+DH+q7AuJemAGFoyY4a4P3Ik8Pix5vexZAlw9qyYBmzFCu2pNq5NnJyAU6fElHSSJFrKGzcWSbvSqVOiN8KRI6IH0ObN4tgaGckXd2owSc9MCedLz16jDIiIiEjDMiNJB0ShJUCM6fz8OWP3pWkhIcDAgeL++PGiEF5qmZqK5B4AVq4Uxaso7fbvB6KixN9tmTJyR5N6vXuL7u7h4aKHhia/0gcGAr//Lu7Png0UKKC5bWc1xsbi/9HHRwzH8fUVrevnzolpFmvVEhfWSpQQFfl1rUcCq7tnpp9+En9RL17Ez2tCRERElEqSBDx4IO5ndJJeqZIozvT0qWgBbd48Y/enSQMGAO/eiW77w4enfTs1a4ru7itWiArx16+L5J1ST9kjo00b3Wwl1tMTF21cXUVi2LQpoKmyF1evAhERIsHs2VMz28zqvLxEi3nbtuLC5Y8/xj/n4SGm+suVS7740opJemYyMRGf8r6+4izHJJ2IiIjS4OVL4NMnkTA4OWXsvhQKoHVrUXBt1y7dSdL/+gvYtk10b1+zRsz3nh4zZ4qiVYGBour23LmaiTM7iYiIr22g7KGhi4oXF8MgRo0C/v1Xs9s2MRGJZUYVpcuKypYVk2d17y5moTAwAObNE2PQdfFCEMAkPfM1aCCSdF9fYPBguaMhIiIiHaTs6l6oUOrnCk6LNm1Ekv7XX6KgVWbsMz0+fBDdkgHRgv7DD+nfprm5KBzWpAmwYAHQvr2o/E7qO3hQJOqFCmnmdyKn334TPUzevtXsdqtWFTUQKHUsLEQvjX/+ERcudXEoRUJM0jObu7soE/rff2Jgl7Gx3BERERGRjsms8ehK1aoBtrZAcLAoxJTZ8wun1rBhordBsWJibmVNadwY+PlnYONGUYH70iXdKUSlDZRV3Vu31t0WTiU9PaBzZ7mjoIQUiqwzTSI7UmS2UqWA/PlFgn7ihNzREBERkQ7K7CRdXx9o1UrcVyZa2srfH1i9WtxfvVp0H9ak+fMBa2vg5k1g+nTNbjsri4oC/v5b3Ne1qu5EmY1JemZTKOKrvHMqNiIiIkqDzE7SgfjEau9eICYm8/abGp8+xRfc6ttX1OzVtLx5xVROADBtmigiR9/n5yemX7O3T1zci4iSYpIuh4RTsRERERGlkhxJes2agJWVmItYWzsDjhkDBAUBBQtmbCt3u3ZAixbAly+i27u2XrTQJgm7urMoGlHK+C8ih3r1RIv69etiwBQRERGRmuLiMm/6tYRy5BCJKRA/jZY2OXNGFLcDRIG3jJx2SaEAli0TxaouXIjfLyXvyxdRdBBgV3cidTBJl0PevECFCuI+u7wTERFRKjx/Lkrb5MghqmRnJuW0WXv2iIsF2iIqSrRoSxLg6Zk5he3s7eOnYRszJv7CCSV17Bjw/r0Yy1+9utzREGk/July4bh0IiIiSgNlV3cnJ5GoZ6a6dUXr8cuXouVaW0ydCty+DeTLJwq7ZZbu3cUx+fwZ6NFDuy5caBNlz4uWLUURQiJKGZN0ubi7i59+fvxEJyIiIrXJMR5dydAwfoojbanyfvVq/PjzpUuBPHkyb98KBbBqFWBqKmbXXbUq8/atK2JjRbFBIL4nBhGlTNYk/fjx42jWrBns7e2hUCiwV/kfnIJjx47hhx9+gJGREYoUKQIfH58MjzND/PgjYGYGvH4NBATIHQ0RERHpCDmTdCB+TPGuXaJ7uZxiYuILt7VuLU8S6OQkqrwDwPDhwLNnmR+DNjt5EggJAXLnBmrXljsaIt0ga5L+6dMnuLq6YunSpWqtHxQUhCZNmqB27doICAjAoEGD0KNHD/jqYpV0Q0OgTh1xn13eiYiISE1yJ+nu7kDOnMCTJ8DFi/LEoDRvHnDpEmBpGT8tmhz69RPtLx8/Ar/+Kv/FC22i7HHRvDlgYCBvLES6QtYkvVGjRpgyZQpatWql1vorVqyAk5MT5s6di5IlS6Jfv35o27Yt5mfm4CNN4lRsRERElEpyJ+kmJkCTJuK+nF3e790Dxo8X9+fPB+zs5ItFXx9YvVq0wezfD2zZIl8s2iQuDti9W9xnVXci9enUmPQzZ86gXr16iZa5u7vjTAqVS6KiohAWFpbopjWU49JPnQLCw+WNhYiIiLRebCzw8KG4L1eSDsjf5T0uThRq+/wZqF8f8PLK/Bi+VqoUMHasuD9ggBjRmN2dOydmI8iVS/yeiEg9OpWkBwcHw8bGJtEyGxsbhIWFITIyMtnXTJ8+HRYWFqqbg4NDZoSqHmdnMZDpyxcxNwURERFRCp4+BaKjRYttwYLyxdG4MWBsLKYdu3498/f/xx/AiROi2/3KlaKAmzYYMQIoWxZ4+1Yk6tosKirjL7Aoe1o0bSr+XohIPTqVpKfFqFGjEBoaqro9ffpU7pDiKRTxrekcl05ERETfoezqXriwvFNZmZnFf4XJ7C7vT54Av/0m7k+fnvlzxafEwABYswbQ0wO2bgUmTdLOSXx8fAArK6ByZSAwMGP2IUnxfxvs6k6UOjqVpNva2uLVq1eJlr169Qrm5uYwMTFJ9jVGRkYwNzdPdNMqHJdOREREapJ7PHpCykrqyjmwM4MkicJs4eFAtWpAnz6Zt291VagQP1Z+/HjRivz2rbwxKUVGAj17At26AZ8+icJ/FSoAf/+t+X1duQI8eiRqGDRsqPntE2VlOpWkV61aFf7+/omW+fn5oWrVqjJFpAF16ohL4ffuiU8yIiIiom/QpiS9aVPRcnzrFnDnTubsc9Mm4MAB0d1/9Wp5exOkZNw4YO1a0cX7wAHghx+ACxfkjSkwEHBzA/78U3Tm/P13UZH+wwdReX3UKDGVnaYoL940aiSGJRCR+mRN0sPDwxEQEICA/88THhQUhICAADx58gSA6Kru6empWv/XX39FYGAgfvvtN9y5cwfLli3D9u3bMXjwYDnC1wwLC/EJCbDLOxEREaVIm5J0S0tAWc83M7q8v3oFDBwo7o8fD5QokfH7TI+uXYGzZ4EiRUQX/Z9+ApYvl6fQ3r594kLBlStA3ryiA+fUqcB//8WPnZ8xQxR3Cw5O//4SdnWXY+56Il0na5J+8eJFlC9fHuXLlwcADBkyBOXLl8e4ceMAAC9fvlQl7ADg5OSE/fv3w8/PD66urpg7dy7+/PNPuCsHRekqZZd3JulERESUAm1K0oHEVd4z2oABwLt3QLlywPDhGb8/TXB1FV3KW7USBf/69AG6dBFdzTNDTAwwciTQogUQGirahS5fjq+0bmgILFwoxs+bmYk6xj/8IIrypcfNm6KTqKFh/HR9RKQ+hSTJcT1PPmFhYbCwsEBoaKj2jE8/d058alpaivk6cuSQOyIiIspEWnlu0nFZ8ZjGxIjxvTExwOPH8lZ3V3rzBrC1jZ8arnDhjNnP3r0i0dXXB86fF4mkLpEkYN48Uf09NhYoXVpc2ChePOP2GRwMdOwYP4HQwIHArFkicU7OnTviosutW+I4z5gBDB2atsr5EycCEyaIIREZMd6dSBel5rykU2PSs6yKFYHcucWgILkHLBEREZFWevxYJOjGxkCBAnJHI+TNC9SqJe5nVGv6+/dA797i/vDhupegAyLRHToUOHoUsLMTLc0VKwLbt2fM/k6cEMfp2DHRQr5tG7BgwbcTdEAMHzh3DujUSVxIGD5cJO2hoanfP6u6E6UPk3RtoK8fP6iLXd6JiIgoGcqu7s7OYoovbZHRXd6HDROtwsWKxVdN11XVq4vu5rVqiQr1Hh6ihTs6WjPblyRgzhygdm3g5UvRYn/hAtC+vXqvNzMDNm4Eli0TCf2ePeJiwtWr6sdw7x5w/broGNq8edreB1F2p0Uf8dkcp2IjIiKiFGjbeHSlVq1ES/G5c8CzZ5rd9uHDYt5xhUJUczc21uz25WBrC/j5ibHiALBokUja03vsQkPFBZPhw0VLeOfO4neS2gJ7CoXouXDypBhS8eCBGJXp46Pe65UXa+rUAfLkSd2+iUhgkq4tlEn6uXOi2zsRERFRAtqapNvaiqm9AGD3bs1t99MnMac3APTtK6qjZxU5cgDTpwN//SUm+jlzBihfXlyUSIurV0WL9549ogV8+XJgw4b0TX1WqZJo9W/UCPj8Wcyt3rOnmGs9JezqTpR+TNK1RcGC4lJnXBzw1VzwWdqLF2JekC9f5I6EiIhIq2lrkg7EJ2TKubE1YfRo4NEj8RVp2jTNbVebNG8uEuHy5UURvgYNgMmTxddBdfn4iJbuBw/EsTp5Evj117QVfPualRXwzz/ApElie3/+KS7IBAYmv/6jR8ClS2I4RsuW6d8/UXbFJF2bKKeSyw5d3l+9AgYPFmVgW7QAevWSZ+JQIiIiHaHNSXrr1uLnyZOamWf7zBnRDRwAVq4EcuVK/za1VeHCwOnTopVakoBx40RV9LdvU35dZKR4TbduoqW7USOR8FeqpNn49PSAsWPF19O8ecVc6z/8INpYvqbsSVG9OpAvn2bjIMpOmKRrk8aNxc9t29JWSlMXvHkj5h8pXFiUGY2KEst9fIDFi+WMjIiISGt9+SJaKQHtTNILFgQqVxZJ5t696dtWVBTg7S225eUV34aRlRkbi4sRPj7i/oEDIhH+1qQ/gYGiRfvPP0UL9+TJosXbyirjYqxfX1wE+PFH8TW1RQtg1Cgx44CSsidF27YZFwdRdsAkXZvUqweUKgWEhYmymlnJ+/fiMqyTk5ikMyJCnM19fYG5c8U6Q4YAR47IGycREZEWevRIFAMzNQXs7eWOJnmaqvI+ZQpw+zZgYyPmFs9OvLxEeaIiRYAnT8Q4/OXLE3c23LdPJPBXroiW7UOHgDFjMqfiv4MD8N9/oiI9IOZSr19f9J54/lz0gABEMUEiSjsm6dpETy++1OeCBd+vzKELwsLEQCYnJ3HWDQ8XA6/+/hs4e1YMvho8GOjSRXz7aN8eCAqSO2oiIiKtouzqXqSIZsYaZwRlkn706Pe7an/L1asi8QOApUuzZ3XwsmWBixfFEILoaKBPH/E1KTRUtFy3aCHuV60qEnXlLL6ZxdBQfE3dtk1M2XbsmLhoMHaseL5qVSB//syNiSirYZKubTp0ABwdgZAQYO1auaNJu0+fxFnWyUlMahoaCpQpIy6vX7okBlspv2UoFMAff4iypG/fikojnz7JGj4REZE20ebx6ErOzkC5cuKa+19/pf71MTFA9+7iZ+vW2bs6uIWF6Do+dy6grw9s2iR6UCgvYAwcKJLjAgXki7F9e9Edv3RpMSe78mtrdv69EWkKk3RtY2AADBsm7s+enXigjy6IjBR905ycxOXed++A4sWBLVvE5fHWrZNvAjAxEfOG2NgA166JKigsJEdERARAN5J0IH1d3ufNE2OeLS1FK3p2p1CIkYDHjgF2dmKkoJkZsH27aMk2NJQ7QjEx0blzYk52QMSsLCJIRGnHJF0bde8OWFuLAWhbt8odjXqiooAlS8Rl9KFDgdevxf3164EbN0QPge8NlipQQJzVDQyAHTvEBKJERJTlfPz4EYMGDYKjoyNMTExQrVo1XEhQIUuSJIwbNw52dnYwMTFBvXr1cF+ZpWZTupak+/mlrgbuvXui4x0AzJ8v5l4n4aefRLf2WbPERYx27eSOKLGcOcWc7Nu3i5uTk9wREek+JunayNQUGDRI3J8xI3WTZWa2L19EOdKiRYH+/UV/p4IFRbnR27fFIKocOdTfnpubSPYBUQVl//6MiZuIiGTTo0cP+Pn5YcOGDbh+/ToaNGiAevXq4fnz5wCAWbNmYdGiRVixYgXOnTuHnDlzwt3dHZ8/f5Y5cvnoSpJesqS4ffkiqo2rIy4O6NFDTCPWoIEonkaJ2dgAw4dr7+9foRAXD1jVnUgzmKRrqz59xKSgN29qZ6IaEyPmCSleHPjlF+DpU1ElZNky8U3C21u0iKdFr17Ar7+K7u6dOgF37mg0dCIikk9kZCR27dqFWbNmoUaNGihSpAgmTJiAIkWKYPny5ZAkCQsWLMCYMWPQokULlC1bFuvXr8eLFy+wN71ze+mo6Gjg8WNxX1uTtIRS2+V9xQrgxAnRIvvHH9pbGI+IKLMwSddWlpZA797i/vTp2jM+OzZWVC8pVUqMGw8KEpd3FywAHjwQMWtikNTChUD16qI6fIsWwIcP6d8mERHJLiYmBrGxsTA2Nk603MTEBCdPnkRQUBCCg4NRL0HJagsLC1SpUgVnlPM7JSMqKgphYWGJbllFYKBobTYzE6dcbadsTT1wQEzqkpInT4ARI8T96dOBQoUyNDQiIp3AJD2drl7NwI0PHgwYGYlJJ48fz8AdqUmSRML888+itdzKSgyQevhQlBn96gtXuhgairKmDg5ioFrnzuICARER6bRcuXKhatWqmDx5Ml68eIHY2Fhs3LgRZ86cwcuXLxEcHAwAsPkqG7WxsVE9l5zp06fDwsJCdXNwcMjQ95GZdGH6tYTKlhVlaT5/Fon6t0iS6IwXHi5Gu/Xtm3kxEhFpMybp6bB0qZhqZNasDGrotrUVrdVA/Jwbctq4UXS9NzYWc54HBYkBUjlzZsz+8uUTFd+NjYF//42fgJOIiHTahg0bIEkS8ufPDyMjIyxatAgdO3aE3vcKjKZg1KhRCA0NVd2ePn2qwYjlpSvj0ZUUCvW6vG/cCBw8KNoj/vzz+/VliYiyC34cpkNgoPg5YoQYQp4hs6UNHy7OWgcPitKecnn/Pn5quPHjgdGjxZj5jFahArB6tbg/fbooG0pERDrN2dkZ//33H8LDw/H06VOcP38eX758QeHChWH7/7Ler169SvSaV69eqZ5LjpGREczNzRPdsgpdS9KB+CR9/34xO+vXXr2Kr5E7fryYyouIiAQm6ekwd64Yiq1QiKInLVt+f+xVqhUuDHh4iPtytqaPHg2EhIiSrUOGZO6+O3WKv0DQrVsGjzEgIqLMkjNnTtjZ2eH9+/fw9fVFixYt4OTkBFtbW/j7+6vWCwsLw7lz51C1alUZo5WPLibplSqJEWvh4cChQ0mf798fePdO9EhUnuKJiEhgkp5OAweKrlzGxuJqcc2aYhYyjRo5UvzcuTP+TJ2ZLlwQVyEAUb1dE4XhUmvGDDEvS0SEGBf/5k3mx0BERBrh6+uLgwcPIigoCH5+fqhduzZKlCiBbt26QaFQYNCgQZgyZQr27duH69evw9PTE/b29mjZsqXcoctCF5P0lLq879kD7NgB6OsDa9akfTIYIqKsikm6BrRqBRw9CuTNC1y+DPz4I3DrlgZ3ULYs0KSJKO06e7YGN6yG2FhRsV2SRMG4WrUyd/9K+vrA1q2iEs3jx0D79mISViIi0jmhoaHo27cvSpQoAU9PT/z000/w9fWFwf+ztd9++w39+/dHr169UKlSJYSHh+PgwYNJKsJnB58/i1lOAd1K0oH4JH3fPjGNHCBGz/XpI+7/9htQvrw8sRERaTOFJGnL3F6ZIywsDBYWFggNDdX4eLWHD4FGjcQVbwsLcaW4dm0NbfzUKeCnn0QrdlAQYG+voQ1/x9KlQL9+4g3dvSv/3C83b4qrIOHhoq/cokXyxkNEpAEZeW7KrrLKMb15EyhTBjA3F7OR6kJ1d6W4OCB/fiA4WFR5b9gQ8PYWrefFiwMBAZqdGIaISJul5rzElnQNcnYWs6W5uQGhoYC7u6hcqhFubmLe8OhoYN48DW30O4KDxVh0AJg2Tf4EHQBKl44/qIsXA2vXyhsPERFRBkrY1V2XEnRA1L1t1Urc37UL8PMTCbpCIWrCMkEnIkoek3QNs7ICDh8G2rUTvbG7dBGzlWmkv4JybPqKFaLaSkYbNkxcbahYUUxkqi1atAAmTBD3f/0VOHtW1nCIiIgyii6OR0+obVvxc88eoFcvcb9vX9H2QEREyWOSngGMjcXw6eHDxeOxY4GePTUwhLpRI8DVFfj0SXRDz0hHjwKbNonL3cuXizHh2mTsWFFOPzoaaN0aePFC7oiIiIg0TteT9Bo1RAPG27fAo0dAwYJiRlUiIvo2JukZRE8PmDVLFEPX0xPdupo1A8LC0rFRhSK+NX3hQpGsZ4To6PiqLr17i5Z0baOnB6xfL7q/v3wpqtNERckdFRERkUbpepKeI4e4pq60ciVgZiZbOEREOoFJegbr3Rv46y/A1BTw9RXDyp8/T8cG27YVg9/fvgX+/FNjcSYydy5w5w6QLx8wdWrG7EMTcuUC9u4FLC1Fl3dlFXoiIqIsQteTdECMmDMwEPVe3d3ljoaISPsxSc8ETZsC//0n6q5duyaKk1+7lsaN5cgR349+zpz4OU005dEjYPJkcX/uXJEAa7MiRYBt20TL+tq1wJIlckdERESkERER8Rf2dTlJr1RJdP5buFDuSIiIdAOT9ExSsaJo7C1ZEnj2TMymduhQGjfm5QXY2ooNbd6s0TgxcCAQGSnmQ+/cWbPbzigNGoixBQAweLAYT09ERKTjHjwQP3PnFuO6dZmBge5VpycikguT9ExUqJCY7rxWLeDjR6BJkzTOIGZsDAwZIu7PnCkmItWEffvELUcOMZhel86mQ4YAP/8MxMaK0vqPHskdERERUbpkha7uRESUekzSM1nu3MDBg6KROiYG6N4dGDcuDUOpf/lFdEW/c0eMy06vT5+AAQPE/WHDRJO/LlEoRDWaChXEeP26dYGgILmjIiIiSjMm6URE2ROTdBkYGQEbNgCjR4vHkyeLHuypGl5ubi4mGgXEXCbpLZg2dSrw+DHg6AiMGZO+bcnFxERcsHB2BgIDRZW+O3fkjoqIiChNmKQTEWVPTNJlolAAU6YAq1aJKcg3bBDToH/4kIqNDBwoEtOLF4EjR9IezO3boggdACxaBOTMmfZtya1AAeD4caBUKVFtp0YN4OpVuaMiIiJKNSbpRETZE5N0mfXoAezfL+YMPXJEFJR78kTNF1tbiw0AojU9LSRJzIn+5YuYyL1587RtR5vY24ty+uXLA69fiyIAZ8/KHRUREVGqMEknIsqemKRrAXd34MQJkVvevAm0b5+K3utDh4pCb/7+wIULqd/5pk3AsWOiRX7RotS/XlvlzSuuelSrJron1Ksn3icREZEO+PgRCA4W95mkExFlL0zStUS5csDp0yJXPncuFdOzOToCnTqJ+6ltTf/wQST5ADB2rCg/n5VYWooDWbeuKIzXqBHw779yR0VERPRdyunX8uYVpzMiIso+mKRrEUdHUbQdEMXk1G5NHzFC/NyzR4wvV9eYMUBICFCiRHyyntXkzAn884/oyv/5M9CyJbBzp3zxfPkCrFghjv3p05qbPo+IiLIUdnUnIsq+mKRrmeHDRfX3U6fEsGq1lColkk8AmDVLvddcvCjmQgfET0PD1IaqO4yNgV27AA8PkSR7eADr1mV+HCdOiHHyvXuLavpubqLQXd++omt+TEzmx0RERFqJSToRUfbFJF3L2NsD3t7i/uTJqXjhyJHi58aN3688FxsrEkVJEhO2166dplh1ioGBGH/v7S1ar7t2jb9IkdHevAG6dxeV5m/eFH0X27YFcuUCXr4UcdStC9jZiUKABw6kcj4+IiLKapikExFlX0zStdCIEaIW3JEjoke0WqpUEcl2TAwwd27K665cKVrSLSzip17LDvT1xXsfOFA87ttX/Z4HaREXB6xZAxQvDqxdK5b17Cnmbt+xQ1Se/+cfoFs3IE8ekcyvXg00bgzkywd06SLmfY+MzLgYiYhIKzFJJyLKvpika6GCBQEvL3F/ypRUvHDUKPFz1SqR8CXn1av49aZOBWxt0xynTtLTA+bPF2PCAXFFZOzYVBQAUNONG0DNmqLl/t07wMVFjGFYuRKwshLrGBkBTZqIRD44GPDzEz0cbG2B0FDRK6JVKzHVXvv2wLZtotwvERFleUzSiYiyL4UkaTo70W5hYWGwsLBAaGgozM3N5Q7nmx4+FA2wsbFiZrWKFdV4kSQBlSoBly6JxHPSpKTreHoCGzYAP/wAnD8vWpezq5kz44cJDBoEzJsHKBTp2+anT+K4z5snejXkzAlMnAgMGCC63KsjNhY4c0aMo9+1C3j6NP45IyMxZ1+bNqIYXu7c6YtXKTpaXBhQ3j59AkxNAXNz0ePC3FyM7U/v8SGiZOnKuUmX6PIxDQ2Nr+geFiZGRxERkW5LzXmJSboWU+bTLVqIXs9q2bVLjHe2tBRj0xOe2f/7D6hVSyRa586JhD67W7oU6NdP3O/RQ1ReT+uFi337gP7942sCtGoFLFwIODikPT5JEkMTlAm7ck4eQIyJqFsXaN1azAOvTLTDwhIn3N9alnD558/fjyVHDpGsp3RTJvTJLXd2FtsgoiR06dykK3T5mF68KE7RNjbxc6UTEZFuY5KeAl06ad+5Iwq3SxJw9SpQtqwaL4qNFS+6dw+YPRsYNkwsj44Wk7Hfvi26VGdW0TRdsG6dKOwWFwd06ACsX69+qzcAPH4sWsr37ROPHR2BJUuApk01G6ckAdevxyfsN29qdvsAYGYmEuqcOcVY+LAwcdPEx4Szs6iG6OEhhh0QkYounZt0hS4f0y1bgE6dgJ9+EhODEBGR7kvNeYnNWlqsRAmgXTtg+3YxfHzbNjVepK8vxll7e4su1/37iy7S8+eLBD1fPrExiuflJbp2d+oEbN0KRESIg21snPLrvnwRx3XiRPGaHDnERZGxY8X2NE2hEFdqypYV+7x7F9i9WyTsV6/GJ9jK1mzl/YS3lJabmyffiyAuTrw/Zcv7927Jrff6tRjD0amTKNY3fbrots/u80Q6o1ChQujevTu6du2KggULyh1Olsbx6ERE2Rtb0rXc9esiJ1MoRMNpyZJqvCg6WrRaPnsmCpU1aCBa1yMiRCtxly4ZHrdO+vdfMdb782fRfXzvXtGinJyTJ0WPhBs3xOPq1YHly4HSpTMtXJ3y6ROwYIFI0MPCxLKaNUWyXrWqrKERaQNdODctWLAAPj4+uHHjBmrXrg1vb2+0atUKRkZGcoeWLF04pt/SpYuoHTptWnytVyIi0m2pOS+xz6mWc3EBWrYUvY2nTVPzRYaGwNCh4v6sWaI1PSJCzNP9888ZFarua9xYJOo5cwKHD4uW3tDQxOu8eSN6KVSvLhL0vHnF9Gr//ccEPSU5cwKjRwOBgaK3gZGROGbVqok/8Izouk9EGjVo0CAEBATg/PnzKFmyJPr37w87Ozv069cPly9flju8LIUt6URE2Rtb0nXApUuiuruenujhXKSIGi8KDxdjo9+9E49z5AACAphIquPsWaBRI+DDB1EF39dXzGPu4wP89hvw9q1Yr0cPYMaM+CnVSH3Pnoku+2vWiO70CoWolDhxovi7JcpmdPHc9OXLFyxbtgwjRozAly9f4OLiggEDBqBbt25QaMFQFl08pkpWVuL0HRAAuLrKHQ0REWkCW9KzmAoVRM4YFydyQrWYmYliZkpDhzJBV9ePPwLHjon5yS9fFt2ylXOev30rujecPCnmo2eCnjYFCojjd/OmGGIgSaKAX7FiwODBYgw7EWmlL1++YPv27WjevDmGDh2KihUr4s8//0SbNm3w+++/o3PnznKHqNPevYu/vq7WRXkiIspy2JKuI86cET2Dc+QQs3Cp1dj47h1QpowoDHbx4rfHV1Py7twRY9OfPxeP0zLnOannwgUxZ/2RI+KxmZnoFj9kCCcIpmxBF85Nly9fxtq1a7Flyxbo6enB09MTPXr0QIkSJVTr3LhxA5UqVUJkZKSMkQq6cEyTc+6cuFZsbx9/+iEiIt3HlvQsqGpVMSV2TAwwc6aaL8qTR1TUvnKFCXpalCgh5r756ScxbditW6JHAhN0zatUCfD3B/z8RNeR8HBgwgSgcGEx13xUlNwREmV7lSpVwv3797F8+XI8f/4cc+bMSZSgA4CTkxM6dOggU4RZA8ejExGR7En60qVLUahQIRgbG6NKlSo4f/58iusvWLAAxYsXh4mJCRwcHDB48GB8/vw5k6KV15gx4ufq1am4um5i8v2pxOjbnJxEor51K8AphzJevXrA+fNi3sGiRUWhvkGDgOLFxcwEsbFyR0iUbQUGBuLgwYNo164dDL5xsTJnzpxYu3ZtJkeWtTBJJyIiWZP0bdu2YciQIRg/fjwuX74MV1dXuLu7IyQkJNn1N2/ejJEjR2L8+PG4ffs2Vq9ejW3btuH333/P5MjlUbOmaNSNjgbmzJE7GqIMoqcHtGsnxquvXCn6fD5+LOazd3UF9u0TY9iJKFOFhITg3LlzSZafO3cOFy9elCGirIlJOhERyZqkz5s3Dz179kS3bt1QqlQprFixAqamplizZk2y658+fRpubm7o1KkTChUqhAYNGqBjx47fbX3PKhQKYOxYcf+PP4BvXMsgyhoMDICePUURhlmzgNy5ReLeogXg5iamcCOiTNO3b188ffo0yfLnz5+jb9++MkSUNTFJJyIi2ZL06OhoXLp0CfXq1YsPRk8P9erVw5kzZ5J9TbVq1XDp0iVVUh4YGIh///0XjRs3/uZ+oqKiEBYWluimy+rXF8N3IyOBuXPljoYoE5iYAMOHiznWR40Sj8+cAWrVAurUYbJOlElu3bqFH374Icny8uXL49atWzJElPVIEpN0IiKSMUl/8+YNYmNjYWNjk2i5jY0NgoODk31Np06dMGnSJPz0008wMDCAs7MzatWqlWJ39+nTp8PCwkJ1c3Bw0Oj7yGwJW9OXLo2fspsoy7O0BKZNE8UQe/cWLe1Hj4pkvXZtJutEGczIyAivXr1Ksvzly5fIkSOHDBFlPW/eAKGh4r6zs7yxEBGRfGQvHJcax44dw7Rp07Bs2TJcvnwZu3fvxv79+zF58uRvvmbUqFEIDQ1V3ZLrqqdrmjYFypUDPn0Sha+JshU7O2DZMtENXpmsHzsWn6wfOyZzgERZU4MGDVTnVKUPHz7g999/R/369WWMLOtQtqI7OIhOQ0RElD3JlqTnzZsX+vr6Sa7Kv3r1Cra2tsm+ZuzYsejSpQt69OgBFxcXtGrVCtOmTcP06dMRFxeX7GuMjIxgbm6e6KbrFIr4Su+LFsVfdSfKVgoWFMn6w4dAnz6AoaFI0GvXFgk7k3UijZozZw6ePn0KR0dH1K5dG7Vr14aTkxOCg4Mxl+OvNIJd3YmICJAxSTc0NESFChXg7++vWhYXFwd/f39UrVo12ddERERATy9xyPr6+gAAKZtVe27VCihVSiToixfLHQ2RjBwcxNiPBw/ik/X//hPJes2aokt8Nvt8IMoI+fPnx7Vr1zBr1iyUKlUKFSpUwMKFC3H9+nWdH0qmLZikExERAMg6iGzIkCHw8vJCxYoVUblyZSxYsACfPn1Ct27dAACenp7Inz8/pk+fDgBo1qwZ5s2bh/Lly6NKlSp48OABxo4di2bNmqmS9exCTw8YPRro3BmYPx8YOBDIlUvuqIhkpEzWR40CZswAVq0Cjh8XxeVq1AAmTBAt7AqF3JES6aycOXOiV69ecoeRZTFJJyIiQOYk3cPDA69fv8a4ceMQHByMcuXK4eDBg6pick+ePEnUcj5mzBgoFAqMGTMGz58/h7W1NZo1a4apU6fK9RZk5eEh8o7794EVK0QBbKJsr0ABYMkSYOTIpMl69erin6Z2bSbrRGl069YtPHnyBNHR0YmWN2/eXKaIsg4m6UREBAAKKQ39xJ8+fQqFQoECBQoAAM6fP4/NmzejVKlSWn+FPSwsDBYWFggNDc0S49N9fIBu3YB8+YCgIMDUVO6IiLTMs2fAzJnAypWAMqlgsk5aRhfOTYGBgWjVqhWuX78OhUKhGmam+P//UGxsrJzhJaELxzQhSQLMzYHwcODWLaBkSbkjIiIiTUrNeSlNY9I7deqEo0ePAgCCg4NRv359nD9/HqNHj8akSZPSsklKo86dgUKFgJAQ0WBIRF8pUEAUbnj4EOjXDzAyAk6cAOrWFd3g/f05Zp1IDQMHDoSTkxNCQkJgamqKmzdv4vjx46hYsSKOsVBjur16JRJ0PT2gcGG5oyEiIjmlKUm/ceMGKleuDADYvn07ypQpg9OnT2PTpk3w8fHRZHz0HQYGolcvAMyaBXz+LG88RForYbLev79I1k+eBOrVEy3rBw4AX77IHSWR1jpz5gwmTZqEvHnzQk9PD3p6evjpp58wffp0DBgwQO7wdJ6yq3vBguLjiYiIsq80JelfvnyB0f/PIIcPH1aNQytRogRevnypuehILV27ivzjxQvR/Z2IUpA/v5i7MDAQGDBAfBs+dQpo3BiwtgY6dAA2bgTevJE7UiKtEhsbi1z/r1CaN29evHjxAgDg6OiIu3fvyhlalsDx6EREpJSmJL106dJYsWIFTpw4AT8/PzRs2BAA8OLFC1hZWWk0QPo+IyPgt9/E/Rkz2BhIpBZ7e2DhQpGsDxwI5M0r5jTctg3o0gWwsQHc3IDp04Hr19klnrK9MmXK4OrVqwCAKlWqYNasWTh16hQmTZqEwuyfnW5M0omISClNSfrMmTPxxx9/oFatWujYsSNcXV0BAPv27VN1g6fM1aOHyCkePwY2bJA7GiIdYm8PLFgABAcDp08Dv/8OuLoCcXHxj8uWBRwdxTzs//4LREbKHTVRphszZgzi4uIAAJMmTUJQUBCqV6+Of//9F4sWLZI5Ot3HJJ2IiJTSVN0dEN3ewsLCkDt3btWyR48ewdTUFPny5dNYgJqma9VeU2POHDENm7MzcOcOkEPWCfaIdNzTp8D+/eJ2+HDigg8mJqLwXNOmQJMmYrwJUTro6rnp3bt3yJ07t6rCuzbRtWPq6gpcuwb884/4WCEioqwlw6u7R0ZGIioqSpWgP378GAsWLMDdu3e1OkHP6n79FbCyEnWxtm2TOxoiHefgIP6p/v4bePtWfHPu3Vssj4wUj3/9VTwuVw4YMwY4cwbQsmmoiDThy5cvyJEjB27cuJFoeZ48ebQyQdc1kgQ8eCDusyWdiIjSlKS3aNEC69evBwB8+PABVapUwdy5c9GyZUssX75cowGS+szMgMGDxf2pU0VvXSLSAFNT0bS1bJkYU3L1KjBtGlCtmphn/epV8U9XrRpgawt4eQE7dgAREXJHTqQRBgYGKFiwoNbNhZ5VvHwpPi709QEnJ7mjISIiuaUpSb98+TKqV68OANi5cydsbGzw+PFjrF+/nuPSZNavH2BpCdy+DezaJXc0RFmQQiHGqI8aJarCh4SIQhAeHoCFhagKv3490L69SNh79BDzsrPwHOm40aNH4/fff8e7d+/kDiXLUY5HL1RITK1KRETZW5qS9IiICNU0LIcOHULr1q2hp6eHH3/8EY8fP9ZogJQ6FhZiVikAmDKFrelEGS5vXuDnn4GtW4HXr4Fjx4Bhw0Rz2MePwOrVQI0aQJEiwMSJQFCQ3BETpcmSJUtw/Phx2Nvbo3jx4vjhhx8S3SjtWDSOiIgSSlNpsSJFimDv3r1o1aoVfH19Mfj/faxDQkJ0ojhLVjdwIDBvXnwBmv9PY09EGc3AAKhZU9xmzgROngTWrRNd3wMDgQkTxK1GDdElvl074P8XPIm0XcuWLeUOIctikk5ERAmlqbr7zp070alTJ8TGxqJOnTrw8/MDAEyfPh3Hjx/HgQMHNB6opuhatde0GjlS5AgVKwLnz4seukQkk4gIYM8ekbAfPhzf9d3EBGjdWiTsdeqIAamULWWXc1Nm0qVj2rq1+IhYtAjo31/uaIiIKCOk5ryU5inYgoOD8fLlS7i6ukJPT/SaP3/+PMzNzVGiRIm0bDJT6NJJOz1CQsTYtshI4MABoGFDuSMiIgBiareNG0XCfvdu/PICBYAuXUTCXry4fPGRLLLLuSkz6dIxdXEBbtzg+ZqIKCvLlCRd6dmzZwCAAjoyT7AunbTTa8gQYP580Zr+33+iQDURaQlJEt1c1q0T49nfv49/rkoVkax36AD8f6pLytp04dykp6eX4nRr2lb5XReOKSBqx+TMCXz+LKZhc3aWOyIiIsoIGT5PelxcHCZNmgQLCws4OjrC0dERlpaWmDx5MuJYqUxrDB8uTvwXLwJ164qi00SkJRQKkYwvWybmX9qxA2jaVHR5P3cO6NNHVIdv104Ul4iJkTtiyub27NmD3bt3q27btm3DyJEjYWdnh5UrV6q9ndjYWIwdOxZOTk4wMTGBs7MzJk+ejIRtBl27doVCoUh0a5hFm5ifPxcJeo4cgKOj3NEQEZE2SFPhuNGjR2P16tWYMWMG3NzcAAAnT57EhAkT8PnzZ0ydOlWjQVLa2NkBvr5As2bA2bOAmxtw8CDnYCXSOkZGQNu24vbqFbBpk2hhv3YN2LlT3PLlA0aPFvMs6qXp+ipRurRo0SLJsrZt26J06dLYtm0bvL291drOzJkzsXz5cqxbtw6lS5fGxYsX0a1bN1hYWGCAcnoSAA0bNsTatWtVj42MjNL/JrSQsmhc4cIiUSciIkrTN71169bhzz//RO/evVG2bFmULVsWffr0wapVq+Dj46PhECk93NzEVM4FCwL37gHVqgFXrsgdFRF9k42NGKty9ar4Zx00CLC2FoUmBg4EGjQA/j/MiEgb/Pjjj/D391d7/dOnT6NFixZo0qQJChUqhLZt26JBgwY4f/58ovWMjIxga2uruuXOokM/WNmdiIi+lqYk/d27d8kWhytRogTevXuX7qBIs0qWBE6fFoVpgoPF7FCHD8sdFRF9V7lyorDE8+fA0qWiGry/v/hn3rJF7uiIEBkZiUWLFiF//vxqv6ZatWrw9/fHvXv3AABXr17FyZMn0ahRo0TrHTt2DPny5UPx4sXRu3dvvH37NsXtRkVFISwsLNFNFzBJJyKir6UpSXd1dcWSJUuSLF+yZAnKli2b7qBI8/LnB06cAGrVAj5+BBo3Fj1qiUgHGBiIMeoBAUClSsCHD0CnTkDHjgAvjFImyZ07N/LkyaO65c6dG7ly5cKaNWswe/ZstbczcuRIdOjQASVKlICBgQHKly+PQYMGoXPnzqp1GjZsiPXr18Pf3x8zZ87Ef//9h0aNGqVYnG769OmwsLBQ3RwcHNL1fjMLk3QiIvpamqq7//fff2jSpAkKFiyIqlWrAgDOnDmDp0+f4t9//0X16tU1Hqim6Eq114wSFQV4egLbt4vHs2cDQ4dyHnUinfHlCzBtGjB5MhAbK67ArV0L1K8vd2SUDrpwbvLx8UlU3V1PTw/W1taoUqVKqrqib926FcOHD8fs2bNRunRpBAQEYNCgQZg3bx68vLySfU1gYCCcnZ1x+PBh1K1bN9l1oqKiEBUVpXocFhYGBwcHrT6mAFCqFHD7NnDoEP+NiYiyskyZgu3FixdYunQp7ty5AwAoWbIkevXqhSlTpqSqymtm04UvQhktLk4k5gsWiMeDBgFz57IWFZFOuXAB+PlnUWwCAAYMAGbMEF3iSedkp3OTg4MDRo4cib59+6qWTZkyBRs3blR9p0iOtbU1pkyZgl9++UWt/ejCMY2NFdOjRkcDQUFAoUJyR0RERBklw6dgAwB7e3tMnToVu3btwq5duzBlyhS8f/8eq1evTusmKZPo6QHz5olWdEAk6x07ilZ2ItIRlSqJwnLKRGfRIuCHH4BLl+SNi7KstWvXYseOHUmW79ixA+vWrVN7OxEREdD76qqwvr5+ilO4Pnv2DG/fvoWdnZ36AeuAp09Fgm5oCOhI73wiIsoEbDvNphQKYNgwYONGMdx1+3agYUMgNFTuyIhIbaamwJIlwIEDYk71O3eAH38Epk7lvOqkcdOnT0fevHmTLM+XLx+mTZum9naaNWuGqVOnYv/+/Xj06BH27NmDefPmoVWrVgCA8PBwDB8+HGfPnsWjR4/g7++PFi1aoEiRInB3d9fY+9EGyvHozs6Avr68sRARkfZgkp7Nde4M/PsvYGYGHDsGVK8uCkkTkQ5p2BC4cQNo00Yk52PGADVqAA8fyh2Z/L58ER9uI0YAdeqIngd79/KKZBo8efIETk5OSZY7OjriyZMnam9n8eLFaNu2Lfr06YOSJUti2LBh+OWXXzB58mQAolX92rVraN68OYoVKwZvb29UqFABJ06cyHJzpbNoHBERJSeH3AGQ/OrVA44fBxo1Aq5fF3OpHzwopm4jIh1hZQXs2CG6x/TrB5w5A7i6iincevTIXtUhnz0TvQsOHBDzTX78GP/c0aPAsmWi2bJyZVGpq359oEoV0a2Ivilfvny4du0aCn01cPrq1auwsrJSezu5cuXCggULsEBZGOUrJiYm8PX1TUekuoNJOhERJSdVSXrr1q1TfP7Dhw/piYVkVL68+E7fsKGoQ+XmBvz9t/hJRDpCoQC6dBGt6F5ewH//Ab16Afv2AX/+CdjYyB1hxoiOBk6dik/Mb9xI/Ly1tfhw++kn4No1wM9PfNCdOSNukyYBuXIBtWvHJ+3FimWvCxtq6NixIwYMGIBcuXKhRo0aAMRsLwMHDkSHDh1kjk43MUknIqLkpCpJt7Cw+O7znp6e6QqI5OPkJL7nNm0KnDsnWti3bAFatpQ7MiJKFUdH4MgR0Yr+++/AP/8ALi7AqlVAixZyR6cZT58mbi0PD49/Tk9PtIw3aiRuP/yQdPqKx49Fsu7nB/j7A2/fiosZ+/aJ5x0c4hP2unVFop/NTZ48GY8ePULdunWRI4f4+hAXFwdPT89UjUmneEzSiYgoOWmegk1X6cKULHL79Ano0EF8r9fTA5YuBX79Ve6oiChNrl8XU7VduyYee3uL5D1XLnnjSq3oaODkyfjE/ObNxM/nyydayxs1Eol1KrpfIy5OVMpXJu0nT4r9JVS+PNCggdi2mxtgbJz+95SALp2b7t+/j4CAAJiYmMDFxQWOjo5yh5QsbT+mMTFixsSYGODJE1Z3JyLK6jJlnnRdpe0nbW0REyMSc+WMemPGiB6h7P1JpIOiooBx48S8i5Ikus1s2KD941mePIlPyv39k7aW//hjfGt5+fJJW8vTKiICOHECOHRIJO3Xryd+3thYDClQtrSXLZvuD0eemzRP24/pw4dAkSLiz+nTJ839+RIRkXZikp4CbT9paxNJAiZOFDcA6N4dWLGCtZWIdNbx44Cnp+jqracHNGkimu9sbMQUbsqfyvsabi1WiYsDXr8WU0k8fy4KvSV3Pyws8esStpY3aADkyZMx8X0tOFh0qVe2tL98Gf9c3rzAq1fpzrB04dzUpk0bVK5cGSNGjEi0fNasWbhw4UKyc6jLSduP6cGD4k+5TJmk14GIiCjrSc15idXd6ZsUCmDCBMDeHujdG1izRnxX3b4dyJlT7uiIKNVq1BDd3gcMANatE9UhU2Jh8e0EPuHPfPkAQ0Pxmuho4MWL5BNv5c8XL8TUaN+Tka3lqWFrK4YM/PyzuHp561Z8wm5nl22aQI8fP44JEyYkWd6oUSPMnTs38wPScRyPTkRE38Iknb6rVy/xHdXDQ8ypXqeOqEnFRJ1IB5mbAz4+QM+eQECAuPIWHCxagxP+jI4Wc4mHhopK6N+TJ4+Y1uz1a/XiUChEkp8/P1CggPj59X0HB+37oFEogNKlxW3QILmjyVTh4eEwVF6MScDAwABhX/d6oO9SJulFisgbBxERaR8m6aSW5s1FYt60KXD+vCgYvXCh3FERUZq5uX17TLokieQ8ueRd+VN5/9UrUcTi3bv41xsZJZ90J7xvZ8exMzrGxcUF27Ztw7hx4xIt37p1K0qVKiVTVLqLLelERPQtTNJJbVWrAps3iyGhixcDbdsC1avLHRURaZxCAVhailuJEimvGxcHvH8vkvaYGJGAW1mxymQWNHbsWLRu3RoPHz5EnTp1AAD+/v7YvHkzdu7cKXN0uodJOhERfQuTdEoVd3dRQG7NGvHz6lXA1FTuqIhINnp6IilPzZRnpJOaNWuGvXv3Ytq0adi5cydMTEzg6uqKI0eOIE9mFfHLIr58AR49EveZpBMR0deyR7Ub0qi5c0Vj2YMHwNixckdDRESZpUmTJjh16hQ+ffqEwMBAtG/fHsOGDYOrq6vcoemUoCAgNlZc5La3lzsaIiLSNkzSKdUsLYGVK8X9+fOB06dlDYeIiDLR8ePH4eXlBXt7e8ydOxd16tTB2bNn5Q5LpyQsGseRIURE9DUm6ZQmjRsDXl6ivlT37kBkpNwRERFRRgkODsaMGTNQtGhRtGvXDubm5oiKisLevXsxY8YMVKpUSe4QdQrHoxMRUUqYpFOazZ8vCjTfvSvmUycioqynWbNmKF68OK5du4YFCxbgxYsXWLx4sdxh6TQm6URElBIm6ZRmuXMDK1aI+3PmAOfOyRsPERFp3oEDB+Dt7Y2JEyeiSZMm0NfXlzsknccknYiIUsIkndKleXOgc2cxC1P37sDnz3JHREREmnTy5El8/PgRFSpUQJUqVbBkyRK8efNG7rB0GpN0IiJKCZN0SreFCwEbG+DWLWDSJLmjISIiTfrxxx+xatUqvHz5Er/88gu2bt0Ke3t7xMXFwc/PDx8/fpQ7RJ0SFQU8eSLuM0knIqLkMEmndLOyApYvF/dnzQIuXpQ3HiIi0rycOXOie/fuOHnyJK5fv46hQ4dixowZyJcvH5o3by53eDrj1i3R+yxPHnGBm4iI6GtM0kkjWrUCPDzEvK/duomWAiIiypqKFy+OWbNm4dmzZ9iyZYvc4eiUgADx09WV068REVHymKSTxixeDFhbAzduAFOnyh0NERFlNH19fbRs2RL79u2TOxSdoUzSy5WTMwoiItJmTNJJY6ytgaVLxf3p04ErV+SNh4iISNtcvSp+MkknIqJvYZJOGtWuHdCmDRATI7q9R0fLHREREZF2kKTE3d2JiIiSwySdNG7pUlFM7upVYMYMuaMhIiLSDk+eAKGhgIEBULKk3NEQEZG2YpJOGmdjI8anA8DkycC1a/LGQ0REpA2UreilSgGGhrKGQkREWoxJOmWIDh2Ali3ju71/+SJ3RERERPLieHQiIlIHk3TKEAqFmDs9d27g8mUxfzoREVF2xvHoRESkDtmT9KVLl6JQoUIwNjZGlSpVcP78+RTX//DhA/r27Qs7OzsYGRmhWLFi+PfffzMpWkoNW1tg0SJxf9Ik4OZNeeMhIiKSE1vSiYhIHbIm6du2bcOQIUMwfvx4XL58Ga6urnB3d0dISEiy60dHR6N+/fp49OgRdu7cibt372LVqlXInz9/JkdO6urcGWjaVFR579ZNdH8nIiLKbkJDgcBAcZ8t6URElBJZk/R58+ahZ8+e6NatG0qVKoUVK1bA1NQUa9asSXb9NWvW4N27d9i7dy/c3NxQqFAh1KxZE64822kthQJYsQKwsAAuXADmzpU7IiIiosynLKLq4ADkySNvLEREpN1kS9Kjo6Nx6dIl1KtXLz4YPT3Uq1cPZ86cSfY1+/btQ9WqVdG3b1/Y2NigTJkymDZtGmJjY7+5n6ioKISFhSW6UebKnx9YsEDcHz8euH1b1nCIiIgyHbu6ExGRumRL0t+8eYPY2FjY2NgkWm5jY4Pg4OBkXxMYGIidO3ciNjYW//77L8aOHYu5c+diypQp39zP9OnTYWFhobo5ODho9H2Qery8gEaNgKgooHt3IIXrKkRERFkOi8YREZG6ZC8clxpxcXHIly8fVq5ciQoVKsDDwwOjR4/GihUrvvmaUaNGITQ0VHV7+vRpJkZMSgoF8McfgLk5cPZsfMs6ERFRdsCWdCIiUpdsSXrevHmhr6+PV69eJVr+6tUr2NraJvsaOzs7FCtWDPr6+qplJUuWRHBwMKKjo5N9jZGREczNzRPdSB4ODvFj0seMAe7dkzceIiKizBATA1y/Lu6zJZ2IiL5HtiTd0NAQFSpUgL+/v2pZXFwc/P39UbVq1WRf4+bmhgcPHiAuLk617N69e7Czs4OhoWGGx0zp5+0N1K8PfP7Mbu9ERJQ93LsnhnuZmQGFC8sdDRERaTtZu7sPGTIEq1atwrp163D79m307t0bnz59Qrdu3QAAnp6eGDVqlGr93r174927dxg4cCDu3buH/fv3Y9q0aejbt69cb4FSSaEA/vxTfFE5dQpYskTuiIiIiDKWcjx62bKAnk4NNCQiIjnkkHPnHh4eeP36NcaNG4fg4GCUK1cOBw8eVBWTe/LkCfQSnM0cHBzg6+uLwYMHo2zZssifPz8GDhyIESNGyPUWKA0KFgRmzwZ69wZGjQKaNAGKFJE7KiIioozB8ehERJQaCkmSJLmDyExhYWGwsLBAaGgox6fLKC5OdHs/cgSoUQM4epStC0SUffHcpHnadEzd3YFDh0QB1V69ZA2FiIhkkprzEtMikoWenuj2njMncPy4GKv++bPcUREREWmesrs7W9KJiEgdTNJJNk5OwLJlImH38REt6pwhj4iIspLgYCAkRJzrypSROxoiItIFTNJJVp6ewMGDQJ48wIULQIUKwH//yR0VERGRZihb0YsVA0xNZQ2FiIh0BJN0kl39+sDFi6Ib4OvXQN26wMKFQPaqlkBERFkRi8YREVFqMUknreDkJKZk+/lnMXf6oEFAly5ARITckREREaWdsiXd1VXWMIiISIcwSSetYWoKrF8vWtH19YFNmwA3NyAoSO7IiIiI0oYt6URElFpM0kmrKBTAgAGAvz9gbS1aICpWBPz85I6MiIgodSIjgbt3xX22pBMRkbqYpJNWqlkTuHQJqFQJePcOaNgQmDWL49SJiEh33LgBxMUB+fIBtrZyR0NERLqCSTppLQcHMYd69+7iS86IEYCHBxAeLndkRERE35dwPLpCIWsoRESkQ5ikk1YzNgb+/BNYvhwwMAB27AB+/BF48EDuyIiIiFKmTNI5Hp2IiFKDSTppPYUC+PVX4Ngx0V3w5k0xTn3/frkjIyIi+jYWjSMiorRgkk46o1o14PJl8TM0FGjWDJg8WXSFJyIi0iZxcfFJOovGERFRajBJJ51iZwccPQr06SOKyI0bB7RuLZJ2IiIibREUJGqoGBkBxYvLHQ0REekSJumkcwwNgaVLgTVrxJefv/4CqlQBbt+WOzIiIiJBOR69TBkgRw5ZQyEiIh3DJJ10VrduwIkTQIECYh7aypWBPXvkjoqIiIjj0YmIKO2YpJNOq1RJzKdes6boVti6NTBmDBAbK3dkRESUnSWcfo2IiCg1mKSTzsuXD/DzAwYNEo+nTgV++kkk70REpF1iY2MxduxYODk5wcTEBM7Ozpg8eTIkSVKtI0kSxo0bBzs7O5iYmKBevXq4f/++jFGnHqdfIyKitGKSTlmCgQEwfz6wcSOQMydw9qxoZf/lF+DNG7mjIyIipZkzZ2L58uVYsmQJbt++jZkzZ2LWrFlYvHixap1Zs2Zh0aJFWLFiBc6dO4ecOXPC3d0dnz9/ljFy9b17Bzx9Ku6XLStvLEREpHuYpFOW0rmzGJ/eqZOo/r5yJVCsmCg0FxMjd3RERHT69Gm0aNECTZo0QaFChdC2bVs0aNAA58+fByBa0RcsWIAxY8agRYsWKFu2LNavX48XL15g79698gavJuV4dCcnwMJC3liIiEj3MEmnLCd/fmDTJuD4cTEW8P17oF8/oGJFUWiOiIjkU61aNfj7++PevXsAgKtXr+LkyZNo1KgRACAoKAjBwcGoV6+e6jUWFhaoUqUKzpw5883tRkVFISwsLNFNLiwaR0RE6cEknbKs6tWBixdFK3ru3OJLU40aorX9+XO5oyMiyp5GjhyJDh06oESJEjAwMED58uUxaNAgdO7cGQAQHBwMALCxsUn0OhsbG9VzyZk+fTosLCxUNwcHh4x7E9/BonFERJQeTNIpS8uRA+jTB7h3T4xPVyiAzZuB4sWBmTOBqCi5IyQiyl62b9+OTZs2YfPmzbh8+TLWrVuHOXPmYN26dena7qhRoxAaGqq6PVUOCpcBW9KJiCg9mKRTtpA3L7BiBXDhAlC1KvDpEzByJODiAhw8KHd0RETZx/Dhw1Wt6S4uLujSpQsGDx6M6dOnAwBsbW0BAK9evUr0ulevXqmeS46RkRHMzc0T3eQQHQ3cvCnusyWdiIjSgkk6ZSsVKgAnTwLr1gE2NsD9+0CjRkCLFkBgoNzRERFlfREREdDTS/z1Q19fH3FxcQAAJycn2Nrawt/fX/V8WFgYzp07h6pVq2ZqrGlx5w7w5YsoGOfoKHc0RESki5ikU7ajpwd4eoou8EOHii7x+/YBpUoBY8cCERFyR0hElHU1a9YMU6dOxf79+/Ho0SPs2bMH8+bNQ6tWrQAACoUCgwYNwpQpU7Bv3z5cv34dnp6esLe3R8uWLeUNXg0Jx6MrFLKGQkREOiqH3AEQycXcHJgzB/D2BgYOBPz8gClTRCv73LlA27bp+4IVGipa5x8+THwLDBStK6tXA87Omns/RES6YPHixRg7diz69OmDkJAQ2Nvb45dffsG4ceNU6/z222/49OkTevXqhQ8fPuCnn37CwYMHYWxsLGPk6lEm6RyPTkREaaWQJEmSO4jMFBYWBgsLC4SGhso2Xo20jyQBe/cCgwcDjx+LZXXqAIsWAaVLJ/+auDggODhpEq68vX2b8j6trMQ+f/pJk++EiHQRz02aJ9cxrVsXOHJEXIjt3j3TdktERFouNecltqQTQbSYt2oFuLsDs2aJyu9Hjojuiv36ieVfJ+FBQUBkZMrbzZdPtJYXLix+OjuLedxHjhTTw9WtC6xdC3TqlDnvk4iIMo4ksSWdiIjSjy3pRMl49AgYMgTYsyfl9fT1gYIF4xPwhLfChYFcuZJ/3adPQJcu8dufMAEYN47jF4myK56bNE+OY/rsGeDgIGqdfPwI6EDvfCIiyiRsSSdKp0KFgN27xTj18eOB8PCkCbizsxhbbmCQ+u3nzAns3Cla1GfPFkn6/fuie6SRkabfDRERZQZlK3qJEkzQiYgo7ZikE6Wgfn1xywh6eqJrfdGiQO/ewKZNYjz8nj1iXnciItItV6+Kn+zqTkRE6cEp2Ihk1rMncPCgmFP35Engxx+Bu3fljoqIiFIr4fRrREREacUknUgL1KsHnD4tutk/fCgS9aNH5Y6KiIhSgy3pRESkCUzSibREqVLAuXMiQf/wAWjQQFR+JyIi7ffxI/DggbjPlnQiIkoPJulEWiRfPjH1m4cHEBMj5tj9/XcxJzsREWmv69fFFGz29oC1tdzREBGRLmOSTqRlTEyAzZuBMWPE4+nTRdL+vTnZiYhIPsqu7mxFJyKi9GKSTqSF9PSAyZOBdevEFG87dwK1agGvXskdGRERJUdZNI7j0YmIKL2YpBNpMU9P4PBhIE8e4Px5oEoV4MYNuaMiIqKvsWgcERFpCpN0Ii1XowZw9qyYT/3xY8DNDfD1lTsqIiJSio0Frl0T99ndnYiI0otJOpEOKFoUOHNGJOxhYUCTJsCKFXJHRUREgKjqHhkJmJoCRYrIHQ0REek6JulEOsLKCjh0SHSBj40FevcGhgwR94mISD7K8eguLoC+vqyhEBFRFsAknUiHGBkBPj7AlCni8fz5QOvWQHi4rGEREWVrHI9ORESaxCSdSMcoFMDo0cCWLSJp37cPqF4dOHpUzNFLRESZS9mSzvHoRESkCUzSiXRUhw4iMbe2Fl8Q69QBKlYENm0CvnyROzoiouyD068REZEmMUkn0mFVqwKXLonx6SYmwOXLwM8/A05OwKxZwIcPckdIRJS1hYQAL1+KXk4uLnJHQ0REWQGTdCId5+AALFsGPH0qxqrb2ADPnwMjRojnBg0CgoLkjpKIKGtSjkcvUgQwM5M3FiIiyhqYpBNlEVZWYqz648fAmjVAmTKioNzCheLLY7t2Yr51IiLSHBaNIyIiTWOSTpTFGBkB3boB164Bvr5AgwZAXBywc6foHl+tGrBrF6duIyLSBBaNIyIiTWOSTpRFKRQiQff1FQl7166AgQFw5gzQti1QrBiweDGnbyMiSg+2pBMRkaYxSSfKBlxcgLVrRVf40aOBPHmAwEBgwAAxbn3kSDGOnYiI1Pf5M3D7trjPlnQiItIUJulE2YidnSgu9+QJsHSpGKv+4QMwcyZQqBDQpUt8100iIkrZrVti6JCVFZA/v9zREBFRVqEVSfrSpUtRqFAhGBsbo0qVKjh//rxar9u6dSsUCgVatmyZsQESZTE5cwJ9+gB37gB79wLVqwMxMcDGjUD58kDdusDmzewKT0SUkoTj0RUKWUMhIqIsRPYkfdu2bRgyZMj/2rvzqCiufA/g32YViICCbIooRsWVKFEeJiaojIg+gxkSlzCKxl10NMb3TKIGTd6MmZhjNMbB6CjOjIlGc9QYjXoUt8S4JGIUDcOog6hPAZewCLI8+r4/7tDaQrPZ3VVUfz/n1KG7uqr63qoufv3ruvcWkpKSkJaWhtDQUERHRyMvL6/W9a5evYp58+ahf//+ViopkfbY2wOxscCxY8Dp08CoUXLeoUNAfLy8nduYMcCuXUBZmdKlJSJSl6oknf3RiYjInBRP0pcvX47JkydjwoQJ6Nq1K9asWQNXV1ds2LDB5DqVlZWIj4/HkiVLEBwcbMXSEmlXnz7Ali3AlSuy33pwMFBSIufFxgJ+fsCkSUBqKkeGJyICHg4ax/7oRERkToom6eXl5Thz5gyioqIM8+zs7BAVFYUTJ06YXO+9996Dj48PJk6cWOd7lJWVobCw0GgiItOCgmS/9cuXgVOngDlzZF/2/Hxg/XogKgpo0waYPVved10IpUtMRGR9QnBkdyIisgxFk/Q7d+6gsrISvr6+RvN9fX2Rk5NT4zrff/891q9fj3Xr1tXrPZYuXQoPDw/DFBgY+MTlJrIFOh3Qty/w8cfA9euyCfykSUCLFkBODvDJJ/K+608/La+8X7igdImJiKwnOxsoKACcnICQEKVLQ0REWqJ4c/eGKCoqwtixY7Fu3Tp4e3vXa523334bBQUFhun69esWLiWR9tjbAwMGAOvWyQR91y7ZV93VVd7K7Y9/lLd569FDPs7KUrrERESWVdUfvWtXmagTERGZi4OSb+7t7Q17e3vk5uYazc/NzYWfn1+15a9cuYKrV69i+PDhhnl6vR4A4ODggMzMTHTo0MFoHWdnZzg7O1ug9ES2yckJGD5cTsXFwDffAJs3A3v3yqvpCxbIKTxcJvIjR8rm8kREWsKm7kREZCmKJulOTk4ICwtDamqq4TZqer0eqampmDlzZrXlQ0JCkJ6ebjRv4cKFKCoqwsqVK9mUncjK3NyA0aPl9OuvwPbt8tZthw/L/uynTgFz58qr8L17y6bynp6m//JqFBE1FY/efo2IiMicFE3SAWDu3LlISEjAs88+i759+2LFihUoLi7GhAkTAADjxo1D69atsXTpUjRr1gzdu3c3Wt/T0xMAqs0nIutq0QKYOFFOt24BW7fKK+ynTskR4VNT696Gq+vDpL2uhL5XLznIHRGREnglnYiILEXxJH3UqFG4ffs23n33XeTk5OCZZ57Bvn37DIPJXbt2DXZ2TarrPJHN8/eXo7/Pni37rO/cCfzv/8qr7fn51f8WFMj1SkrkdPNm3e/h4CBHnl+0CHB3t1hViIiqyc9/OPYGr6QTEZG56YSwrRsoFRYWwsPDAwUFBXDnN3siVaisBAoLTSfxv/5q/Dgn52FTUz8/YNkyID5ejkhP1BQxNpmfJffpsWPAiy8CbdvKUd6JiIjq0pC4pPiVdCIie/uHTdzra+9eeaX+0iVg7FhgzRrg00/Z9JSILK+qqTuvohMRkSWwHTkRNUkxMUB6OrB0qezLfvw4EBYGJCYC9+4pXToi0rKqljz8UZCIiCyBSToRNVnOzsBbbwGZmXKEeb0e+POfgU6dgLVrZTN6IiJz46BxRERkSUzSiajJa9NGjiR/+DDQvTtw9y4wdaq8V/vJk0qXjoi05P/+D7hwQT5mc3ciIrIEJulEpBmRkcDZs8DKlXLE9zNngIgIYMIEIDdX6dIRkRZkZgJlZUDz5kD79kqXhoiItIhJOhFpioMD8PvfA//8p0zOAWDjRtkEfuVKoKJC0eIRURNX1R+9Z0+Ad4glIiJLYHghIk3y9QU2bABOnJADyhUWyvuq9+olm8UTETUG+6MTEZGlMUknIk37j/8ATp2SA8l5eQEXLwIDBwKjRgHXrytdOiJqaqqupLM/OhERWQqTdCLSPHt7YPJk2QQ+MVE2Ud26FQgJAf74R9m/lIioLkLw9mtERGR5TNKJyGa0bAl8+imQlgY8/zxQUgIsWCBHhP/6azlqMxGRKTk5wO3b8oe+7t2VLg0REWkVk3QisjmhocCxY8CmTYC/P3D5MjBihHw8aRKwdy+vrhNRdVVX0Tt3BlxcFC0KERFpGJN0IrJJOh0QHy9vpzR/vuyvfucOsH49MHQo4OMjX9++HSguVrq0RKQGHDSOiIisgUk6Edm05s2BDz6QzVhTU2WfdX9/ORr8F18AcXFAq1by7+efAwUFSpeYiJTCQeOIiMgamKQTEUHeX33gQNln/cYN4IcfgDffBNq1Ax48kFfUf/c7mbAPHQr85S+ybyoR2Q5eSSciImtgkk5E9Bg7OyAiAvjoI+Bf/5IDzS1cCHTpAlRUyD7rkycDfn7AgAHAqlUysSci7SopkXeIAHglnYiILItJOhFRLXQ6oFcv4P33gV9+kdP//A/Quzeg1wNHjgC//z0QGCjvyb5sGXDlitKlJiJzu3BBnvO+vvIHOiIiIktxULoARERNSZcu8rZtCxYAWVnAjh2yKfwPPwCnTsnpv/9bXmmbOhUYNw5wc1O61JZTWir76Z4+Lfv1u7g8nFxda35c02sOjEakcuyPTkRE1sKvRUREjdS+PTB3rpxu3XqYsB85IvuuzpgBvPMOMGUKMHOmvNrelOn1wKVL8oeI06fl33PnZBeAJ+XgUD15d3aWrwkh31sI46m+86rmOzoCMTHyWHTr9uRlJttSlaSzPzoREVmaTgghlC6ENRUWFsLDwwMFBQVwd3dXujhEpEF378p7sK9a9bDpu729HCF+zhzZ370pyM19mIyfPg38+COQn199uVatgPBwIDhYXll/8ED2333woPbHDx5YvUoGAwYAs2YBw4er4yo+Y5P5mXufPvecbDHz+efAa6+ZoYBERGRTGhKXmKQTEVlIZSWwZw+wYgVw+PDD+eHhMlmPi5NXd9WgpEQOkPfoVfLs7OrLNWsGhIXJOvTtK/8GBcm++w0lRO1JfWmpHMRPpzOeappnav6j8/LygHXrgJ075ZV1AGjbFpg+HZg0CfD2fqJd+ERsKTa1a9cO2TV8uGbMmIHVq1cjMjISR48eNXpt6tSpWLNmTYPex5z7VK8H3N2B4mLg4kWga9cn2hwREdkgJum1sKUvQkSkHufOAStXyqtw5eVyXuvWsun15MmAl5f1ylJZCWRkGF8lT0+X8x+l08lkpCoZ79sX6N5dPT8sNNa1a8CaNTJhv3NHznN2BsaMkccjLMz6ZbKl2HT79m1UPvJhu3DhAn7zm9/g8OHDiIyMRGRkJDp16oT33nvPsIyrq2uD94s59+nly0DHjvJHqqIidbS+ICKipoVJei1s6YsQEalPbi7w2WfAn/8sHwOy//W4ccDs2XJgOnO7cUMm4lVJ+U8/AffvV18uIMD4CnlYmLx6qFWlpcCXX8puCWfOPJwfESGT9VdeAZycrFMWW45Nc+bMwe7du3Hp0iXodDpERkbimWeewYoVK55ou+bcp199Bbz6KvDss7LbBxERUUMxSa+FLX8RIiL1KCuTCeLHHz8ckAoAoqNlU/jBg2VT7YYqLJRJ+KNXyW/erL7cU089bLZeNbVu3djaNG1CyH21ahWwbdvDgfB8feUI/VOnyh8wLMlWY1N5eTkCAgIwd+5cvPPOOwCAyMhIXLx4EUII+Pn5Yfjw4Vi0aBFcXV1r3VZZWRnKysoMzwsLCxEYGGiWfbpokbz14qRJsgUGERFRQzFJr4WtfhEiInUSAvjuO9lvfedO+RwAQkLklfWxY03fwq2iQjZTf/QqeUbGw21UsbcHevQwbrbepYucT8ZycoC1a2Vz+Fu35DwHBzl+wKxZQL9+jet/XxdbjU1bt27Fa6+9hmvXriHg37+ErF27FkFBQQgICMD58+cxf/589O3bF9u3b691W4sXL8aSJUuqzTfHPh0+HNi9W/6QM3PmE22KiIhsFJP0WtjqFyEiUr9//Qv49FPgL3+R/V4BoEULeQu3xESZlD+akKelySbbj2vXzjgh791b3tqM6q+iQt5O79NPge+/fzj/mWdksj5mjOymYC62Gpuio6Ph5OSEb775xuQyhw4dwqBBg3D58mV06NDB5HKWvJLeti1w/br8Qe35559oU0REZKOYpNfCVr8IEVHTUVgIpKQAn3wiE/faeHoaJ+R9+shm2mQ+Z88Cq1fLQf+qfhRp2VI2fZ4+Xf4o8qRsMTZlZ2cjODgY27dvR2xsrMnliouL8dRTT2Hfvn2Ijo6u9/bNtU/v3n048n9BgbbHaSAiIstpSFxqRI9HIiKyJHd32dT9n/+UTeAjI+V8JyeZiM+cCfz970BmJnDvHrB/P/Dee8B//icTdEvo1Uu2brhxA/jwQ5mU37snH4eGKnu/96YsJSUFPj4+GDZsWK3L/fzvQRv8/f2tUKrqzp2Tf4ODmaATEZF18CYiREQqZW8PxMbKKS8P8PCQtwojZXh5Af/1X8DcucCePbIp/NNPm7fZu63Q6/VISUlBQkICHB65n9mVK1fwxRdfYOjQofDy8sL58+fxxhtv4IUXXkDPnj0VKauDAxAVBQQFKfL2RERkg5ikExE1AT4+SpeAqtjbAy+9JKfH7y1P9XPw4EFcu3YNr7/+utF8JycnHDx4ECtWrEBxcTECAwMRFxeHhQsXKlRS4IUXgAMHFHt7IiKyQUzSiYiIGokj5DfO4MGDUdOQOIGBgTh69KgCJSIiIlIP9kknIiIiIiIiUgkm6UREREREREQqwSSdiIiIiIiISCWYpBMRERERERGpBJN0IiIiIiIiIpVgkk5ERERERESkEkzSiYiIiIiIiFSCSToRERERERGRSjBJJyIiIiIiIlIJJulEREREREREKuGgdAGsTQgBACgsLFS4JERERFJVTKqKUfTkGO+JiEhNGhLrbS5JLyoqAgAEBgYqXBIiIiJjRUVF8PDwULoYmsB4T0REalSfWK8TNvazvV6vx82bN9G8eXPodDqli9NohYWFCAwMxPXr1+Hu7q50cRpNK/UAWBc10ko9AO3URSv1AMxbFyEEioqKEBAQADs79kQzBy3Ee54v6qOVegDaqYtW6gFopy5aqQegXKy3uSvpdnZ2aNOmjdLFMBt3d/cm/+EHtFMPgHVRI63UA9BOXbRSD8B8deEVdPPSUrzn+aI+WqkHoJ26aKUegHbqopV6ANaP9fy5noiIiIiIiEglmKQTERERERERqQST9CbK2dkZSUlJcHZ2VrooT0Qr9QBYFzXSSj0A7dRFK/UAtFUXUictfca0Uhet1APQTl20Ug9AO3XRSj0A5epicwPHEREREREREakVr6QTERERERERqQSTdCIiIiIiIiKVYJJOREREREREpBJM0omIiIiIiIhUgkm6Ci1duhR9+vRB8+bN4ePjgxEjRiAzM7PWdTZu3AidTmc0NWvWzEolNm3x4sXVyhUSElLrOtu2bUNISAiaNWuGHj164Ntvv7VSaU1r165dtXrodDokJibWuLyajsexY8cwfPhwBAQEQKfTYefOnUavCyHw7rvvwt/fHy4uLoiKisKlS5fq3O7q1avRrl07NGvWDOHh4Th9+rSFaiDVVo+KigrMnz8fPXr0gJubGwICAjBu3DjcvHmz1m025vNpDnUdk/Hjx1cr15AhQ+rcrpqOCYAazxmdTodly5aZ3KYSx6Q+/3NLS0uRmJgILy8vPPXUU4iLi0Nubm6t223suUW2gbGesd6ctBLrAe3Ee63EeoDxXol4zyRdhY4ePYrExEScPHkSBw4cQEVFBQYPHozi4uJa13N3d8etW7cMU3Z2tpVKXLtu3boZlev77783uewPP/yAMWPGYOLEiTh79ixGjBiBESNG4MKFC1YscXU//vijUR0OHDgAAHj11VdNrqOW41FcXIzQ0FCsXr26xtc//PBDfPLJJ1izZg1OnToFNzc3REdHo7S01OQ2v/zyS8ydOxdJSUlIS0tDaGgooqOjkZeXZ6lq1FqPkpISpKWlYdGiRUhLS8P27duRmZmJl156qc7tNuTzaS51HRMAGDJkiFG5Nm/eXOs21XZMABiV/9atW9iwYQN0Oh3i4uJq3a61j0l9/ue+8cYb+Oabb7Bt2zYcPXoUN2/exG9/+9tat9uYc4tsB2M9Y705aSXWA9qJ91qJ9QDjvSLxXpDq5eXlCQDi6NGjJpdJSUkRHh4e1itUPSUlJYnQ0NB6Lz9y5EgxbNgwo3nh4eFi6tSpZi7Zk5k9e7bo0KGD0Ov1Nb6u1uMBQOzYscPwXK/XCz8/P7Fs2TLDvPz8fOHs7Cw2b95scjt9+/YViYmJhueVlZUiICBALF261CLlftzj9ajJ6dOnBQCRnZ1tcpmGfj4toaa6JCQkiNjY2AZtpykck9jYWDFw4MBal1HDMXn8f25+fr5wdHQU27ZtMyyTkZEhAIgTJ07UuI3GnltkuxjrGevNRSuxXgjtxHutxHohGO8fZ6l4zyvpTUBBQQEAoGXLlrUud//+fQQFBSEwMBCxsbG4ePGiNYpXp0uXLiEgIADBwcGIj4/HtWvXTC574sQJREVFGc2Ljo7GiRMnLF3MeisvL8emTZvw+uuvQ6fTmVxOrcfjUVlZWcjJyTHa5x4eHggPDze5z8vLy3HmzBmjdezs7BAVFaWq41RQUACdTgdPT89al2vI59Oajhw5Ah8fH3Tu3BnTp0/H3bt3TS7bFI5Jbm4u9uzZg4kTJ9a5rNLH5PH/uWfOnEFFRYXR/g0JCUHbtm1N7t/GnFtk2xjrGestRcuxHmja8V5rsR5gvAfME++ZpKucXq/HnDlz8Nxzz6F79+4ml+vcuTM2bNiAr7/+Gps2bYJer0e/fv1w48YNK5a2uvDwcGzcuBH79u1DcnIysrKy0L9/fxQVFdW4fE5ODnx9fY3m+fr6IicnxxrFrZedO3ciPz8f48ePN7mMWo/H46r2a0P2+Z07d1BZWanq41RaWor58+djzJgxcHd3N7lcQz+f1jJkyBD87W9/Q2pqKv70pz/h6NGjiImJQWVlZY3LN4Vj8te//hXNmzevs8mY0sekpv+5OTk5cHJyqvYFsLb925hzi2wXY736zg3GevXHFaBpx3stxnqA8b4+69SHQ6PXJKtITEzEhQsX6uyjERERgYiICMPzfv36oUuXLvjss8/w/vvvW7qYJsXExBge9+zZE+Hh4QgKCsLWrVvr9QubGq1fvx4xMTEICAgwuYxaj4ctqKiowMiRIyGEQHJycq3LqvXzOXr0aMPjHj16oGfPnujQoQOOHDmCQYMGKVauJ7FhwwbEx8fXOaiS0sekvv9zicyJsV59GOvVr6nHey3GeoDx3lx4JV3FZs6cid27d+Pw4cNo06ZNg9Z1dHREr169cPnyZQuVrnE8PT3RqVMnk+Xy8/OrNoJibm4u/Pz8rFG8OmVnZ+PgwYOYNGlSg9ZT6/Go2q8N2efe3t6wt7dX5XGqCtjZ2dk4cOBArb+q16Suz6dSgoOD4e3tbbJcaj4mAPDdd98hMzOzwecNYN1jYup/rp+fH8rLy5Gfn2+0fG37tzHnFtkmxnpJTecGY73644oW431Tj/UA431916kPJukqJITAzJkzsWPHDhw6dAjt27dv8DYqKyuRnp4Of39/C5Sw8e7fv48rV66YLFdERARSU1ON5h04cMDol2olpaSkwMfHB8OGDWvQemo9Hu3bt4efn5/RPi8sLMSpU6dM7nMnJyeEhYUZraPX65GamqrocaoK2JcuXcLBgwfh5eXV4G3U9flUyo0bN3D37l2T5VLrMamyfv16hIWFITQ0tMHrWuOY1PU/NywsDI6Ojkb7NzMzE9euXTO5fxtzbpFtYaxnrLcWLcV6QLvxvqnHeoDxvopZ4n2jh5wji5k+fbrw8PAQR44cEbdu3TJMJSUlhmXGjh0r3nrrLcPzJUuWiP3794srV66IM2fOiNGjR4tmzZqJixcvKlEFgzfffFMcOXJEZGVliePHj4uoqCjh7e0t8vLyhBDV63H8+HHh4OAgPvroI5GRkSGSkpKEo6OjSE9PV6oKBpWVlaJt27Zi/vz51V5T8/EoKioSZ8+eFWfPnhUAxPLly8XZs2cNo6B+8MEHwtPTU3z99dfi/PnzIjY2VrRv3148ePDAsI2BAweKVatWGZ5v2bJFODs7i40bN4pffvlFTJkyRXh6eoqcnBxF6lFeXi5eeukl0aZNG/Hzzz8bnTdlZWUm61HX51OJuhQVFYl58+aJEydOiKysLHHw4EHRu3dv0bFjR1FaWmqyLmo7JlUKCgqEq6urSE5OrnEbajgm9fmfO23aNNG2bVtx6NAh8dNPP4mIiAgRERFhtJ3OnTuL7du3G57X59wi28VYz1hvTlqJ9XXVpSnFe63E+rrqUoXx3rzxnkm6CgGocUpJSTEs8+KLL4qEhATD8zlz5oi2bdsKJycn4evrK4YOHSrS0tKsX/jHjBo1Svj7+wsnJyfRunVrMWrUKHH58mXD64/XQwghtm7dKjp16iScnJxEt27dxJ49e6xc6prt379fABCZmZnVXlPz8Th8+HCNn6eq8ur1erFo0SLh6+srnJ2dxaBBg6rVMSgoSCQlJRnNW7VqlaGOffv2FSdPnlSsHllZWSbPm8OHD5usR12fTyXqUlJSIgYPHixatWolHB0dRVBQkJg8eXK1AKz2Y1Lls88+Ey4uLiI/P7/GbajhmNTnf+6DBw/EjBkzRIsWLYSrq6t4+eWXxa1bt6pt59F16nNuke1irGesNyetxPq66tKU4r1WYn1ddanCeG/eeK/79xsRERERERERkcLYJ52IiIiIiIhIJZikExEREREREakEk3QiIiIiIiIilWCSTkRERERERKQSTNKJiIiIiIiIVIJJOhEREREREZFKMEknIiIiIiIiUgkm6UREREREREQqwSSdiCxOp9Nh586dSheDiIiILISxnsh8mKQTadz48eOh0+mqTUOGDFG6aERERGQGjPVE2uKgdAGIyPKGDBmClJQUo3nOzs4KlYaIiIjMjbGeSDt4JZ3IBjg7O8PPz89oatGiBQDZPC05ORkxMTFwcXFBcHAwvvrqK6P109PTMXDgQLi4uMDLywtTpkzB/fv3jZbZsGEDunXrBmdnZ/j7+2PmzJlGr9+5cwcvv/wyXF1d0bFjR+zatcvw2q+//or4+Hi0atUKLi4u6NixY7UvGkRERGQaYz2RdjBJJyIsWrQIcXFxOHfuHOLj4zF69GhkZGQAAIqLixEdHY0WLVrgxx9/xLZt23Dw4EGjwJycnIzExERMmTIF6enp2LVrF55++mmj91iyZAlGjhyJ8+fPY+jQoYiPj8e9e/cM7//LL79g7969yMjIQHJyMry9va23A4iIiDSOsZ6oCRFEpGkJCQnC3t5euLm5GU1/+MMfhBBCABDTpk0zWic8PFxMnz5dCCHE2rVrRYsWLcT9+/cNr+/Zs0fY2dmJnJwcIYQQAQEBYsGCBSbLAEAsXLjQ8Pz+/fsCgNi7d68QQojhw4eLCRMmmKfCRERENoaxnkhb2CedyAYMGDAAycnJRvNatmxpeBwREWH0WkREBH7++WcAQEZGBkJDQ+Hm5mZ4/bnnnoNer0dmZiZ0Oh1u3ryJQYMG1VqGnj17Gh67ubnB3d0deXl5AIDp06cjLi4OaWlpGDx4MEaMGIF+/fo1qq5ERES2iLGeSDuYpBPZADc3t2pN0szFxcWlXss5OjoaPdfpdNDr9QCAmJgYZGdn49tvv8WBAwcwaNAgJCYm4qOPPjJ7eYmIiLSIsZ5IO9gnnYhw8uTJas+7dOkCAOjSpQvOnTuH4uJiw+vHjx+HnZ0dOnfujObNm6Ndu3ZITU19ojK0atUKCQkJ2LRpE1asWIG1a9c+0faIiIjoIcZ6oqaDV9KJbEBZWRlycnKM5jk4OBgGbNm2bRueffZZPP/88/j8889x+vRprF+/HgAQHx+PpKQkJCQkYPHixbh9+zZmzZqFsWPHwtfXFwCwePFiTJs2DT4+PoiJiUFRURGOHz+OWbNm1at87777LsLCwtCtWzeUlZVh9+7dhi8OREREVDfGeiLtYJJOZAP27dsHf39/o3mdO3fGP/7xDwByNNYtW7ZgxowZ8Pf3x+bNm9G1a1cAgKurK/bv34/Zs2ejT58+cHV1RVxcHJYvX27YVkJCAkpLS/Hxxx9j3rx58Pb2xiuvvFLv8jk5OeHtt9/G1atX4eLigv79+2PLli1mqDkREZFtYKwn0g6dEEIoXQgiUo5Op8OOHTswYsQIpYtCREREFsBYT9S0sE86ERERERERkUowSSciIiIiIiJSCTZ3JyIiIiIiIlIJXkknIiIiIiIiUgkm6UREREREREQqwSSdiIiIiIiISCWYpBMRERERERGpBJN0IiIiIiIiIpVgkk5ERERERESkEkzSiYiIiIiIiFSCSToRERERERGRSvw/QQKLRICKgUQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to 2D (e.g., 480x1344)\n",
    "# standardized_data = standardized_data.view(-1, 1, 480, 1344)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset and split it into training and testing sets\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 120 * 336, 128)  # Updated input size\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 120 * 336)  # Updated view size\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # Further reduced learning rate and added L2 regularization\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/best_model.pth'\n",
    "\n",
    "# Load the best model parameters if available\n",
    "try:\n",
    "    load_model(model, model_path)\n",
    "    print(\"Loaded the best model parameters.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved model found. Starting training from scratch.\")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "            total_predictions += batch_x.size(0)\n",
    "    \n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Save the model if it has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        save_model(model, model_path)\n",
    "        print(\"Saved the best model parameters.\")\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n",
    "\n",
    "# Plotting the metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-fold Cross Validation to make sure that the model generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/20], Loss: 2.2281\n",
      "Accuracy on training set: 13.96%\n",
      "Accuracy on validation set: 8.96%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Loss: 2.1677\n",
      "Accuracy on training set: 23.40%\n",
      "Accuracy on validation set: 5.97%\n",
      "Epoch [3/20], Loss: 2.1320\n",
      "Accuracy on training set: 25.66%\n",
      "Accuracy on validation set: 7.46%\n",
      "Epoch [4/20], Loss: 2.0826\n",
      "Accuracy on training set: 27.55%\n",
      "Accuracy on validation set: 7.46%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 46\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[1;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m120\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m336\u001b[39m)  \u001b[38;5;66;03m# Updated view size\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\nn\\functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined\n",
    "# Convert standardized_data and y_one_hot to PyTorch tensors\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to 2D (e.g., 480x1344)\n",
    "# standardized_data = standardized_data.view(-1, 1, 480, 1344)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Split the dataset into training+validation and test sets\n",
    "test_size = int(0.2 * len(dataset))\n",
    "train_val_size = len(dataset) - test_size\n",
    "train_val_dataset, test_dataset = random_split(dataset, [train_val_size, test_size])\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 120 * 336, 128)  # Updated input size\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 9)  # Assuming y is one-hot encoded with 9 classes\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 120 * 336)  # Updated view size\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/test_model.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # Create data loaders for training and validation sets\n",
    "    train_subset = Subset(train_val_dataset, train_idx)\n",
    "    val_subset = Subset(train_val_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, optimizer, and scheduler\n",
    "    model = CNNModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        # Evaluate the model on the training set\n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the model if it has the best accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            save_model(model, model_path)\n",
    "            print(\"Saved the best model parameters.\")\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct_test_predictions = 0\n",
    "total_test_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_test_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_test_predictions += batch_x.size(0)\n",
    "\n",
    "test_accuracy = correct_test_predictions / total_test_predictions * 100\n",
    "print(f\"Accuracy on test set: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Plotting the metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo modify the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/30], Training Loss: 2.2448\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 3.9955\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [2/30], Training Loss: 2.2346\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 4.2240\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [3/30], Training Loss: 2.1875\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 3.3970\n",
      "Accuracy on validation set: 13.25%\n",
      "\n",
      "Epoch [4/30], Training Loss: 2.2029\n",
      "Accuracy on training set: 11.78%\n",
      "Validation Loss: 3.0566\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Epoch [5/30], Training Loss: 2.1818\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 3.0697\n",
      "Accuracy on validation set: 18.07%\n",
      "\n",
      "Epoch [6/30], Training Loss: 2.1805\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.3602\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [7/30], Training Loss: 2.1735\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 2.5074\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [8/30], Training Loss: 2.1452\n",
      "Accuracy on training set: 13.60%\n",
      "Validation Loss: 2.7895\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [9/30], Training Loss: 2.1590\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 2.5311\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [10/30], Training Loss: 2.1516\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.3037\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [11/30], Training Loss: 2.1417\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.3134\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [12/30], Training Loss: 2.0914\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2713\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [13/30], Training Loss: 2.1283\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2831\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [14/30], Training Loss: 2.0984\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2915\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [15/30], Training Loss: 2.0935\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.3347\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [16/30], Training Loss: 2.0636\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.4392\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [17/30], Training Loss: 2.0672\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.4522\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [18/30], Training Loss: 2.0439\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.4326\n",
      "Accuracy on validation set: 9.64%\n",
      "\n",
      "Epoch [19/30], Training Loss: 2.0639\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.3839\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Epoch [20/30], Training Loss: 2.0401\n",
      "Accuracy on training set: 13.29%\n",
      "Validation Loss: 2.3557\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Epoch [21/30], Training Loss: 2.0078\n",
      "Accuracy on training set: 13.29%\n",
      "Validation Loss: 2.3846\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Epoch [22/30], Training Loss: 2.0401\n",
      "Accuracy on training set: 13.29%\n",
      "Validation Loss: 2.4032\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Epoch [1/30], Training Loss: 2.2672\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2112\n",
      "Accuracy on validation set: 10.84%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/30], Training Loss: 2.2680\n",
      "Accuracy on training set: 11.18%\n",
      "Validation Loss: 2.2038\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/30], Training Loss: 2.2347\n",
      "Accuracy on training set: 10.57%\n",
      "Validation Loss: 2.1963\n",
      "Accuracy on validation set: 12.05%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/30], Training Loss: 2.2869\n",
      "Accuracy on training set: 11.75%\n",
      "Validation Loss: 2.1971\n",
      "Accuracy on validation set: 8.54%\n",
      "\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\2952190570.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming y_one_hot and standardized_data are already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32).view(414, 84, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "# y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the CNN model\n",
    "class EEGCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=84, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(512 * 960, 1024)  # Adjust input size based on pooling\n",
    "        self.bn4 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn7 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 9)  # Assuming 9 classes for classification\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 512 * 960)  # Adjust view size based on pooling\n",
    "        x = torch.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn6(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn7(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EEGCNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.01)\n",
    "num_epochs = 30  # Increased number of epochs\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/imf_data_cnn.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = EEGCNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code is old model, not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the CNN model\n",
    "class EEGCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=84, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 3840, 512)  # Adjust input size based on pooling\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)  # Assuming 9 classes for classification\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 256 * 3840)  # Adjust view size based on pooling\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Assuming standardized_data and y_indices are already defined\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_indices = torch.tensor(y_indices, dtype=torch.long)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = EEGCNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model in best-model.pth on a general test data randomly sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7680"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_imf.to_numpy()[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\1827752004.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 9.64%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the best model parameters\n",
    "model = CNNModel()\n",
    "load_model(model, model_path)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming standardized_data and y_one_hot are already defined and converted to tensors\n",
    "# standardized_data = torch.tensor(standardized_data, dtype=torch.float32)\n",
    "# y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Split the dataset into training and test sets (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "        total_predictions += batch_x.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Accuracy on test dataset: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imf_array = X_imf.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 84, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(84):\n",
    "        reshaped_data[i, j, :] = X_imf_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Normalize the data\n",
    "mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "standardized_data = (standardized_data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\832631245.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/20], Loss: 2.2623\n",
      "Accuracy on training set: 16.62%\n",
      "Accuracy on validation set: 19.28%\n",
      "Epoch [2/20], Loss: 2.2065\n",
      "Accuracy on training set: 25.08%\n",
      "Accuracy on validation set: 7.23%\n",
      "Epoch [3/20], Loss: 2.2282\n",
      "Accuracy on training set: 22.36%\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [4/20], Loss: 2.1390\n",
      "Accuracy on training set: 27.79%\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [5/20], Loss: 2.1309\n",
      "Accuracy on training set: 55.59%\n",
      "Accuracy on validation set: 16.87%\n",
      "Epoch [6/20], Loss: 2.1043\n",
      "Accuracy on training set: 56.80%\n",
      "Accuracy on validation set: 14.46%\n",
      "Epoch [7/20], Loss: 2.0346\n",
      "Accuracy on training set: 45.92%\n",
      "Accuracy on validation set: 8.43%\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Epoch [1/20], Loss: 2.2778\n",
      "Accuracy on training set: 11.78%\n",
      "Accuracy on validation set: 16.87%\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/20], Loss: 2.2896\n",
      "Accuracy on training set: 14.50%\n",
      "Accuracy on validation set: 12.05%\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/20], Loss: 2.3001\n",
      "Accuracy on training set: 12.39%\n",
      "Accuracy on validation set: 13.25%\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/20], Loss: 2.2764\n",
      "Accuracy on training set: 16.57%\n",
      "Accuracy on validation set: 14.63%\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\832631245.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming X_imf is already defined as a DataFrame\n",
    "# Convert the DataFrame to a NumPy array\n",
    "X_imf_array = X_imf.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 84, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(84):\n",
    "        reshaped_data[i, j, :] = X_imf_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Normalize each channel separately across the entire dataset\n",
    "mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "standardized_data = (standardized_data - mean) / std\n",
    "\n",
    "# Assuming y_one_hot is already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = standardized_data.view(414, 84, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the CNN model with increased dropout\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=84, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256 * 1920, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.7)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 256 * 1920)\n",
    "        x = torch.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)  # Increased weight decay\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)\n",
    "num_epochs = 20\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/test_model.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model = CNNModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above model wasn't able to generalise for some reason maybe I have to change the learning rate or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\2111946378.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/30], Training Loss: 2.3128\n",
      "Accuracy on training set: 14.50%\n",
      "Validation Loss: 5.6926\n",
      "Accuracy on validation set: 4.82%\n",
      "\n",
      "Epoch [2/30], Training Loss: 2.3133\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.6821\n",
      "Accuracy on validation set: 7.23%\n",
      "\n",
      "Epoch [3/30], Training Loss: 2.2984\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 2.2385\n",
      "Accuracy on validation set: 6.02%\n",
      "\n",
      "Epoch [4/30], Training Loss: 2.2263\n",
      "Accuracy on training set: 12.99%\n",
      "Validation Loss: 2.2197\n",
      "Accuracy on validation set: 8.43%\n",
      "\n",
      "Epoch [5/30], Training Loss: 2.1487\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.2876\n",
      "Accuracy on validation set: 7.23%\n",
      "\n",
      "Epoch [6/30], Training Loss: 2.1321\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.2866\n",
      "Accuracy on validation set: 7.23%\n",
      "\n",
      "Epoch [7/30], Training Loss: 2.0732\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.3295\n",
      "Accuracy on validation set: 6.02%\n",
      "\n",
      "Epoch [8/30], Training Loss: 2.0530\n",
      "Accuracy on training set: 13.90%\n",
      "Validation Loss: 2.3205\n",
      "Accuracy on validation set: 4.82%\n",
      "\n",
      "Epoch [9/30], Training Loss: 1.9881\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.3171\n",
      "Accuracy on validation set: 4.82%\n",
      "\n",
      "Early stopping\n",
      "Fold 2/5\n",
      "Epoch [1/30], Training Loss: 2.3148\n",
      "Accuracy on training set: 13.29%\n",
      "Validation Loss: 7.2710\n",
      "Accuracy on validation set: 3.61%\n",
      "\n",
      "Early stopping\n",
      "Fold 3/5\n",
      "Epoch [1/30], Training Loss: 2.3517\n",
      "Accuracy on training set: 10.88%\n",
      "Validation Loss: 3.0144\n",
      "Accuracy on validation set: 15.66%\n",
      "\n",
      "Early stopping\n",
      "Fold 4/5\n",
      "Epoch [1/30], Training Loss: 2.3491\n",
      "Accuracy on training set: 10.88%\n",
      "Validation Loss: 2.2576\n",
      "Accuracy on validation set: 13.25%\n",
      "\n",
      "Early stopping\n",
      "Fold 5/5\n",
      "Epoch [1/30], Training Loss: 2.3799\n",
      "Accuracy on training set: 12.35%\n",
      "Validation Loss: 2.6425\n",
      "Accuracy on validation set: 8.54%\n",
      "\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\2111946378.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming X_imf is already defined as a DataFrame\n",
    "# Convert the DataFrame to a NumPy array\n",
    "X_imf_array = X_imf.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 84, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(84):\n",
    "        reshaped_data[i, j, :] = X_imf_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Normalize each channel separately across the entire dataset\n",
    "# mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "# std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "# standardized_data = (standardized_data - mean) / std\n",
    "\n",
    "# Assuming y_one_hot is already defined\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the CNN input requirements\n",
    "# Reshape each sample to (batch_size, in_channels, sequence_length)\n",
    "standardized_data = standardized_data.view(414, 84, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the CNN model with increased dropout\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=84, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc1 = nn.Linear(512 * 960, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)  # Ensure the batch size is preserved\n",
    "        # x = x.view(-1, 512 * 960)\n",
    "        x = torch.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer with L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)  # Increased weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/test_model.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = CNNModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        early_stopping(val_loss, model, model_path)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying out with RNN (LSTM layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\3981922237.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/10], Training Loss: 2.1990\n",
      "Accuracy on training set: 19.34%\n",
      "Validation Loss: 2.2044\n",
      "Accuracy on validation set: 10.84%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 2.1704\n",
      "Accuracy on training set: 25.08%\n",
      "Validation Loss: 2.2134\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [3/10], Training Loss: 2.1369\n",
      "Accuracy on training set: 29.31%\n",
      "Validation Loss: 2.2329\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [4/10], Training Loss: 2.0620\n",
      "Accuracy on training set: 38.97%\n",
      "Validation Loss: 2.2846\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [5/10], Training Loss: 1.8759\n",
      "Accuracy on training set: 47.43%\n",
      "Validation Loss: 2.4452\n",
      "Accuracy on validation set: 12.05%\n",
      "Saved the best model parameters.\n",
      "Epoch [6/10], Training Loss: 1.6108\n",
      "Accuracy on training set: 59.82%\n",
      "Validation Loss: 2.5793\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [7/10], Training Loss: 1.3444\n",
      "Accuracy on training set: 68.58%\n",
      "Validation Loss: 2.7193\n",
      "Accuracy on validation set: 12.05%\n",
      "Epoch [8/10], Training Loss: 1.0589\n",
      "Accuracy on training set: 76.74%\n",
      "Validation Loss: 2.7797\n",
      "Accuracy on validation set: 8.43%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming X_imf and y_one_hot are already defined\n",
    "# Convert the DataFrame to a NumPy array\n",
    "X_imf_array = X_imf.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 84, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(84):\n",
    "        reshaped_data[i, j, :] = X_imf_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Normalize each channel separately across the entire dataset\n",
    "mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "standardized_data = (standardized_data - mean) / std\n",
    "\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the RNN input requirements\n",
    "# Reshape each sample to (batch_size, sequence_length, input_size)\n",
    "standardized_data = standardized_data.view(414, 84, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/test_model.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "input_size = 7680\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 9\n",
    "num_epochs = 10\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # Create data loaders for training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, optimizer, and scheduler\n",
    "    model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        # Evaluate the model on the training set\n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the model if it has the best accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            save_model(model, model_path)\n",
    "            print(\"Saved the best model parameters.\")\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n",
    "\n",
    "# Plotting the metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/10], Training Loss: 2.4521\n",
      "Accuracy on training set: 17.82%\n",
      "Validation Loss: 2.1968\n",
      "Accuracy on validation set: 12.05%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 1.9662\n",
      "Accuracy on training set: 29.00%\n",
      "Validation Loss: 2.1967\n",
      "Accuracy on validation set: 16.87%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/10], Training Loss: 1.8557\n",
      "Accuracy on training set: 40.79%\n",
      "Validation Loss: 2.1968\n",
      "Accuracy on validation set: 15.66%\n",
      "Epoch [4/10], Training Loss: 1.6370\n",
      "Accuracy on training set: 46.83%\n",
      "Validation Loss: 2.1965\n",
      "Accuracy on validation set: 20.48%\n",
      "Saved the best model parameters.\n",
      "Epoch [5/10], Training Loss: 1.6244\n",
      "Accuracy on training set: 56.50%\n",
      "Validation Loss: 2.1976\n",
      "Accuracy on validation set: 16.87%\n",
      "Epoch [6/10], Training Loss: 1.5514\n",
      "Accuracy on training set: 58.31%\n",
      "Validation Loss: 2.1991\n",
      "Accuracy on validation set: 18.07%\n",
      "Epoch [7/10], Training Loss: 1.5184\n",
      "Accuracy on training set: 63.75%\n",
      "Validation Loss: 2.2010\n",
      "Accuracy on validation set: 16.87%\n",
      "Epoch [8/10], Training Loss: 1.4532\n",
      "Accuracy on training set: 68.88%\n",
      "Validation Loss: 2.2035\n",
      "Accuracy on validation set: 14.46%\n",
      "Epoch [9/10], Training Loss: 1.4371\n",
      "Accuracy on training set: 71.90%\n",
      "Validation Loss: 2.2128\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [10/10], Training Loss: 1.3417\n",
      "Accuracy on training set: 75.53%\n",
      "Validation Loss: 2.2273\n",
      "Accuracy on validation set: 12.05%\n",
      "Fold 2/5\n",
      "Epoch [1/10], Training Loss: 2.3526\n",
      "Accuracy on training set: 22.05%\n",
      "Validation Loss: 2.1952\n",
      "Accuracy on validation set: 9.64%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 2.0131\n",
      "Accuracy on training set: 28.10%\n",
      "Validation Loss: 2.1939\n",
      "Accuracy on validation set: 10.84%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/10], Training Loss: 1.9036\n",
      "Accuracy on training set: 34.14%\n",
      "Validation Loss: 2.1919\n",
      "Accuracy on validation set: 9.64%\n",
      "Epoch [4/10], Training Loss: 1.6762\n",
      "Accuracy on training set: 39.27%\n",
      "Validation Loss: 2.1917\n",
      "Accuracy on validation set: 9.64%\n",
      "Epoch [5/10], Training Loss: 1.5929\n",
      "Accuracy on training set: 48.64%\n",
      "Validation Loss: 2.1899\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [6/10], Training Loss: 1.5111\n",
      "Accuracy on training set: 54.68%\n",
      "Validation Loss: 2.1873\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [7/10], Training Loss: 1.5193\n",
      "Accuracy on training set: 59.21%\n",
      "Validation Loss: 2.1859\n",
      "Accuracy on validation set: 10.84%\n",
      "Epoch [8/10], Training Loss: 1.4487\n",
      "Accuracy on training set: 62.84%\n",
      "Validation Loss: 2.1842\n",
      "Accuracy on validation set: 13.25%\n",
      "Saved the best model parameters.\n",
      "Epoch [9/10], Training Loss: 1.4506\n",
      "Accuracy on training set: 69.18%\n",
      "Validation Loss: 2.1837\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [10/10], Training Loss: 1.3670\n",
      "Accuracy on training set: 71.30%\n",
      "Validation Loss: 2.1967\n",
      "Accuracy on validation set: 13.25%\n",
      "Fold 3/5\n",
      "Epoch [1/10], Training Loss: 2.4473\n",
      "Accuracy on training set: 19.64%\n",
      "Validation Loss: 2.1958\n",
      "Accuracy on validation set: 9.64%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 2.0015\n",
      "Accuracy on training set: 27.19%\n",
      "Validation Loss: 2.1963\n",
      "Accuracy on validation set: 12.05%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/10], Training Loss: 1.8550\n",
      "Accuracy on training set: 32.63%\n",
      "Validation Loss: 2.1970\n",
      "Accuracy on validation set: 13.25%\n",
      "Saved the best model parameters.\n",
      "Epoch [4/10], Training Loss: 1.6943\n",
      "Accuracy on training set: 36.56%\n",
      "Validation Loss: 2.1977\n",
      "Accuracy on validation set: 18.07%\n",
      "Saved the best model parameters.\n",
      "Epoch [5/10], Training Loss: 1.6343\n",
      "Accuracy on training set: 47.73%\n",
      "Validation Loss: 2.1995\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [6/10], Training Loss: 1.5399\n",
      "Accuracy on training set: 54.38%\n",
      "Validation Loss: 2.2010\n",
      "Accuracy on validation set: 14.46%\n",
      "Epoch [7/10], Training Loss: 1.4729\n",
      "Accuracy on training set: 64.35%\n",
      "Validation Loss: 2.2044\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [8/10], Training Loss: 1.4378\n",
      "Accuracy on training set: 70.09%\n",
      "Validation Loss: 2.2101\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [9/10], Training Loss: 1.4738\n",
      "Accuracy on training set: 71.30%\n",
      "Validation Loss: 2.2199\n",
      "Accuracy on validation set: 13.25%\n",
      "Epoch [10/10], Training Loss: 1.3202\n",
      "Accuracy on training set: 75.23%\n",
      "Validation Loss: 2.2381\n",
      "Accuracy on validation set: 14.46%\n",
      "Fold 4/5\n",
      "Epoch [1/10], Training Loss: 2.4630\n",
      "Accuracy on training set: 18.43%\n",
      "Validation Loss: 2.1959\n",
      "Accuracy on validation set: 9.64%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 1.9884\n",
      "Accuracy on training set: 29.00%\n",
      "Validation Loss: 2.1959\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [3/10], Training Loss: 1.8619\n",
      "Accuracy on training set: 36.25%\n",
      "Validation Loss: 2.1960\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [4/10], Training Loss: 1.7762\n",
      "Accuracy on training set: 45.02%\n",
      "Validation Loss: 2.1958\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [5/10], Training Loss: 1.5561\n",
      "Accuracy on training set: 51.66%\n",
      "Validation Loss: 2.1953\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [6/10], Training Loss: 1.5407\n",
      "Accuracy on training set: 56.80%\n",
      "Validation Loss: 2.1959\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [7/10], Training Loss: 1.4996\n",
      "Accuracy on training set: 61.03%\n",
      "Validation Loss: 2.1964\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [8/10], Training Loss: 1.4057\n",
      "Accuracy on training set: 66.16%\n",
      "Validation Loss: 2.1984\n",
      "Accuracy on validation set: 9.64%\n",
      "Epoch [9/10], Training Loss: 1.4293\n",
      "Accuracy on training set: 69.79%\n",
      "Validation Loss: 2.2047\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [10/10], Training Loss: 1.4345\n",
      "Accuracy on training set: 73.41%\n",
      "Validation Loss: 2.2175\n",
      "Accuracy on validation set: 8.43%\n",
      "Fold 5/5\n",
      "Epoch [1/10], Training Loss: 2.4854\n",
      "Accuracy on training set: 18.37%\n",
      "Validation Loss: 2.1977\n",
      "Accuracy on validation set: 9.76%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/10], Training Loss: 2.0853\n",
      "Accuracy on training set: 27.71%\n",
      "Validation Loss: 2.1960\n",
      "Accuracy on validation set: 12.20%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/10], Training Loss: 1.8472\n",
      "Accuracy on training set: 34.34%\n",
      "Validation Loss: 2.1934\n",
      "Accuracy on validation set: 10.98%\n",
      "Epoch [4/10], Training Loss: 1.7650\n",
      "Accuracy on training set: 43.98%\n",
      "Validation Loss: 2.1885\n",
      "Accuracy on validation set: 12.20%\n",
      "Epoch [5/10], Training Loss: 1.6804\n",
      "Accuracy on training set: 53.61%\n",
      "Validation Loss: 2.1832\n",
      "Accuracy on validation set: 13.41%\n",
      "Saved the best model parameters.\n",
      "Epoch [6/10], Training Loss: 1.5463\n",
      "Accuracy on training set: 59.34%\n",
      "Validation Loss: 2.1780\n",
      "Accuracy on validation set: 18.29%\n",
      "Saved the best model parameters.\n",
      "Epoch [7/10], Training Loss: 1.4825\n",
      "Accuracy on training set: 64.16%\n",
      "Validation Loss: 2.1728\n",
      "Accuracy on validation set: 18.29%\n",
      "Epoch [8/10], Training Loss: 1.4647\n",
      "Accuracy on training set: 68.07%\n",
      "Validation Loss: 2.1655\n",
      "Accuracy on validation set: 15.85%\n",
      "Epoch [9/10], Training Loss: 1.3514\n",
      "Accuracy on training set: 71.39%\n",
      "Validation Loss: 2.1590\n",
      "Accuracy on validation set: 14.63%\n",
      "Epoch [10/10], Training Loss: 1.3083\n",
      "Accuracy on training set: 73.49%\n",
      "Validation Loss: 2.1520\n",
      "Accuracy on validation set: 15.85%\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\820576789.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 155\u001b[0m\n\u001b[0;32m    152\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m    154\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    157\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining and validation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\Emotion-Recognition-kuwpk_hL\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(-1)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "        weighted_sum = torch.bmm(attention_weights.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        return weighted_sum\n",
    "\n",
    "class ComplexRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ComplexRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        out = self.fc1(attn_out)\n",
    "        out = self.bn(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define the model, optimizer, and criterion\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_classes = 9\n",
    "model = ComplexRNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=5*1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # Create data loaders for training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, optimizer, and scheduler\n",
    "    model = ComplexRNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=5*1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        # Evaluate the model on the training set\n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the model if it has the best accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            save_model(model, model_path)\n",
    "            print(\"Saved the best model parameters.\")\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n",
    "\n",
    "# Plotting the metrics\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\820576789.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/20], Training Loss: 2.2018\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2010\n",
      "Accuracy on validation set: 7.23%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Training Loss: 2.1852\n",
      "Accuracy on training set: 12.39%\n",
      "Validation Loss: 2.2051\n",
      "Accuracy on validation set: 8.43%\n",
      "Saved the best model parameters.\n",
      "Epoch [3/20], Training Loss: 2.1849\n",
      "Accuracy on training set: 13.29%\n",
      "Validation Loss: 2.2083\n",
      "Accuracy on validation set: 7.23%\n",
      "Epoch [4/20], Training Loss: 2.1758\n",
      "Accuracy on training set: 16.31%\n",
      "Validation Loss: 2.2086\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [5/20], Training Loss: 2.1495\n",
      "Accuracy on training set: 23.56%\n",
      "Validation Loss: 2.2224\n",
      "Accuracy on validation set: 8.43%\n",
      "Early stopping triggered.\n",
      "Fold 2/5\n",
      "Epoch [1/20], Training Loss: 2.1996\n",
      "Accuracy on training set: 11.78%\n",
      "Validation Loss: 2.2139\n",
      "Accuracy on validation set: 8.43%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Training Loss: 2.1908\n",
      "Accuracy on training set: 11.78%\n",
      "Validation Loss: 2.2128\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [3/20], Training Loss: 2.1840\n",
      "Accuracy on training set: 12.08%\n",
      "Validation Loss: 2.2123\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [4/20], Training Loss: 2.1750\n",
      "Accuracy on training set: 16.92%\n",
      "Validation Loss: 2.2143\n",
      "Accuracy on validation set: 8.43%\n",
      "Early stopping triggered.\n",
      "Fold 3/5\n",
      "Epoch [1/20], Training Loss: 2.2069\n",
      "Accuracy on training set: 11.78%\n",
      "Validation Loss: 2.1985\n",
      "Accuracy on validation set: 8.43%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Training Loss: 2.1964\n",
      "Accuracy on training set: 12.08%\n",
      "Validation Loss: 2.2032\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [3/20], Training Loss: 2.1843\n",
      "Accuracy on training set: 15.41%\n",
      "Validation Loss: 2.2097\n",
      "Accuracy on validation set: 8.43%\n",
      "Epoch [4/20], Training Loss: 2.1750\n",
      "Accuracy on training set: 28.70%\n",
      "Validation Loss: 2.2294\n",
      "Accuracy on validation set: 7.23%\n",
      "Early stopping triggered.\n",
      "Fold 4/5\n",
      "Epoch [1/20], Training Loss: 2.1997\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2026\n",
      "Accuracy on validation set: 4.82%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Training Loss: 2.1919\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2028\n",
      "Accuracy on validation set: 4.82%\n",
      "Epoch [3/20], Training Loss: 2.1878\n",
      "Accuracy on training set: 12.69%\n",
      "Validation Loss: 2.2044\n",
      "Accuracy on validation set: 4.82%\n",
      "Epoch [4/20], Training Loss: 2.1746\n",
      "Accuracy on training set: 18.13%\n",
      "Validation Loss: 2.2068\n",
      "Accuracy on validation set: 4.82%\n",
      "Early stopping triggered.\n",
      "Fold 5/5\n",
      "Epoch [1/20], Training Loss: 2.2004\n",
      "Accuracy on training set: 10.84%\n",
      "Validation Loss: 2.2021\n",
      "Accuracy on validation set: 12.20%\n",
      "Saved the best model parameters.\n",
      "Epoch [2/20], Training Loss: 2.1983\n",
      "Accuracy on training set: 11.14%\n",
      "Validation Loss: 2.2036\n",
      "Accuracy on validation set: 12.20%\n",
      "Epoch [3/20], Training Loss: 2.1937\n",
      "Accuracy on training set: 13.86%\n",
      "Validation Loss: 2.2049\n",
      "Accuracy on validation set: 12.20%\n",
      "Epoch [4/20], Training Loss: 2.1894\n",
      "Accuracy on training set: 22.59%\n",
      "Validation Loss: 2.2041\n",
      "Accuracy on validation set: 17.07%\n",
      "Saved the best model parameters.\n",
      "Epoch [5/20], Training Loss: 2.1754\n",
      "Accuracy on training set: 28.31%\n",
      "Validation Loss: 2.2018\n",
      "Accuracy on validation set: 13.41%\n",
      "Epoch [6/20], Training Loss: 2.1637\n",
      "Accuracy on training set: 32.83%\n",
      "Validation Loss: 2.2029\n",
      "Accuracy on validation set: 13.41%\n",
      "Epoch [7/20], Training Loss: 2.1357\n",
      "Accuracy on training set: 31.93%\n",
      "Validation Loss: 2.2069\n",
      "Accuracy on validation set: 9.76%\n",
      "Early stopping triggered.\n",
      "Loaded the best model parameters for final evaluation or further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8416\\820576789.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/gAAAGJCAYAAAA+OpjaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAADcLUlEQVR4nOzdd3hT5RfA8W+6W7qgjDLL3hsKAjJEZMoWQVFAFFCWqCjyUxRxsGUqKCpDAREZMhTZKFNW2Zuyl6yW0t3e3x+vN23poCO75/M8eZqkyb0nadLk3Pe85zVomqYhhBBCCCGEEEIIu+Zk7QCEEEIIIYQQQgiRc5LgCyGEEEIIIYQQDkASfCGEEEIIIYQQwgFIgi+EEEIIIYQQQjgASfCFEEIIIYQQQggHIAm+EEIIIYQQQgjhACTBF0IIIYQQQgghHIAk+EIIIYQQQgghhAOQBF8IIYQQQgghhHAAkuALAfTp04eSJUtm676jR4/GYDCYNiAbc+HCBQwGA/PmzbPofrdu3YrBYGDr1q3G6zL7tzJXzCVLlqRPnz4m3WZmzJs3D4PBwIULFyy+byGEsDT5XM6YfC4nsdbnshC2ShJ8YdMMBkOmTsk/aITIqZ07dzJ69Gju379v7VCEEMKmyOeysAb5XBYi81ysHYAQGfnxxx9TXF6wYAEbNmxIdX2lSpVytJ85c+aQmJiYrft++OGHvP/++znav8i8nPytMmvnzp188skn9OnTB39//xS/O3XqFE5OcmxUCJE7yeeyeJR8LgthWyTBFzbtpZdeSnF59+7dbNiwIdX1j4qMjMTLyyvT+3F1dc1WfAAuLi64uMhbyVJy8rcyBXd3d6vuXwghrEk+l8Wj5HPZPjx8+JA8efJYOwxhAXK4S9i9Zs2aUbVqVfbv30+TJk3w8vLif//7HwC//fYb7dq1o0iRIri7u1OmTBk+/fRTEhISUmzj0flj+jyxSZMm8e2331KmTBnc3d0JDg5m7969Ke6b1lw/g8HA4MGDWblyJVWrVsXd3Z0qVaqwbt26VPFv3bqVunXr4uHhQZkyZfjmm28yPX/w77//plu3bpQoUQJ3d3eKFy/OW2+9RVRUVKrH5+3tzdWrV+nUqRPe3t4UKFCA4cOHp3ou7t+/T58+ffDz88Pf35/evXtnqiRu3759GAwG5s+fn+p3f/75JwaDgTVr1gBw8eJFBg4cSIUKFfD09CQgIIBu3bplan55WnP9Mhvz4cOH6dOnD6VLl8bDw4PAwED69u3LnTt3jLcZPXo07777LgClSpUylpvqsaU11+/8+fN069aNfPny4eXlxRNPPMHatWtT3Eaft/jLL7/w+eefU6xYMTw8PHj66ac5e/bsYx93er7++muqVKmCu7s7RYoUYdCgQake+5kzZ+jatSuBgYF4eHhQrFgxevToQVhYmPE2GzZs4Mknn8Tf3x9vb28qVKhgfB8JIURWyOeyfC7nhs/lrDxn9+/f56233qJkyZK4u7tTrFgxevXqxe3bt423iY6OZvTo0ZQvXx4PDw8KFy5Mly5dOHfuXIp4H53+klZvA/31de7cOdq2bYuPjw89e/YEMv8aBTh58iTPP/88BQoUwNPTkwoVKvDBBx8AsGXLFgwGAytWrEh1v0WLFmEwGNi1a9djn0dhenJ4UziEO3fu0KZNG3r06MFLL71EoUKFANWYzNvbm7fffhtvb282b97MRx99RHh4OBMnTnzsdhctWsSDBw8YMGAABoOBCRMm0KVLF86fP//YI9bbt29n+fLlDBw4EB8fH6ZPn07Xrl25dOkSAQEBABw8eJDWrVtTuHBhPvnkExISEhgzZgwFChTI1ONeunQpkZGRvPHGGwQEBPDPP/8wY8YMrly5wtKlS1PcNiEhgVatWlG/fn0mTZrExo0bmTx5MmXKlOGNN94AQNM0OnbsyPbt23n99depVKkSK1asoHfv3o+NpW7dupQuXZpffvkl1e2XLFlC3rx5adWqFQB79+5l586d9OjRg2LFinHhwgVmzZpFs2bNOH78eJZGebIS84YNGzh//jyvvPIKgYGBHDt2jG+//ZZjx46xe/duDAYDXbp04fTp0yxevJgpU6aQP39+gHT/Jjdv3qRhw4ZERkYydOhQAgICmD9/Ph06dODXX3+lc+fOKW4/btw4nJycGD58OGFhYUyYMIGePXuyZ8+eTD9m3ejRo/nkk09o0aIFb7zxBqdOnWLWrFns3buXHTt24OrqSmxsLK1atSImJoYhQ4YQGBjI1atXWbNmDffv38fPz49jx47x7LPPUr16dcaMGYO7uztnz55lx44dWY5JCCFAPpflc9nxP5cz+5xFRETQuHFjTpw4Qd++falduza3b99m1apVXLlyhfz585OQkMCzzz7Lpk2b6NGjB2+++SYPHjxgw4YNHD16lDJlymT6+dfFx8fTqlUrnnzySSZNmmSMJ7Ov0cOHD9O4cWNcXV3p378/JUuW5Ny5c6xevZrPP/+cZs2aUbx4cRYuXJjqOV24cCFlypShQYMGWY5bmIAmhB0ZNGiQ9ujLtmnTphqgzZ49O9XtIyMjU103YMAAzcvLS4uOjjZe17t3by0oKMh4OTQ0VAO0gIAA7e7du8brf/vtNw3QVq9ebbzu448/ThUToLm5uWlnz541Xnfo0CEN0GbMmGG8rn379pqXl5d29epV43VnzpzRXFxcUm0zLWk9vrFjx2oGg0G7ePFiiscHaGPGjElx21q1aml16tQxXl65cqUGaBMmTDBeFx8frzVu3FgDtLlz52YYz8iRIzVXV9cUz1lMTIzm7++v9e3bN8O4d+3apQHaggULjNdt2bJFA7QtW7akeCzJ/1ZZiTmt/S5evFgDtL/++st43cSJEzVACw0NTXX7oKAgrXfv3sbLw4YN0wDt77//Nl734MEDrVSpUlrJkiW1hISEFI+lUqVKWkxMjPG206ZN0wDtyJEjqfaV3Ny5c1PEdOvWLc3NzU1r2bKlcR+apmkzZ87UAO2HH37QNE3TDh48qAHa0qVL0932lClTNED7999/M4xBCCEeJZ/Lj3988rnsmJ/LmX3OPvroIw3Qli9fnur2iYmJmqZp2g8//KAB2pdffpnubdJ67jUt6b2R/HnVX1/vv/9+puJO6zXapEkTzcfHJ8V1yePRNPX6cnd31+7fv2+87tatW5qLi4v28ccfp9qPsAwp0RcOwd3dnVdeeSXV9Z6ensbzDx484Pbt2zRu3JjIyEhOnjz52O12796dvHnzGi83btwYUKVfj9OiRYsUR1yrV6+Or6+v8b4JCQls3LiRTp06UaRIEePtypYtS5s2bR67fUj5+B4+fMjt27dp2LAhmqZx8ODBVLd//fXXU1xu3Lhxisfy+++/4+LiYhw5AHB2dmbIkCGZiqd79+7ExcWxfPly43Xr16/n/v37dO/ePc244+LiuHPnDmXLlsXf358DBw5kal/ZiTn5fqOjo7l9+zZPPPEEQJb3m3z/9erV48knnzRe5+3tTf/+/blw4QLHjx9PcftXXnkFNzc34+WsvKaS27hxI7GxsQwbNixFc6F+/frh6+trLEX08/MDVDlmZGRkmtvSGxb99ttvZm+UJITIHeRzWT6XHf1zObPP2bJly6hRo0aqUW7AOO1j2bJl5M+fP83nKCdLPib/G6QVd3qv0X///Ze//vqLvn37UqJEiXTj6dWrFzExMfz666/G65YsWUJ8fPxj+3II85EEXziEokWLpvjnrDt27BidO3fGz88PX19fChQoYPyHk3z+cXoe/aemf6m4d+9elu+r31+/761bt4iKiqJs2bKpbpfWdWm5dOkSffr0IV++fMb5e02bNgVSPz4PD49U5WzJ4wE1n6xw4cJ4e3unuF2FChUyFU+NGjWoWLEiS5YsMV63ZMkS8ufPT/PmzY3XRUVF8dFHH1G8eHHc3d3Jnz8/BQoU4P79+5n6uySXlZjv3r3Lm2++SaFChfD09KRAgQKUKlUKyNzrIb39p7UvvYP0xYsXU1yfk9fUo/uF1I/Tzc2N0qVLG39fqlQp3n77bb777jvy589Pq1at+Oqrr1I83u7du9OoUSNee+01ChUqRI8ePfjll18k2RdCZJt8LsvnsqN/Lmf2OTt37hxVq1bNcFvnzp2jQoUKJm0O6eLiQrFixVJdn5nXqH5w43FxV6xYkeDgYBYuXGi8buHChTzxxBOZfs8I05M5+MIhJD8aqbt//z5NmzbF19eXMWPGUKZMGTw8PDhw4AAjRozIVPLi7Oyc5vWappn1vpmRkJDAM888w927dxkxYgQVK1YkT548XL16lT59+qR6fOnFY2rdu3fn888/5/bt2/j4+LBq1SpeeOGFFB9aQ4YMYe7cuQwbNowGDRrg5+eHwWCgR48eZk0qn3/+eXbu3Mm7775LzZo18fb2JjExkdatW1ssmTX36yItkydPpk+fPvz222+sX7+eoUOHMnbsWHbv3k2xYsXw9PTkr7/+YsuWLaxdu5Z169axZMkSmjdvzvr16y322hFCOA75XJbP5cyw589lSz9n6Y3kP9qUUefu7p5q+cCsvkYzo1evXrz55ptcuXKFmJgYdu/ezcyZM7O8HWE6kuALh7V161bu3LnD8uXLadKkifH60NBQK0aVpGDBgnh4eKTZqTUz3VuPHDnC6dOnmT9/Pr169TJev2HDhmzHFBQUxKZNm4iIiEhx5P3UqVOZ3kb37t355JNPWLZsGYUKFSI8PJwePXqkuM2vv/5K7969mTx5svG66OjoTHUFzm7M9+7dY9OmTXzyySd89NFHxuvPnDmTaptZKYcLCgpK8/nRS02DgoIyva2s0Ld76tQpSpcubbw+NjaW0NBQWrRokeL21apVo1q1anz44Yfs3LmTRo0aMXv2bD777DMAnJycePrpp3n66af58ssv+eKLL/jggw/YsmVLqm0JIUR2yOdy1snnsmKLn8uZfc7KlCnD0aNHM9xWmTJl2LNnD3Fxcek2i9QrCx7d/qMVCRnJ7GtU/17xuLgBevTowdtvv83ixYuJiorC1dU1xfQPYXlSoi8cln5ENvkR2NjYWL7++mtrhZSCs7MzLVq0YOXKlVy7ds14/dmzZ/njjz8ydX9I+fg0TWPatGnZjqlt27bEx8cza9Ys43UJCQnMmDEj09uoVKkS1apVY8mSJSxZsoTChQun+CKnx/7okfEZM2akexTaFDGn9XwBTJ06NdU29XViM/PFpm3btvzzzz8ploJ5+PAh3377LSVLlqRy5cqZfShZ0qJFC9zc3Jg+fXqKx/T9998TFhZGu3btAAgPDyc+Pj7FfatVq4aTkxMxMTGAKpF8VM2aNQGMtxFCiJySz+Wsk89lxRY/lzP7nHXt2pVDhw6luZycfv+uXbty+/btNEe+9dsEBQXh7OzMX3/9leL3WXn/ZPY1WqBAAZo0acIPP/zApUuX0oxHlz9/ftq0acNPP/3EwoULad26tXGlA2EdMoIvHFbDhg3JmzcvvXv3ZujQoRgMBn788UezlkJn1ejRo1m/fj2NGjXijTfeICEhgZkzZ1K1alVCQkIyvG/FihUpU6YMw4cP5+rVq/j6+rJs2bIsz+VOrn379jRq1Ij333+fCxcuULlyZZYvX57leXDdu3fno48+wsPDg1dffTVVidizzz7Ljz/+iJ+fH5UrV2bXrl1s3LjRuEyROWL29fWlSZMmTJgwgbi4OIoWLcr69evTHDmqU6cOAB988AE9evTA1dWV9u3bG79gJPf++++zePFi2rRpw9ChQ8mXLx/z588nNDSUZcuWpXrsplKgQAFGjhzJJ598QuvWrenQoQOnTp3i66+/Jjg42DindfPmzQwePJhu3bpRvnx54uPj+fHHH3F2dqZr164AjBkzhr/++ot27doRFBTErVu3+PrrrylWrFiKJkVCCJET8rmcdfK5rNji53Jmn7N3332XX3/9lW7dutG3b1/q1KnD3bt3WbVqFbNnz6ZGjRr06tWLBQsW8Pbbb/PPP//QuHFjHj58yMaNGxk4cCAdO3bEz8+Pbt26MWPGDAwGA2XKlGHNmjXcunUr0zFn5TU6ffp0nnzySWrXrk3//v0pVaoUFy5cYO3ataneC7169eK5554D4NNPP836kylMShJ84bACAgJYs2YN77zzDh9++CF58+blpZde4umnnzau+2ptderU4Y8//mD48OGMGjWK4sWLM2bMGE6cOPHYbsKurq6sXr3aOJ/aw8ODzp07M3jwYGrUqJGteJycnFi1ahXDhg3jp59+wmAw0KFDByZPnkytWrUyvZ3u3bvz4YcfEhkZmWaZ1rRp03B2dmbhwoVER0fTqFEjNm7cmK2/S1ZiXrRoEUOGDOGrr75C0zRatmzJH3/8kaJbMkBwcDCffvops2fPZt26dSQmJhIaGprmF4lChQqxc+dORowYwYwZM4iOjqZ69eqsXr3aOIpuLqNHj6ZAgQLMnDmTt956i3z58tG/f3+++OILY4lfjRo1aNWqFatXr+bq1at4eXlRo0YN/vjjD2On4g4dOnDhwgV++OEHbt++Tf78+WnatCmffPKJsQu/EELklHwuZ518Liu2+Lmc2efM29ubv//+m48//pgVK1Ywf/58ChYsyNNPP21sgufs7Mzvv//O559/zqJFi1i2bBkBAQE8+eSTVKtWzbitGTNmEBcXx+zZs3F3d+f5559n4sSJj22Gp8vKa7RGjRrs3r2bUaNGMWvWLKKjowkKCuL5559Ptd327duTN29eEhMT6dChQ1afSmFiBs2WDpsKIQDo1KkTx44dS3MemhBCCCEsSz6XhUhffHw8RYoUoX379nz//ffWDifXkzn4QlhZVFRUistnzpzh999/p1mzZtYJSAghhMjF5HNZiKxZuXIl//77b4rGfcJ6ZARfCCsrXLgwffr0Ma5dPmvWLGJiYjh48CDlypWzdnhCCCFEriKfy0Jkzp49ezh8+DCffvop+fPn58CBA9YOSSBz8IWwutatW7N48WJu3LiBu7s7DRo04IsvvpAvEUIIIYQVyOeyEJkza9YsfvrpJ2rWrMm8efOsHY74j4zgCyGEEEIIIYQQDkDm4AshhBBCCCGEEA5AEnwhhBBCCCGEEMIByBz8NCQmJnLt2jV8fHwwGAzWDkcIIYRA0zQePHhAkSJFcHKS4/M5JZ/1QgghbI0pPuslwU/DtWvXKF68uLXDEEIIIVK5fPkyxYoVs3YYdk8+64UQQtiqnHzWS4KfBh8fH0A9sb6+vlaORgghhIDw8HCKFy9u/IwSOSOf9UIIIWyNKT7rJcFPg16q5+vrKx/6QgghbIqUk5uGfNYLIYSwVTn5rJdJfEIIIYQQQgghhAOQBF8IIYQQQgghhHAAkuALIYQQQgghhBAOQObgCyFEDmiaRnx8PAkJCdYORdg5Z2dnXFxcZI69DZH3t3Bkrq6uODs7WzsMIYSJSYIvhBDZFBsby/Xr14mMjLR2KMJBeHl5UbhwYdzc3KwdSq4n72/h6AwGA8WKFcPb29vaoQghTEgSfCGEyIbExERCQ0NxdnamSJEiuLm5yciryDZN04iNjeXff/8lNDSUcuXK4eQks+isRd7fwtFpmsa///7LlStXKFeunIzkC+FAJMEXQohsiI2NJTExkeLFi+Pl5WXtcIQD8PT0xNXVlYsXLxIbG4uHh4e1Q8q15P0tcoMCBQpw4cIF4uLiJMEXwoHI8IAQQuSAjLIKU5LXk22Rv4dwZFKVIoRjkk8uIYQQQgghhBDCAUiCL4SwrtOn4eZNa0chhBBCCCFymbtRdzlw/QCaplk7FJORBF8IYT3nz0ONGvDMM9aORORQyZIlmTp1aqZvv3XrVgwGA/fv3zdbTADz5s3D39/frPsQwtHZ6vtbCCFyquPPHanzbR0a/tCQvy/+be1wTEISfCGE9axaBdHRcOQIXL9u7WhyBYPBkOFp9OjR2dru3r176d+/f6Zv37BhQ65fv46fn1+29ieESE3e30IIkXlHbx1l+6XtAOy+spsm85rQYXEHjt06ZuXIcka66AshrOePP5LO790LHTpYL5Zc4nqyAylLlizho48+4tSpU8brkq+HrGkaCQkJuLg8/qOiQIECWYrDzc2NwMDALN1HCJExeX9bX2xsLG5ubtYOQwiRCfND5gPQonQLyuQtw3cHvmP16dWsPbOW3jV6M+apMRTzLWblKLNORvCFENYRGQnbtiVd/ucf68ViIpoGDx9a/pSVaWOBgYHGk5+fHwaDwXj55MmT+Pj48Mcff1CnTh3c3d3Zvn07586do2PHjhQqVAhvb2+Cg4PZuHFjiu0+WsJrMBj47rvv6Ny5M15eXpQrV45Vq1YZf/9oCa9eSv/nn39SqVIlvL29ad26dYqEJT4+nqFDh+Lv709AQAAjRoygd+/edOrUKUt/p1mzZlGmTBnc3NyoUKECP/74Y7K/ocbo0aMpUaIE7u7uFClShKFDhxp///XXX1OuXDk8PDwoVKgQzz33XJb2LeyXpmk8jH1olVNm54bmtvf3nTt3eOGFFyhatCheXl5Uq1aNxYsXp7hNYmIiEyZMoGzZsri7u1OiRAk+//xz4++vXLnCCy+8QL58+ciTJw9169Zlz549APTp0yfV/ocNG0azZs2Ml5s1a8bgwYMZNmwY+fPnp1WrVgB8+eWXVKtWjTx58lC8eHEGDhxIREREim3t2LGDZs2a4eXlRd68eWnVqhX37t1jwYIFBAQEEBMTk+L2nTp14uWXX073+RBCZF58Yjw/HfkJgEHBg5j97GyODjxKl0pdSNQSmRsyl3IzyjFiwwjuRd2zcrRZIyP4Qgjr2LIFkn952bvXerGYSGQkJBsgs5iICMiTx3Tbe//995k0aRKlS5cmb968XL58mbZt2/L555/j7u7OggULaN++PadOnaJEiRLpbueTTz5hwoQJTJw4kRkzZtCzZ08uXrxIvnz50rx9ZGQkkyZN4scff8TJyYmXXnqJ4cOHs3DhQgDGjx/PwoULmTt3LpUqVWLatGmsXLmSp556KtOPbcWKFbz55ptMnTqVFi1asGbNGl555RWKFSvGU089xbJly5gyZQo///wzVapU4caNGxw6dAiAffv2MXToUH788UcaNmzI3bt3+ftvx5ivJx4vMi4S77FWeIMDESMjyONmmje5I72/o6OjqVOnDiNGjMDX15e1a9fy8ssvU6ZMGerVqwfAyJEjmTNnDlOmTOHJJ5/k+vXrnDx5EoCIiAiaNm1K0aJFWbVqFYGBgRw4cIDExMQsPafz58/njTfeYMeOHcbrnJycmD59OqVKleL8+fMMHDiQ9957j6+//hqAkJAQnn76afr27cu0adNwcXFhy5YtJCQk0K1bN4YOHcqqVavo1q0bALdu3WLt2rWsX78+S7EJIdK28fxGbkTcIMAzgLbl2gJQMX9Flj2/jF2Xd/HexvfYfmk7E3ZOYM6BOfyv8f8YXG8wHi4eVo48EzSRSlhYmAZoYWFh1g5FCMc1aJCmgaYFB6ufefNqWmKitaPKtKioKO348eNaVFSU8bqICPVQLH2KiMjeY5g7d67m5+dnvLxlyxYN0FauXPnY+1apUkWbMWOG8XJQUJA2ZcoU42VA+/DDD5M9NxEaoP3xxx8p9nXv3j1jLIB29uxZ432++uorrVChQsbLhQoV0iZOnGi8HB8fr5UoUULr2LFjph9jw4YNtX79+qW4Tbdu3bS2bdtqmqZpkydP1sqXL6/Fxsam2tayZcs0X19fLTw8PN395VRaryudfDaZVkbPZ5rv75gIjdFY5RQRk/U3eW54f6elXbt22jvvvKNpmqaFh4dr7u7u2pw5c9K87TfffKP5+Phod+7cSfP3vXv3TrX/N998U2vatKnxctOmTbVatWo9Nq6lS5dqAQEBxssvvPCC1qhRo3Rv/8Ybb2ht2rQxXp48ebJWunRpLdGEn5MZ/b8RwtH1+LWHxmi0Ib8PSfP3iYmJ2upTq7UqX1Ux/i8u/mVxbd7BeVp8QrzZ4jLFZ72M4AshLE/Tkubfv/cevPQS3LsH585B2bLWjS0HvLzUaLo19mtKdevWTXE5IiKC0aNHs3btWq5fv058fDxRUVFcunQpw+1Ur17deD5Pnjz4+vpy69atdG/v5eVFmTJljJcLFy5svH1YWBg3b940jsoBODs7U6dOnSyNtp04cSJVs7BGjRoxbdo0ALp168bUqVMpXbo0rVu3pm3btrRv3x4XFxeeeeYZgoKCjL9r3bq1sURZOD4vVy8iRlrhDf7fvk3Fkd7fCQkJfPHFF/zyyy9cvXqV2NhYYmJijO/JEydOEBMTw9NPP53m/UNCQqhVq1a6VQeZVadOnVTXbdy4kbFjx3Ly5EnCw8OJj48nOjqayMhIvLy8CAkJMY7Op6Vfv34EBwdz9epVihYtyrx58+jTpw8GgyFHsQohICw6jJUnVwLQq0avNG9jMBh4tvyztCnbhgWHFvDR1o+4HH6ZPr/1YfKuyYxrMY42ZdvY5HtS5uALISzv7Fm1RJ6rK7RqBTVrquvtfB6+waBK5S19MvVnS55H6v2HDx/OihUr+OKLL/j7778JCQmhWrVqxMbGZrgdV1fXR54fQ4Zf1tO6vWbhdWmLFy/OqVOn+Prrr/H09GTgwIE0adKEuLg4fHx8OHDgAIsXL6Zw4cJ89NFH1KhRQ5YCyyUMBgN53PJY5WTKL5CO9P6eOHEi06ZNY8SIEWzZsoWQkBBatWpljN3T0zPD+z/u905OTqlijIuLS3W7R5/TCxcu8Oyzz1K9enWWLVvG/v37+eqrrwAyHVutWrWoUaMGCxYsYP/+/Rw7dow+ffpkeB8hROYsPb6U6PhoKheoTJ3CqQ/QJefs5MwrtV7h9ODTjG8xHn8Pf47cOkK7Re14av5T7Lmyx0JRZ54k+EIIy9NH7xs3Bh8f0EdtHGAeviPasWMHffr0oXPnzlSrVo3AwEAuXLhg0Rj8/PwoVKgQe5O9RhISEjhw4ECWtlOpUqUU82RBPb7KlSsbL3t6etK+fXumT5/O1q1b2bVrF0eOHAHAxcWFFi1aMGHCBA4fPsyFCxfYvHlzDh6ZENZlz+/vHTt20LFjR1566SVq1KhB6dKlOX36tPH35cqVw9PTk02bNqV5/+rVqxMSEsLdu3fT/H2BAgVSNAIENer/OPv37ycxMZHJkyfzxBNPUL58ea5du5Zq3+nFpXvttdeYN28ec+fOpUWLFhQvXvyx+xZCPN78Q6p7fu8avTN9ANXT1ZP3Gr3HuaHneLfhu7g7u7Pt4jae+P4Jui3txuk7px+/EQuRBF8IYXl6gt+mjfoZHKx+2vkIvqMqV64cy5cvJyQkhEOHDvHiiy9muQmVKQwZMoSxY8fy22+/cerUKd58803u3buXpdHNd999l3nz5jFr1izOnDnDl19+yfLlyxk+fDigun1///33HD16lPPnz/PTTz/h6elJUFAQa9asYfr06YSEhHDx4kUWLFhAYmIiFSpUMNdDFsLs7Pn9Xa5cOTZs2MDOnTs5ceIEAwYM4ObNm8bfe3h4MGLECN577z0WLFjAuXPn2L17N99//z0AL7zwAoGBgXTq1IkdO3Zw/vx5li1bxq5duwBo3rw5+/btY8GCBZw5c4aPP/6Yo0ePPvaxlC1blri4OGbMmMH58+f58ccfmT17dorbjBw5kr179zJw4EAOHz7MyZMnmTVrFrdv3zbe5sUXX+TKlSvMmTOHvn37Zun5FEKk7dzdc2y/tB0ngxM9q/XM8v3zeeZjwjMTOD3kNH1q9sGAgV+P/0qVr6tw5s4ZM0ScdZLgCyEsKyoKtm5V5/UEXx/BP3gQ0ih/FNb15ZdfkjdvXho2bEj79u1p1aoVtWvXtngcI0aM4IUXXqBXr140aNAAb29vWrVqhYdH5jvadurUiWnTpjFp0iSqVKnCN998w9y5c43LXvn7+zNnzhwaNWpE9erV2bhxI6tXryYgIAB/f3+WL19O8+bNqVSpErNnz2bx4sVUqVLFTI9YCPOz5/f3hx9+SO3atWnVqhXNmjUzJuvJjRo1infeeYePPvqISpUq0b17d+Pcfzc3N9avX0/BggVp27Yt1apVY9y4cTg7OwPQqlUrRo0axXvvvUdwcDAPHjygV6+05+smV6NGDb788kvGjx9P1apVWbhwIWPHjk1xm/Lly7N+/XoOHTpEvXr1aNCgAb/99hsuLkntsfz8/OjatSve3t5ZXg5UCJG2Hw+rpXFblG5BUd+i2d5OCb8SzO04l0OvH6JduXY8XeppygWUM1WYOWLQLD3B0Q6Eh4fj5+dHWFgYvr6+1g5HCMfyxx/Qti0UKwaXLqkJ5ImJkDcvhIerJF+fk2/DoqOjCQ0NpVSpUllKMIXpJCYmUqlSJZ5//nk+/fRTa4djEhm9ruSzybQyej7l/W19jvj+zo6nn36aKlWqMH36dJNvW17nIrdJ1BIpO70sofdDWdhlIS9We9Fk246Ki8LTNePeGplhis966aIvhLCs5OX5eumlk5Mq09+0Sc3Dt4MEX1jexYsXWb9+PU2bNiUmJoaZM2cSGhrKiy+a7gNaCGEd8v5O6d69e2zdupWtW7fy9ddfWzscIRzC9kvbCb0fio+bD50qdjLptk2R3JuKlOgLISxr3Tr1Uy/P18k8fPEYTk5OzJs3j+DgYBo1asSRI0fYuHEjlSpVsnZoQogckvd3SrVq1aJPnz6MHz9e+nwIYSLzQ1RzvW6Vu5l0+VFbIyP4QgjLOXcOzpwBFxd4dF1i6aQvHqN48eKpOuALIRyDvL9TsvRKBkI4usi4SJYeXwpA75q9rRyNeckIvhDCcvTy/CefhEfnFekj+EePwsOHlo1LCCGEEEI4rJUnV/Ig9gGl/EvxZIknrR2OWUmCL4SwnEeXx0uuaFEoXBgSElSjPSGEEEIIIUxg/iFVnv9y9ZdxMjh2CuzYj04IYTuio2HLFnW+devUvzcYkkbxpUxfCCGEEEKYwNXwq2w8vxGAXjUev9SlvZMEXwhhGdu2QVSUGqmvVi3t2+jz8KXRnhBCCCGEMIGFRxaSqCXyZIknKZOvjLXDMTtJ8IUQlqF3z2/dOml5vEfJCL4QQgghhDARTdOM5fm9azh2cz2dJPjCcm7fhsOHrR2FsJaM5t/r6tZVP8+dgzt3zB+TEEIIIYRwWPuv7+f4v8fxcPGgW+Vu1g7HIiTBF5ahadCyJdSoAatWWTsaYWmhoXDqlFoer0WL9G+XLx+ULavO79tnmdhEtjRr1oxhw4YZL5csWZKpU6dmeB+DwcDKlStzvG9TbScjo0ePpmbNmmbdhxC2ytHf30KI3GPBoQUAdKrYCT8PPytHYxmS4AvL+PvvpM7oAwbA3bvWjUdYlj5637Ah+D3mn6vMwzer9u3b0zqtJofA33//jcFg4HA2Km327t1L//79cxpeCukl2devX6dNRpUgQuRS8v4WQogksQmxLDqyCMg95fkgCb6wlNmzk87fuAFvvWW9WITlZaY8X6cn+DIP3yxeffVVNmzYwJUrV1L9bu7cudStW5fq1atnebsFChTAy8vLFCE+VmBgIO7u7hbZlxD2RN7f9is2NtbaIQjhcH4/8zt3ou4Q6B1Ii9IZVJA6GEnwhfndugW//qrOf/WVarC2YAGsWWPduIRlREfD5s3qfDojSynojfb++UdN7bAnmgYPH1r+lIXn6dlnn6VAgQLMmzcvxfUREREsXbqUV199lTt37vDCCy9QtGhRvLy8qFatGosXL85wu4+W8J45c4YmTZrg4eFB5cqV2bBhQ6r7jBgxgvLly+Pl5UXp0qUZNWoUcXFxAMybN49PPvmEQ4cOYTAYMBgMxpgfLeE9cuQIzZs3x9PTk4CAAPr3709ERITx93369KFTp05MmjSJwoULExAQwKBBg4z7yozExETGjBlDsWLFcHd3p2bNmqzTG0eivpwPHjyYwoUL4+HhQVBQEGPHjgVUg5/Ro0dTokQJ3N3dKVKkCEOHDs30voVtsNbbOytvcXl/Z+79fe7cOTp27EihQoXw9vYmODiYjRs3prhNTEwMI0aMoHjx4ri7u1O2bFm+//574++PHTvGs88+i6+vLz4+PjRu3Jhz584Bqac4AHTq1Ik+ffqkeE4//fRTevXqha+vr7FCIqPnTbd69WqCg4Px8PAgf/78dO7cGYAxY8ZQtWrVVI+3Zs2ajBo1Kt3nQwhHpTfXe6naS7g4uVg5GsuxaoI/duxYgoOD8fHxoWDBgnTq1IlTp05leJ85c+bQuHFj8ubNS968eWnRogX/JCvljYuLY8SIEVSrVo08efJQpEgRevXqxbVr18z9cER65s6FuDg1MjtwILz9trp+wAC4d8+6sQnz+/tviIyEwoVVD4bHqVULnJ3h5k1IYxTKpkVGgre35U+RkZkO0cXFhV69ejFv3jy0ZFnD0qVLSUhI4IUXXiA6Opo6deqwdu1ajh49Sv/+/Xn55ZdT/K/NSGJiIl26dMHNzY09e/Ywe/ZsRowYkep2Pj4+zJs3j+PHjzNt2jTmzJnDlClTAOjevTvvvPMOVapU4fr161y/fp3u3bun2sbDhw9p1aoVefPmZe/evSxdupSNGzcyePDgFLfbsmUL586dY8uWLcyfP5958+alSoIyMm3aNCZPnsykSZM4fPgwrVq1okOHDpw5cwaA6dOns2rVKn755RdOnTrFwoULKVmyJADLli1jypQpfPPNN5w5c4aVK1dSLb2lIgUAs2bNonr16vj6+uLr60uDBg34Q68EAqKjoxk0aBABAQF4e3vTtWtXbt68adaYrPX2zspbXN7fmXt/R0RE0LZtWzZt2sTBgwdp3bo17du359KlS8bb9OrVi8WLFzN9+nROnDjBN998g7e3NwBXr16lSZMmuLu7s3nzZvbv30/fvn2Jj4/P1HOomzRpEjVq1ODgwYPGBDyj5w1g7dq1dO7cmbZt23Lw4EE2bdpEvf8q3/r27cuJEyfYm6wC7uDBgxw+fJhXXnklS7EJYe9uR95m7em1APSumXvK8wHQrKhVq1ba3LlztaNHj2ohISFa27ZttRIlSmgRERHp3ufFF1/UvvrqK+3gwYPaiRMntD59+mh+fn7alStXNE3TtPv372stWrTQlixZop08eVLbtWuXVq9ePa1OnTqZjissLEwDtLCwsBw/xlwvIUHTSpXSNNC0H35Q10VGalr58uq6Pn2sG58wv7ffVn/rV17J/H1q1lT3+fVX88WVQ1FRUdrx48e1qKiopCsjIlTclj5l8D8zLSdOnNAAbcuWLcbrGjdurL300kvp3qddu3baO++8Y7zctGlT7c033zReDgoK0qZMmaJpmqb9+eefmouLi3b16lXj7//44w8N0FasWJHuPiZOnJjif/XHH3+s1ahRI9Xtkm/n22+/1fLmzZvic2Pt2rWak5OTduPGDU3TNK13795aUFCQFh8fb7xNt27dtO7du6cby6P7LlKkiPb555+nuE1wcLA2cOBATdM0bciQIVrz5s21xMTEVNuaPHmyVr58eS02Njbd/enSfF39Jzd9Nq1atUpbu3atdvr0ae3UqVPa//73P83V1VU7evSopmma9vrrr2vFixfXNm3apO3bt0974okntIYNG2ZpHxk9n2n9Haz19s7qW1ze349/f6elSpUq2owZMzRN07RTp05pgLZhw4Y0bzty5EitVKlS6b6nH33+NE3TOnbsqPXu3dt4OSgoSOvUqdNj43r0eWvQoIHWs2fPdG/fpk0b7Y033jBeHjJkiNasWbM0b5vR/xsh7N2MPTM0RqPVml3L2qFkiSk+661aq5C8vBFUyVbBggXZv38/TZo0SfM+CxcuTHH5u+++Y9myZWzatIlevXrh5+eXqlRs5syZ1KtXj0uXLlGiRAnTPgiRsQ0bVAd1f3/Qj857esIPP0DjxjBvHjz/fObmZgv7lJX597p69SAkRM3D79rVLGGZhZcXJCsdteh+s6BixYo0bNiQH374gWbNmnH27Fn+/vtvxowZA0BCQgJffPEFv/zyC1evXiU2NpaYmJhMz8E9ceIExYsXp0iRIsbrGjRokOp2S5YsYfr06Zw7d46IiAji4+Px9fXN0mM5ceIENWrUIE+ePMbrGjVqRGJiIqdOnaJQoUIAVKlSBWdnZ+NtChcuzJEjRzK1j/DwcK5du0ajRo1SXN+oUSMOHToEqDLhZ555hgoVKtC6dWueffZZWrZsCUC3bt2YOnUqpUuXpnXr1rRt25b27dvj4pJ7ygWzqn379ikuf/7558yaNYvdu3dTrFgxvv/+exYtWkTz5s0BNb+8UqVK7N69myeeeMIsMVnr7a3vO7Pk/f3493dERASjR49m7dq1XL9+nfj4eKKioowj+CEhITg7O9O0adM07x8SEkLjxo1xdXXN0uN5VF19adhkHve8hYSE0K9fv3S32a9fP/r27cuXX36Jk5MTixYtSlEBIERuoZfn56bmejqbmoMfFhYGQL58+TJ9n8jISOLi4jK8T1hYGAaDAX9//zR/HxMTQ3h4eIqTMJFZs9TP3r1TfkNp1Aj0+Wn9+sF/f3vhYC5ehBMnVMn9M89k/n7J5+HbE4MB8uSx/MlgyHKor776KsuWLePBgwfMnTuXMmXKGL/MTpw4kWnTpjFixAi2bNlCSEgIrVq1MmkTqF27dtGzZ0/atm3LmjVrOHjwIB988IHZGk09+kXcYDCQmJhosu3Xrl2b0NBQPv30U6Kionj++ed57rnnAChevDinTp3i66+/xtPTk4EDB9KkSZMs9QDIzRISEvj55595+PAhDRo0YP/+/cTFxdEi2ZKbFStWpESJEuzatSvd7eT0s95ab+/svMXl/Z3x+3v48OGsWLGCL774gr///puQkBCqVatmjM/T0zPD/T3u905OTimmSABpvt+TH7iAzD1vj9t3+/btcXd3Z8WKFaxevZq4uDjj/yIhcovj/x5n37V9uDi58GK1F60djsXZTIKfmJjIsGHDaNSoUZoNQtIzYsQIihQpkuKDPrno6GhGjBjBCy+8kO6R47Fjx+Ln52c8FS9ePFuPQTziyhVYvVqdHzAg9e8/+0yteX71KrzzjmVjE5ahj943aKCqODJL76S/bx+YMAkTSZ5//nnj6M6CBQvo27cvhv+yiB07dtCxY0deeuklatSoQenSpTl9+nSmt12pUiUuX77M9evXjdft3r07xW127txJUFAQH3zwAXXr1qVcuXJcvHgxxW3c3NxISEh47L4OHTrEw4cPjdft2LEDJycnKlSokOmYM+Lr60uRIkXYsWNHiut37NhB5cqVU9yue/fuzJkzhyVLlrBs2TLu/rckqKenJ+3bt2f69Ols3bqVXbt2ZbqCILc6cuQI3t7euLu78/rrr7NixQoqV67MjRs3cHNzS3XQvlChQty4cSPd7eWmz3p5f2dsx44d9OnTh86dO1OtWjUCAwO5cOGC8ffVqlUjMTGRbdu2pXn/6tWr8/fff6d7kK5AgQIpnp+EhASOHj362Lgy87xVr16dTZs2pbsNFxcXevfuzdy5c5k7dy49evR47EEBIRzNgkMLAGhbri0F8hSwcjSWZzMJ/qBBgzh69Cg///xzpu8zbtw4fv75Z1asWIGHh0eq38fFxfH888+jaRqz9JHkNIwcOZKwsDDj6fLly9l6DOIR332nkrNmzaBSpdS/9/JSpfoGA3z/Pfz5p8VDFGamJ/iZ6Z6fXOXKairHgwfwmMabInu8vb3p3r07I0eO5Pr16ym6O5crV44NGzawc+dOTpw4wYABA7LUwKxFixaUL1+e3r17c+jQIf7++28++OCDFLcpV64cly5d4ueff+bcuXNMnz6dFStWpLhNyZIlCQ0NJSQkhNu3bxMTE5NqXz179sTDw4PevXtz9OhRtmzZwpAhQ3j55ZeN5bum8O677zJ+/HiWLFnCqVOneP/99wkJCeHNN98E4Msvv2Tx4sWcPHmS06dPs3TpUgIDA/H392fevHl8//33HD16lPPnz/PTTz/h6elJUFCQyeJzRBUqVCAkJIQ9e/bwxhtv0Lt3b44fP57t7eWmz3p5f2esXLlyLF++nJCQEA4dOsSLL76YYsS/ZMmS9O7dm759+7Jy5UpCQ0PZunUrv/zyCwCDBw8mPDycHj16sG/fPs6cOcOPP/5obBTdvHlz1q5dy9q1azl58iRvvPEG9+/fz1Rcj3vePv74YxYvXszHH3/MiRMnOHLkCOPHj09xm9dee43Nmzezbt06+vbtm+3nSQh7lJCYwI+HfwSgV/VeVo7GOmwiwR88eDBr1qxhy5YtFCtWLFP3mTRpEuPGjWP9+vVprumqJ/cXL15kw4YNGc77cnd3N3bq1U8ih+LiYM4cdf7119O/XePGMGSIOv/aa1Kq70hiYkAfZchqjwUXF6hTR51P1g1YmNarr77KvXv3aNWqVYr5tB9++CG1a9emVatWNGvWjMDAQDp16pTp7To5ObFixQqioqKoV68er732Gp9//nmK23To0IG33nqLwYMHU7NmTXbu3JlqGaeuXbvSunVrnnrqKQoUKJDmUl5eXl78+eef3L17l+DgYJ577jmefvppZs6cmbUn4zGGDh3K22+/zTvvvEO1atVYt24dq1atoly5coDqfD1hwgTq1q1LcHAwFy5c4Pfff8fJyQl/f3/mzJlDo0aNqF69Ohs3bmT16tUEBASYNEZH4+bmRtmyZalTpw5jx46lRo0aTJs2jcDAQGJjY1MlTDdv3iQwMDDd7eW2z3p5f6fvyy+/JG/evDRs2JD27dvTqlUrateuneI2s2bN4rnnnmPgwIFUrFiRfv36GSsJAgIC2Lx5MxERETRt2pQ6deowZ84c41SBvn370rt3b3r16kXTpk0pXbo0Tz311GPjyszz1qxZM5YuXcqqVauoWbMmzZs3T7UCQrly5WjYsCEVK1akfv36OXmqhLA7m0I3ce3BNfJ65OXZ8s9aOxzrMFnLv2xITEzUBg0apBUpUkQ7ffp0pu83fvx4zdfXV9u1a1eav4+NjdU6deqkValSRbt161aW48pNnYrNZvly1fq3YEFNi4nJ+LYREZpWurS6fb9+lolPmN+mTepvWqiQWk0hq956S91/0CDTx2YC0n1YmIN00U/fU089pfXu3Vu7f/++5urqqv2abJWNkydPakC63wvSktUu+kLYi8TERK1MmTLa5MmTM7ydvM6FI+q5rKfGaLSBawZaO5Rssfsu+oMGDWLRokX89ttv+Pj4GOfO+fn5GecL9erVi6JFizJ27FgAxo8fz0cffcSiRYsoWbKk8T7e3t54e3sbm4kcOHCANWvWkJCQYLxNvnz5cHNzs8IjzYVmz1Y/X30VHvec58mjSvWbNVOj/s8/D+n0VBB2JHl5vlM2ioX0efgygi9ErjNy5EjatGlDiRIlePDgAYsWLWLr1q38+eef+Pn58eqrr/L222+TL18+fH19GTJkCA0aNDBbB30h7MW///7Lzz//zI0bN3jllVesHY4QFhUeE87yE8sB6FUjd5bnA1g1wdfnxTdr1izF9XPnzjXOF7t06RJOyZKDWbNmERsbm6oj6Mcff8zo0aO5evUqq1atAqBmzZopbrNly5ZU+xJmcPYsrF+v5tZnsJRLCk2bwuDBMHOmOihw9Cj4+Jg3TmFe2VkeLzm9k35ICMTGPv5AkRDCYdy6dYtevXpx/fp1/Pz8qF69On/++SfP/Lcax5QpU3BycqJr167ExMTQqlUrvv76aytHLYT1FSxYkPz58/Ptt9+SN29ea4cjhEX9evxXouKjqBBQgXpF61k7HKuxaoKvPbKESFq2bt2a4nLyLqdpKVmyZKa2K8zo22/Vz9atoVSpzN9v7FhYuxZCQ+G995KW2BP25/JlOHZMjdxnZXm85EqXhnz54O5dOHwY0lgvWAjhmL7//vsMf+/h4cFXX33FV199ZaGIhLAP8h1Y5GZ69/zeNXobVw7JjWyiyZ5wIDExqtwe4I03snZfb2/VTR9UiX8Gy8AIG6eP3tevr5L07DAYkkbxH2kgJIQQQgghhC70XijbLm7DgIGXqr9k7XCsShJ8YVq//gp37kDx4tC2bdbv/9RTSQcGXn1VLZMm7E9Oy/N1djAPX0ZLhCnJ68m2yN9DODJ5fQtHoi+N17xUc4r7FbdyNNYlCb4wLb25Xr9+4OycvW2MHw9BQXDxIrz/vuliE5YRG5v95fEeZcMj+PpySJGRkVaORDgS/fWkv76Edcj7W+QGsbGxADhn9/uaEDZC07QU5fm5nVXn4AsHc+QIbN+uEvvXXsv+dnx8VKl+ixbw9dfw3HNqZF/Yh507VeVFwYLwyLrCWaYn+CdOqG3aUONFZ2dn/P39uXXrFqDWa87N871EzmiaRmRkJLdu3cLf31++cFuZvL+Fo0tMTOTff//Fy8sLFxdJB4R923l5J+funSOPax66VOpi7XCsTt7RwnS++Ub97NQJChfO2baefhoGDFDbfPVVdfAgT54chygsQC/Pb9Uqe8vjJRcYqKZ7XL4M+/erpRRtSGBgIIAxCRAip/z9/Y2vK2Fd8v4Wjs7JyYkSJUpY7OCVpmlyoEyYxfxD8wF4rvJz5HGTfEESfGEaERGwQJXG8PrrptnmhAkqWQwNhZEjYfp002xXmJep5t/r6tVTCf7evTaX4BsMBgoXLkzBggWJi4uzdjjCzrm6usrIvQ2R97dwdG5ubimWojanl1e8zI5LOzj0+iF83G2nGk/Yv6i4KJYcWwJIeb5OEnxhGosXqxLqcuWgeXPTbNPXF777Dlq2hBkzoGtXaNrUNNsW5nHliqq2MBjU380UgoNh2TKbnIevc3Z2lsRMCAcl728hciZRS2TpsaXEJMRw8MZBmgQ1sXZIwoH8duo3wmPCKeFXgqYlJU8AabInTEHTktasHzAg52XZyT3zjGrYB6pU/+FD021bmN66depnvXoQEGCabdpBJ30hhBBCpO1mxE1iEmIAuBp+1crRCEejN9d7ufrLOBkktQVJ8IUp7NsHBw+Cuzv06WP67U+apOZhnzsHH3xg+u1bQ3Q0xMRYOwrTM3V5PkCdOqoi4OJFkLmwQgghhF25cP+C8fy1B9esF4hwONcfXOfPc38C0KtGLytHYzskwRc5p4/eP/+86UZtk/P1hTlz1Pnp0+Hvv02/D0uIi4M1a6BHD8iXDypWdKyKhLg42LhRnTdlgu/rq54rkFF8IYQQws4kT/CvPpARfGE6i48uJlFLpEGxBpQPKG/tcGyGJPgiZ+7dg59/VudN1VwvLa1aQd++ajpAs2ZQtaq6PGuWqiD4by1Xm6Npatm4QYPUygLt28OSJRAVBRcuwG+/WTtC09m1C8LDIX9+qFvXtNvWl8uz4Xn4QgghhEhNEnxhLqtOrQKgR9UeVo7EtkiTPZEzP/6oktXq1aFBA/Pua/JktR76rl1w7Jg6zZ2rfufmBjVrqkQwOFjN265QwbT9ALLi5ElYuFCdQkOTri9UCF54Ae7fh3nzYNEiePFF68RoaqZcHu9R9eqpVRpkBF8IIYSwK6H3k74HSYm+MJWw6DC2X9oOwLPln7VyNLZFEnyRfZoGs2er86+/ruZJm5O/vxoNv35dJXp796oR3b17VSXBP/+kHOH18VHzt+vVS0r8S5QwX5zXr6tqhp9+ggMHkq739oYuXaBnT7XCgIuLOgAwbx78+Sfcvq1Gve2dOebf65KP4Gua+V9rQgghhDCJFCP40mRPmMj6c+tJ0BKoEFCB0nlLWzscmyIJvsi+v/5SI+p58qjk1VIKF4YOHdQJVMJ3/nxSsr93L+zfr5bt27pVnXQFCqhksWRJKFhQjajrJ/2yt3fmE8jwcFixQiX1mzdDYqK63sUFWrdWz0uHDuDllfJ+FStC7drqQMDSpfDGGzl8Uqzs2jU4dMi0y+MlV6MGuLrCnTtqakOpUqbfhxBCCCFM7tEme5qmYZAD9SKH1p5ZC0C7cu2sHIntkQRfZJ8+ev/SS6oRmrUYDFCmjDq98IK6Lj4ejh9POcp/5Aj8+y/8/nvG2/P0TD/518+Hh8PixWoOfXR00n0bNFDPx/PPP35U/sUXVYK/cKH9J/j68nh166qDKKbm7q6S/H371N9TEnwhhBDC5iVqiVwMu2i8HJMQw52oO+T3coDKRWE1iVoif5xVlaPtykuC/yhJ8EX23LwJy5ap8wMGWDeWtLi4qL4A1avDq6+q66KiICRELel3/bp6DLduqZ/6+YcP1e0uXlSnzKhQQSX1L74IpbNQItSjB7z7LuzYoUalS5bM4oO0IXqCb47yfF29eirB37sXunc3336EEEIIYRI3Im4QmxCLs8EZX3df7kXf49qDa5LgixzZf20/tx7ewsfNhydLPGntcGyOJPgie+bOVcui1a8PtWpZO5rM8fRUI+wZNQN8+DDtxF8/r1+Oj4dnn1WJfe3a2ZsTXrSoWhFgyxZVDTByZLYfmlXFx8OGDeq8ORN86aQvhBBC2JXQe6rBXnG/4vi5+3Ev+h5Xw69SvVB1K0cm7Jlenv9MmWdwc3azcjS2RxJ8kXWJifDNN+q8vZeWPypPHjUKn5WR+Jzo2VMl+IsW2W+Cv3u3WhUgICApCTeHevXUz/371UEFF/n3JYQQQtgyff59Sf+SeLl6cejmIVkqT+SYzL/PmJXWEBN27c8/VUm5v7+aay6yr2tXtcTf0aOqR4A90rvnt2wJzs7m20+FCqoBYmSkau4ohBBCCJuWPMEv4l0EkE76ImduRNxg37V9ALQt19bK0dgmSfBF1unN9fr0UWXvIvv8/aHdf0cfFy60aijZpif4rVubdz/OzqqJH6h5+EIIIYSwacYE368kRX2LAqqTvhDZte6s6vtUp3AdAr0DrRyNbZIEX2TN5cuwZo06//rr1o3FUbz4ovq5eHHSMnv24sYN1bQQoFUr8+9P5uELIYQQduNC2AUASuUtRVEfleBLib7ICSnPfzxJ8EXWzJmjktCnnlIl0yLn2rUDHx+4dEl11Lcnf/6pftapo5YQNDd9Hr6M4AshhBA2T2+yV9I/aQRfEnyRXXEJcaw/tx6Q8vyMSIIvMi8uDr77Tp2X0XvT8fRUc/FBNduzJ3p5vjm75yenJ/iHD0N0tGX2KYQQQogsS0hM4FLYJeC/Ofg+ag6+lOiL7Np+aTvhMeEU8CpAcFEzNna2c5Lgi8xbtUqtH1+oEHTqZO1oHItepv/LLxAba91YMmvdOvj1V3W+nYXKpIoXh4IFVRf9kBDL7FMIIYQQWXY94jpxiXG4OLlQxKeIsUT/1sNbxCbYyXcdYVP08vw25drgZJA0Nj3yzIjM05vrvfqq6vwuTKd5c3Xg5O5dWL/e2tE83pEjagWFhATVbLF+fcvs12BIGsWXefhCCCGEzdIb7BX3LY6Lkwv5vfIb1yy//uC6FSMT9ur3M78DMv/+cSTBF6klJEBYmGqod/y4Wud86VLYuFElWP36WTtCx+PsDD16qPO23k3/+nU1Yv/gATRrBt98o14XliKN9oQQQgibl3yJPACDwWAs05d5+CKrQu+FcuL2CZwNzrQs09La4dg0F2sHICwgLg6WL4fQUAgPV6cHD9I+Hx4ODx+mv622baFkSYuFnqv07AnTpsFvv0FEhFrz3dZERkKHDurgT/nysGyZ5as5pNGeEEIIYfP0BL+UfynjdUV8inDh/gWZhy+yTC/Pb1SiEf4e/tYNxsZJgu/oNm2CN9+EY8eyfl9XV/D1TToVKACff276GIVSty6ULQtnz6okv2dPa0eUUmIivPwy7NsHAQHw+++QL5/l46hbV/08fRru3wd/f8vHIIQQQogMJe+grzMulRcuI/gia2R5vMyTBN9RhYbC8OFq5B5UQta+Pfj5qSXZkifu6V12d7fuY8htDAaV1H/yiSrTt7UE//331evJzQ1WroQyZawTR/78ULo0nD+vDja0aGGdOIQQQgiRrgthF4B0Enwp0RdZEBkXyZbQLYAk+JkhCb6jefgQxo2DiRMhJkbN7R44UCWNefNaOzrxOC+8oP5W69fDv/+qqglbMGeOek0BzJ0LTz5p3XiCg1WC/88/kuALIYQQNujROfiALJUnsmVz6GZiEmII8guicoHK1g7H5kmTPUehafDzz1CxInz2mUrumzdXS4lNny7Jvb2oUAHq1FGNDn/5JcObxserP7vZbdgAb7yhzn/ySdKSftYk8/CFvdixQ/U2EUKIXCQhMYFLYZeAR0bwfWUEX2Td2tOqPL9tubYYLNnY2U5Jgu8IQkKgaVM1+nvlimqCt2yZ6npftaq1oxNZpZfmL1qU7k3OnVNTz1991cyxHDsGzz2nDji8/DKMGmXmHWaSo3XSj4tTB+g6dIBvv7XQkRthVnFx8OGH0KQJDB5s7WiEEMKirj24RnxiPK5OrsZRe5A5+CLrNE2T+fdZJAm+Pbt9G15/XY34/v03eHrCmDFqabsuXSy7dJkwne7d1d9u507VSyENf/6pZmPMnatuZhY3b6rl8MLDoXFjVaZvK6+p2rXByQmuXYOrdvwl4d9/VePKkiXVAbrVq2HAAFUxERdn7ehEdp09q6axfP65ak7p4qJKboQQIpcIva++v5TwK4Gzk7Px+uQj+JoczBaZcPTWUS6HX8bDxYOnSj1l7XDsgiT49ig+HmbMgHLl1BrkiYlqDfVTp9QIq6entSMUOVGkiJpeAbB4cZo3OXIk6fz775thwDcqCjp2hIsXVWf/FStsq+linjxQpYo6b49l+gcPwiuvQPHiapT32jUIDFRVEgaDel+3bAl37lg7UpEVmqaOutWsqapL/P3VVJsfflBJvhBC5BJpzb+HpDn4kXGRhMfI9CXxeL+f+R2A5qWa4+XqZeVo7IMk+PZm0yb15XHoULVEWI0asG2bSgSLF7d2dMJU9HnuCxemmb0nT/D//hv++MOE+05MhN69Yc8etQze2rVqFQZbY2/z8OPj4ddfVcl27dowb57qlREcDD/9pA6mLFgAq1aBtzds3aoe4/Hj1o5cZMbdu6r6pm9fVV7TrBkcPgzdulk7MiGEsLj0EnwvVy/jGuYyD19khpTnZ50k+PYiNBS6dlUdw48dUwnXrFmwf79KGIRj6dpVjZgfP66ShGQ0DY4eVedbt1Y/R45UeblJfPghLF0Krq5q5L58eRNt2MTsZR7+3bswfrxa2q9bN3VExsVFleTv2qXi79lTLT8I8Oyz6vpSpdRKAU88Ab//bt3HIDK2das62Lp0qfrbjh2reqDIQVchRC6VXoIPMg9fZN69qHvsvKzmorYt19bK0dgPSfBtXXw8fPopVKqk1iB3doYhQ+D0aTX/3tn58dsQ9sfPT81/h1TN9q5cgbAwlUfMnatuevhwutX8WTN3rkpOAL77zrYPHukj+Pv2mfDohgkdPQr9+0OxYmoexeXLatnDDz9Uo/WLFqnkPS1Vq6rEv0kTePAA2reHL7+0n+Z7mmY/seZEbKz62zZvrt6Y5cqpgzPvvy//m4UQuZqe4JfyL5Xqd9JJX2TWn+f+JEFLoHKBymkeLBJpkwTfll2+rL44fvRR6mXv8uWzdnTC3PRu+osXp0hg9fL8ChXUtO0RI9TlUaNUvpFtmzerhFTfWK9eOdiYBVStCh4eaqrK2bPWjkZJSIDffoOnn4Zq1VRjwqgoNa1m7ly4dEkdsCtS5LGbIn9+tUTha6+pv/8776jzMTFmfxhZ9uCBivXjj9Vj9/ZWS3MGB6vpJh9/rKYh7NmjKhocwalT0LChqs7QNPW3OXAA6ta1dmRCCGF1epO9tJIyfR7+tQfXLBmSsEP6/Hspz88a6fpjq1atUk247t4FHx/4+muV8NlKF3Nhfm3bgq+vOtCzfbtxNF1P8KtVUz/ffFP1XAwNVSusZWtFrpMn1bSA+HhVOv7JJ6Z5DObk6gq1aqkR0717rTeVQJ8zsWaNSuj1lQ+cnaFzZ/UHatQoe+9dNzf1R61aFd5+WzVrO31aVfMUKGDax5EV16+r16R+CglJu4pi3z51elS+fGq0u1w59XfTz5crp17ztkzTVHXLsGEQGakey5w5auUSIYQQxCfGcznsMiAl+iL7EhIT+OOsajIlCX7WSIJva2Ji4L331Cg9qCXwfv5ZdTIXuYuHh0q6585VzfYeSfCrVlU/vbxUkccbb6jB4T591ABqpv37rzqYcP++GpH84Qf7OZAUHJxyHrulRESoiofff1eny5eTfpcvH/TrBwMHQokSOd+XwaAOElSsqJq4bd+uHvfq1UlHecxJ09Ro9fbtqn/A9u2qN8CjSpVSS8M9+aQ6oAFw5ow6nT6ddP7aNXXgcs8edXpUwYIq0a9QATp1gjZtbKcD/Z076m+7YoW63Ly5aoxYtKh143JAY8eOZfny5Zw8eRJPT08aNmzI+PHjqVChgvE2zZo1Y9u2bSnuN2DAAGbPnm3pcIUQyVwNv0qCloCrkyuFfQqn+r0xwZcSfZGBvdf2cjvyNn7ufjQs3tDa4dgVG/nWJAD15bdHD1XmCfDWWzBuXFLzLZH79OypEvylS9UwvZtbqhF8gFdfhcmTVaX6lCmqwj5ToqNVEhUaqprArVypDizYC0t20j9zRiXza9eqlSuSz4fw9FTJXufOqgLCywzLuLRqBbt3q/n4Z8+qgzELF0KHDqbdT2ys+h+UfIT+0eX6DAbVVC55Ql+sWOpt6UsZJvfwoYr/0cT/zBm4dSvptGOHOthUtKjqTP/qqxAUZNrHmhUbN6rVJa5dU9UjX3yhqiqcZKabOWzbto1BgwYRHBxMfHw8//vf/2jZsiXHjx8nT548xtv169ePMWPGGC97meO9J4TIEn3+fZB/EE6G1P8jpURfZMba06p7fssyLXF1drVyNPZFEnxbsXChapoXEaE65M+bp7ppi9ytWTMoXFiVRK9bR1ybDpw8qX6VPMF3dYXPPlPHhyZOVC+lx1Zwa5qaBrJzp1qve+1a65Z9Z4feSf/gQYiLU0+EqURHw19/qefl999Tz/MvVUo1QmzXDpo2VUm+uVWsqEa9u3VTFQSdOqmmiO+9l/2qi7g4dYBk61Z12rFDlZ4n5+EB9eurZL5xY9Uc0M8ve/vLk0cdHKhRI/XvwsKSkv89e+DHH+HqVVWa8tln6iBHv37qIIcp/9YZiYlRjREnTVKXK1RQDRJr17bM/nOpdevWpbg8b948ChYsyP79+2mSrPmnl5cXgYGBlg5PCJGBjBrsgTTZE5nz+1mZf59dkuBb28OHatL0vHnqctOmKtmXkk8Bah53jx5qWH7RIs6U7UBsrCrBf3Qws1s3mDBBDb5+8YW6S7ri42HoUDX9w8UFli1TyaO9KVtWHZy4f1/Ng69VK2fbu3Qpqex+06aUia6rq5om0a6dmtJQvrx1pjLkywfr1qk54F9/rTq2Hzum5upnpvoiLk4tr7lli0rot29PndAHBCSNzj/5pEpmLVFJ5OenpiXVqaNe9+PGqaqSb79VBzTWrVOnQoXUwanXXoMyZcwTi6ap3gJ9+6qfAAMGqNUMZJTY4sLCwgDI90iD2YULF/LTTz8RGBhI+/btGTVqVLqj+DExMcQka1IZHh5uvoCFyMUyarAHSSX6NyJuEJ8Yj4uTpCMipesPrnPg+gEMGGhTro21w7E78o6ypkOH1JzaU6dUmedHH6mRIlleSST34osqW1+1ihOtHwA+VK2aujLYyUkN5rZqpfK+YcPSqWgOD1evu3XrVII6Z44qL7dHTk6qa/nGjTBtmkpE4+KSTrGxKS9ndLp4USXKyRUpopL5du1Ud3gfH+s8zke5usJXX6kS+KFD1Uj3mTNqbvijo5nx8Sqh37pVJfXbt6sDi8kFBKiDi82awVNPQeXKtlF67u6uXqvdu6uR/e+/V1NWbt5Uyf+4cerv0q+fqmZwd8/efuLjVaPJAwdUNYh+0hPAgAC1744dTfbQROYlJiYybNgwGjVqRFW9+Qjw4osvEhQURJEiRTh8+DAjRozg1KlTLF++PM3tjB07lk/soYGoEHZOH8FPL8EvmKcgzgZnErQEbkbcNI7oC6HTu+cHFw2mYJ6CVo7G/kiCbw2aBrNmqfmbMTEqiVi0SH3BFuJRdeqo0eLTp9FWrAReJtl33BSeeUbl6ps3q5XJ9MIQo4sX1dSPo0dVSfnChWreuD2rX18l+PPnq1NOODlBgwZJSX316rbdcHDgQFUy3q2bmp9fr57qsJ+YmDKhj4hIeb98+VIm9FWq2EZCn5GyZdURrDFjVIPBb7+F9etVpcWmTWpZwd69VbKfrBFbKlFRqlOlnsQfOKAuR0envq27u2ry99VXmVvaUJjFoEGDOHr0KNu3b09xfX99WU+gWrVqFC5cmKeffppz585RJo3KjpEjR/L2228bL4eHh1O8eHHzBS5ELvW4BN/ZyZlA70CuPrjKtQfXJMEXqaw9o+bfS3l+9kiCb2n37qmyUn2EoV07lYXlz2/VsIQNMxjUKP7o0ZTavQh4Od3m6QaDyoHq11fNvYcPT+q2z549agTy5k01r3/1anXwwN4NHKi62D98qEa2s3vy91dJ7yMlwDbv6afV37Z9e1UNpPclSC5v3pQJfVolIPbC1VUtSdelC1y4oEbWf/hBNb+bPFmdmjRRif4zz6iR+eTJ/IkTkJCQers+PmqKh36qXVtNW7HUXH+RpsGDB7NmzRr++usviqXVyDGZ+vXrA3D27Nk0E3x3d3fcs1vlIYTItMcl+KDm4V99cJWrD64STBqfWyLXiomPYcP5DQC0LdfWytHYJ0nwzWzyZJVozZsHtaJ3qQ7bFy+qL43jx6s6alseIRS24b8Ev8atDRTgFtWqpV+uVK+eWl1v2TL44AP47TdUF/5evdQoZY0aas32x3xZthtFiuR85N7elSunRvB79IA//0w6WKEn9NWq2W9Cn5GSJVUDvo8/hj/+UKP6v/+umiP+9Vf69ytQQCXwyZP50qUd8zmyU5qmMWTIEFasWMHWrVspVSrtZl3JhfzXK6Fw4dTLcgkhLCM+MZ4r4VeAxyT4+lJ54dJoT6S0/dJ2ImIjKJSnELULS0Pb7JAE38x27oQjhxO5+dYE2P6hGjkqU0Y1N6tb19rhCXtRrhwJdYJx2b+X5/mFatUGZ3jzzz5T07FXrdK4+Po4gr75n/rFs8/C4sWqS59wLP7+Ksm9cAFKlMhdvTxcXFQFQ/v2cOWKGtH//nvVNLFEidTJfJEicmDVxg0aNIhFixbx22+/4ePjw40bNwDw8/PD09OTc+fOsWjRItq2bUtAQACHDx/mrbfeokmTJlSvXt3K0QuRe10Ou0yCloC7szuB3umvcKEn+LJUnniUXp7ftlzbNJdZFI8nCb6ZvdzyJgOWv0zLbarUhB494JtvwNfXuoEJu3Ol8YsE7d9LH9eF5M+fcYJfsSL06x3LE3P7E/TNf6Pbw4appb5yU+KX2xgMavm+3KxYsaSGpZGRcjDLTs2aNQuAZs2apbh+7ty59OnTBzc3NzZu3MjUqVN5+PAhxYsXp2vXrnz44YdWiFYIodPL84P8gzJMzor4qL4mslSeeJTMv885qx4WGTt2LMHBwfj4+FCwYEE6derEqVOnMrzPnDlzaNy4MXnz5iVv3ry0aNGCf/75J8VtNE3jo48+onDhwnh6etKiRQvOnDljzoeStoQEOkx5ipZsIBJPLoz6XjXTk+ReZMP2ot1JwIm6cbvh/PmMb3znDtNOtKQP84nHmaNvfKU68UtyL3ILJydJ7u2Ypmlpnvr06QNA8eLF2bZtG3fu3CE6OpozZ84wYcIEfOXzVQirysz8e8DYWE8SfJHc2btnOX3nNC5OLrQo3cLa4dgtqyb427ZtY9CgQezevZsNGzYQFxdHy5YtefjoEk7JbN26lRdeeIEtW7awa9cuihcvTsuWLbl6NekfxIQJE5g+fTqzZ89mz5495MmTh1atWhGdVpdkc3J2xmns51zyrUpd9vF1dF8pCxXZ9s/lwmzmv+XsFi1K/4ZnzkCDBrjv3ka0mw/PsoaeOwaSmGiZOIX1HDwIL7+sVj6UJb6FEEJYmjHB9yuZ4e1kDr5Ii748XuMSjfHz8LNyNPbLqgn+unXr6NOnD1WqVKFGjRrMmzePS5cusX///nTvs3DhQgYOHEjNmjWpWLEi3333HYmJiWzatAlQR/2nTp3Khx9+SMeOHalevToLFizg2rVrrFy50kKPLJnOndn77UFOUJlfflEr5AmRHUeOwEJ6qgsLF6b9Ytq2DZ54QiX5QUHEbN7Jbr/WHD6spt4Lx/bxx/DTT9C/PwQGqmR/yxbk4I4QQgiLuBB2AXj8CL5eoi9z8EVyUp5vGjbVuSAsLAyAfFlYpioyMpK4uDjjfUJDQ7lx4wYtWiSVdfj5+VG/fn127dqV5jZiYmIIDw9PcTKlNu1dyJNHNc/fu9ekm851jh5VfbNyo6NHYQWdSXRzV0t/HTqU8gbz5qllwe7eVevk7dmDX6OqjBihfj1qFMTGWjxss3v4EKZOVauk5Waaphrpg5qGHhWlkv3mzVVfz08+Uf33hBBCCHMJvRcKQKm8GfeD0Uv0w2LCeBibfuWuyD0iYiPYemErAO3KS4KfEzaT4CcmJjJs2DAaNWpEVePC3Y83YsQIihQpYkzo9U67hQoVSnG7QoUKGX/3qLFjx+Ln52c8FS9ePJuPIm1eXqq5M8CSJSbddK7y779qie9atSCdP6XDunlTPf4HBj8S2/33Ylq4UP1MTFTr4b3yCsTFwfPPq2Hb/94DQ4eq0dzQULWKmKOZMQPeegtatVJJbW514YJ6jbi6qgKOXbvUSL6vr/rd6NGq/16LFuqlk5ufKyGEEOaR2Tn4vu6+eLupPikyD18AbDq/idiEWErnLU2FgArWDseu2UyCP2jQII4ePcrPP/+c6fuMGzeOn3/+mRUrVuDh4ZHtfY8cOZKwsDDj6fLly9neVnqef179XLpUymWz6/RptYz73bsqoctNjhxRP8uWBZeXX1QXFi9Ww9fdu8MXX6jrPvxQXe/pabxvnjyqdBvUkuERERYM3AK2bVM/jx7FWK2QG+mj9zVrgoeHmqnxzTdw/XrSSD7Apk3w0kvqoM/rr8OePTJ1SAghRM7FJsQak/XHJfgg8/BFSvr8+7Zl22KQnmU5YhMJ/uDBg1mzZg1btmyhWLFimbrPpEmTGDduHOvXr0+x5m1goFpz8+bNmyluf/PmTePvHuXu7o6vr2+Kk6m1aaMaOl++rL5Qi6xLftzl55/Vkt+5hZ7gV60KtG0Lfn5w9SpUqwa//qqGbefPVxm8U+q39auvqoMDt26pZvqOIjFRjVTrZsyANWusF4816f9Xnngi5fVeXtCzp0rsQ0PVSH7JkqoJ3zffqNtXqQITJ+a+yhghhBCmcyX8ColaIh4uHhTKU+ixt5d5+EKnaRq/n1UJvpTn55xVE3xN0xg8eDArVqxg8+bNlMrk+s0TJkzg008/Zd26ddStWzfF70qVKkVgYKCx6R5AeHg4e/bsoUGDBiaNPys8PKBjR3VeyvSz58oV9dPFRf184w01gJ0b6Al+tWqAuzs895y6IjQU8uWDjRuhV6907+/qCp99ps5PnKhKuR3BiRMQFqaS2MGD1XWvvKJGrXMbfQS/fv30b1OypKrmOHcuaSTf01M9j++9p+bud+iQuw6eCSFM68L9CwxcO1CStlxIL88P8gvK1AisLJUndIdvHuZK+BW8XL1oVrKZtcOxe1ZN8AcNGsRPP/3EokWL8PHx4caNG9y4cYOoZJNDe/XqxciRI42Xx48fz6hRo/jhhx8oWbKk8T4R/9UdGwwGhg0bxmeffcaqVas4cuQIvXr1okiRInTq1MnSDzEFKdPPGX0Ef8AAKFFCNS0cPdqqIVnM0aPqZ7Vq/13x2mtqycVy5VRm16TJY7fRrZvqX/DgQVJFv73TR+/r1YNJk6BGDbh9G3r3zl3vsZgYtUQeZJzg65ycVMn+jz+qgyHffqtG8hMSYPVqVSTy5psQH2/euIUQjufjrR8za98spuxyoHIxkSl6gv+4Bns6KdEXOr17/tOlnsbDJfvTroVi1QR/1qxZhIWF0axZMwoXLmw8LUk2xH3p0iWuJxuOmzVrFrGxsTz33HMp7jNp0iTjbd577z2GDBlC//79CQ4OJiIignXr1uVonr4ptGqlGl5duwY7d1o1FLukj+BXrAhff63OT5kCISFWC8kiEhPh2DF13pjg60vhHT6skvxMcHKCcePU+a+/VgdI7J3+PmrYUBU26O0HNmxwrKkIj3PokFohISBAdczPCj8/6NdPHSw5fjypEmL6dPU/684d08crhHBc2y9tB+D03dNWjkRYmt5Bv6RfyUzdXk/wr0VItUduJ8vjmZbVS/TTOvXp08d4m61btzJv3jzj5QsXLqR5n9HJhnINBgNjxozhxo0bREdHs3HjRsqXL2+5B5YOd3fQiwikTD/r9BH8YsWgXTtVEZGQoJKThATrxmZO589DZKR6/ZQtm+wXZcqouR9Z8Mwz8NRTKhnUG+/Zs+QJPkClSmrJPICRI2H/fquEZXHJy/Nz0pemUiXVx2DZMtWccfNmVR2hV5AIIURGbkTc4Py98wCcvXvWytEIS7sQdgHIXIM9SJqDLyP4ududyDvsvqK+yLQp18bK0TgGm2iyl5voZfq//urYSak56CP4+iqG06ap0cd9+2DmTOvFZW76/PvKlcHZOWfbMhiSRvEXLLDvxO32bTh1Sp1P3liuXz/o0kWtGPjCC463akBa0muwl11duqgR/VKl1AGmBg1g5UrTbFsI4bh2XNphPH/u7jkStVw0V0pkeok8nczBFwB/nvuTRC2RagWrUcKvhLXDcQiS4FvYM8+Av7/qVr19u7WjsR+xsUkdvvWFFgIDYfx4df7DD1N22XckKRrsmUC9eiqB0zT44APTbNMa9FHrihVVabrOYIA5c6BoUTWLYdgwq4RnUZlpsJdV1arBP/+oio+ICOjcGcaMyV29DYQQWbPjclKCH5MQw5XwK1aMRlhalhN8vUT/wTU5GJSLSXm+6UmCb2FubuqLMkiZflZcv64SUjc3KFAg6fp+/aBRI5WADB7smOt5p2qwZwKff67m5K9aZb/9IB4tz08uXz619rvBAN9/rxpbOqp//1Wj7KAO3phS/vzw559J8/I//lhVIeWGqgghRNYlT/BByvRzk9iEWGOpfWab7AV6B2LAQHxiPLcjb5szPGGjEhITWHd2HSDL45mSJPhWoJfpL1smXaozSx+dL1o05TLvTk5qLW9XV5WsrlhhnfjMydQj+KBGvV95RZ3//HPTbdeS9AQ/vdUvmzVT8/AB+veHS5csEpbF/fOP+lmxoqoOMjVXVzUvf84cdX7ZMnVQ7cIF0+9LCGG/IuMiOXD9AACVC1QGJMHPTS6FXUJDw9PFkwJeBR5/B8DV2ZWCeQoCMg8/t9pzdQ93o+6S1yMvTxQz0TxDIQm+NTz9tBphvHUL/vrL2tHYh0fn3ydXpQqMGKHODx6s1kV3FNHRqswcTJvgA7z/vhrh/v33pIMI9iIuDvbuVefTGsHXjR6tytbv34eePR2z74U5yvPT8tprsGULFCyoFm+oWxe2bjXvPoUQ9mPv1b3EJ8ZTxKcILUq1AODMnTNWjkpYSvLyfEMWur3KPPzcbe1pVZ7fqmwrXJxcrByN45AE3wpcXdUcaJAy/cxK3kE/LR98oFaLu34d/vc/y8VlbidOqKQ0b14oXNi02y5bFrp2VeeTrTJpFw4fVisL+Purkev0uLrCokXg46N6XnzxhcVCtBhTN9jLSKNGqqll7dpq+bxnnlFLLjri1BghRNbo5fmNijeiXIBavvXsPRnBzy2yOv9el3wevsh9ZP69eUiCbyVSpp81GY3gg1otbvZsdX7WLNUB3BEkL8/PyfJn6XnvPfVz0SL7alKYvDzf6TH/xUqXVkkowCef2G/PgbQkJiaV6Jt7BF9XvDj8/bdaoSA+HgYNgtdfV40whRC5V/IEv2w+taarlOjnHtlN8GWpvNzravhVDt08hAEDrcu2tnY4DkUSfCt56inVwOrOHVX2KjL2uBF8gObNoU8fNZrYv78q47Z35miwl1xwsJqrHh+ftH68PciowV5aXnopqUS/Z0/HmcZx6pR6LJ6e5nuNpMXLCxYuVEsuGgzw7bdq6tGtW5aLQQhhOxK1RHZeVv+YG5VImeBLd/TcIacj+FKin/vo/zNqF65Nfq/8Vo7GsUiCbyUuLknl0VKm/3iPG8HXTZqkDpwcPQqTJ5s/LnMzR4O9R+mj+N9+C/fumW8/ppTVBB/UKH6pUqo53OuvO0ZZuV6eX7eu+p9iSQaD6n2xejX4+qopEHXrwsGDlo1DCGF9J/49wf3o+3i5elGjUA1K+pfExcmF6PhoKb3OJULvhwJQyj9zHfR1Mgc/9zp4Q31hqF24tpUjcTyS4FtR9+7q5/LljjHabE76CP7jEvyAAPjyS3X+k0/g3DnzxmVulkjwW7eGqlXV0mf6NAdbdvWq6ojv5JS1ZeF8fdVUBGdn+Pln+PFH88VoKZZqsJeRdu3UgYby5dX7tFEjOWgpRG6jl+fXL1ofV2dXXJxcjCO5UqafO8gcfJFVeoJfK7CWlSNxPJLgW1GTJqoj9b17sGmTtaOxXbGxcPOmOp9Rib7upZdUuXB0tH2P1N67p5JZUCsFmIvBkDSKP22aet5smd5foXp18PbO2n2feEId+AE1d/ysnX/vtGSDvYxUrKhiad0aoqKgRw+1RKEjrloghEgt+fx7nV6mL530HV9MfIwxQZc5+CKzDl7/L8EvLAm+qUmCb0XOzvDcc+q8jHil79o1laS7uUGBTCytajCokWgPD9i4Uc0Vtkf66H2JEuDnZ9599eihqiNu3rT9ke3slOcn9/776uBaRAS8+KL9Nod7+DDpNWLNEXydvz+sWQPvvqsujxsHbdrA7dtWDUsIYQE7Lv2X4JdISvDL5fuvk76M4Du8S2GXAPBy9cryXGq9RP9O1B2i4218hEGYzPUH17n58CZOBieqF6pu7XAcjiT4VqaX6a9YYb+Jhrklb7CX2U7yZcvCRx+p82+9pZoZ2htzN9hLztVVPU+g+hjY8shrThN8Z2f46Se19ODevfDxx6aLzZL271d/pyJFMlfZYgnOzjBhgpoK4eUFGzZAnToqViGEY7oZcZNz985hwECDYg2M1xsb7clSeQ4veXm+IYtL/uT1yIuHiwcgZfq5iV6eXyGgAl6uXlaOxvFIgm9ljRqp9c3DwtSXYZFaZhvsPWr4cDW3/PZtdd7eWGL+fXKvvaZGYU+fhlWrLLPPrIqKggMH1PnsJvigXktz5qjz48fD5s05j83SbKU8Py0vvKD6A5Qtq/olNGoE339v7aiEEOagl+dXLVgVP4+kcjMp0c89sttgD8BgMBjL9CXBzz2kPN+8JMG3MinTf7zMLJGXFldX1RneYIB58+xvOUJLJ/g+PjBwoDo/frxt9i7Yv181pAwMhJIlc7atrl2hXz/1OF9+2f6qPGyhwV5GqlVTFRLt20NMjDqA1L+/7fd4EEJkjbE8P9n8eyDFUnmaLX6gCJPJboM9nXGpPJmHn2tIgz3zkgTfBuhl+r/9Jl9+05LdEXyABg1Uoz2AAQPs5/nVNMuW6OuGDAF3dzU6vH275fabWcnL87NYBZimKVOgQgXV5+HVV23zoEZ6bHkEX+fvDytXwmefqb/XnDnQuLEa1RdCOAZjg70SKRP8kv4lcTY4ExUfxfWI69YITVhIjhN8WSov15EE37wkwbcBDRpA0aIQHg7r11s7GtuT3RF83dixahrEmTPwxRemi8ucLl9W0zZcXFQCaimBgdC7tzo/YYLl9ptZOZ1//6g8eWDxYtXA8bffoFs3uH/fNNs2pytX1AoLzs5qjrstc3KCDz6AP/6AfPlg3z4Vs6wc4phKlizJmDFjuCRHcXKFqLgoDlxX86YeHcF3c3YjyD8IkEZ7js5UI/hSop87hEWHcf7eeUBK9M1FEnwb4OSkEguQMv205GQEH1QH+hkz1Plx4+D4cdPEZU56eX6FCir5tKR33lGjrWvWwLFjlt13RjQtaYm8Bg0yvm1W1KqlpnK4usKyZVCzZlL5u63SR++rVlUHKexBq1ZqikXt2qovRsuW6v1oT1UT4vGGDRvG8uXLKV26NM888ww///wzMTEx1g5LmMnea3uJS4yjsHfhNJM7mYefO+Q0wTculScj+LlCyI0QAEr4lSCfZz7rBuOgJMG3EXqZ/qpVqpGYSJLTEXyALl3UXOC4ODUPODHRNLGZizXK83Xly0Pnzur8pEmW3396zp+HW7fUAY/atU277d69VXVAmTJw8SI8+aTqQ2CrrxN7KM9PS8mSaurHK6+o53bkSNULITzc2pEJUxk2bBghISH8888/VKpUiSFDhlC4cGEGDx7MAb1DpnAYyZfHS6t7uiyV5/ii4pKmYGSnyR7IHPzcRsrzzU8SfBtRv75a7zwiAtats3Y0tiM2Vq3NDtkfwQc1Ij1zphrt3LEDZs82TXzmYukGe4967z31c+HCpAoKa9PL8+vUAQ8P02+/bl3Vob9HD7X83Pvvq3Xc9defLbH1BnsZ8fRUHfW/+UYdrFmxAurVs4/KGpF5tWvXZvr06Vy7do2PP/6Y7777juDgYGrWrMkPP/wgTdcchHH+/SPl+TpZKs/xXQpT03G83byzPRorc/BzF0nwzU8SfBthMEiZflqu/ve/3t0d8ufP2bZKlIDPP1fnhw1Tib6tsnaCX78+NGmiKh6mTbNODI8y9fz7tPj6qjXcv/tOJaLr16uS/Y0bzbfPrIqPT1pX3t5G8HUGg6qk+ftvVZlz6pRK8pcutXZkwlTi4uL45Zdf6NChA++88w5169blu+++o2vXrvzvf/+jZ8+e1g5R5FCilsjOy+of8+MSfCnRd1zJy/PTquLIjORz8OXgn+OTJfLMTxJ8G6KX6a9eDZGR1o3FVuijx8WKmaZr+pAhqlw/Lk79tMU+UHFxcOKEOl+1qvXi0Efxv/nGNhrPWSLBB/U6e/VVtcRblSpw44aaL/7BByq5trajR9X/Bz8/yzZgNId69dTBiqeegocP4fnnYfhw23ieRfYcOHAgRVl+lSpVOHr0KNu3b+eVV15h1KhRbNy4kRUrVlg7VJFDJ2+f5F70PbxcvagZWDPN2yQv0ZfEzTHldP49QGGfwgBEx0dzL/qeCaIStio6Pprj/6qSPRnBNx9J8G1I3bpqjmpkJKxda+1obIMp5t8n5+QE8+dD9epqPnenTiqxsCWnT6sk39sbgoKsF0ebNirBffBAJfnWFB6eVNVgygZ7GalSBf75R400a5pagaFZM+sfFNLL8+vVU69ne1ewoKqU0A8oTZ4MLVrY5tQI8XjBwcGcOXOGWbNmcfXqVSZNmkTFihVT3KZUqVL06NHDShEKU9Hn39crWg9XZ9c0b1PSvyROBicexj3k5kN5UzsiY4LvVzLb2/Bw8SDAMwCQefiO7uitoyRoCQR4BlDM10Rf7kUqDvD10HEYDGoEC+CXX6wbi63IaQf9tHh7q2aGBQrAwYOq4ZctDSzoDfaqVrVuAufkBO++q85PnQrWbIS9Z4/6G5UqpZY8tBQvL3VwY8kSVb6/Y4cq2V+50nIxPEpvsGeP8+/T4+Kimhr++qt6f27bpnot2NIqDiJzzp8/z7p16+jWrRuurmknfXny5GHu3LkWjkyY2uPm3wO4u7hTwq8EIGX6jir0figApfJmr8GeTp+HL0vlObbk5fnZndIhHk8SfBujl+mvXasa7uV2ph7B1wUFqSXRXF3VvF99br4tsPb8++ReeAGKFlVl6j/9ZL04zLE8XlY8/7w6GBQcDPfuqVUGhg6F6GjLx2LPDfYep2tXNTWiYkXVf0M/wCTsx61bt9ijH4VKZs+ePezbt88KEQlzyUyCD9JJ39GZokQfZKm83EIa7FmGJPg2plYttVRXVJRahzy3M8cIvq5xY/j6a3V+1CjVzdsW2FKC7+YGb72lzk+caL1l4yw1/z4jpUurJd6GD1eXZ8xQBxxOn7ZcDPfvw8mT6rwjJvigkvvVq9X5devUsoXCfgwaNIjL+pHZZK5evcqgQYOsEJEwh5sRNzl79ywGDDQonvGRV2MnfUnwHZKpEnxZKi93kATfMiTBtzFSpp+SuUbwda+9phrvAbz8Mhw+bJ79ZIWe4FuzwV5y/furhm6nTiUlXpaUmJg0gm/NBB/UAY+JE+H339WqDiEhULs2/PijZfa/d6/6Wbq0mmLiqMqWhebN1bSM77+3djQiK44fP07t2rVTXV+rVi2Oy1qIDkPvnl+lYBX8PfwzvK2xk/5dKdF3NFFxUcbeCiZL8GUE32ElJCZw+Kb6oi0d9M1LEnwbpJfp//67anCWm5lzBF/35ZeqqdfDh9ChA/z7r/n29TgPHkComs5mEyP4AD4+MHCgOj9hguX3f/y4arKXJ4/tPCdt2sChQ0nd33v1gj59zD+tRi/Pt9fl8bJiwAD18/vvpau+PXF3d+dmGh0Sr1+/jouLixUiEuaQ2fJ8kBJ9R3YxTJVY+bj5kNcjb462JXPwHd/pO6eJjIvEy9XL+H9BmIck+DaoenUoX141NVu1ytrRWE9MTFInbXON4INq8LVkiZoacfEiPPccxMaab38Z0Qe4AgPVCLGtGDpUjV7v3KkazVmSXp5fv776W9mKIkVgwwYYMyZpdYYmTcybjDpig730dOqkqhSuXZNVRexJy5YtGTlyJGFhYcbr7t+/z//+9z+eeeYZK0YmTCkrCX7yEn1ZKs+xhN5TIxIl/UvmuGGazMF3fHp5fo1CNXB2crZyNI5NEnwbZDAkjeLn5jL9a/8dxHV3N3+ymy+fOpji4wN//aXK9q3xPcSW5t8nFxgIvXur85YexbeF+ffpcXZW/Ru2bIG8eVUjvt9+M8++NM2xG+w9ys1NVUUAfPutVUMRWTBp0iQuX75MUFAQTz31FE899RSlSpXixo0bTJ48OdPbGTt2LMHBwfj4+FCwYEE6derEqVOnUtwmOjqaQYMGERAQgLe3N127dk2zekCYVlRcFPuv7QegUYnHJ/il8pbCgIEHsQ+49fCWucMTFqTPv89pB32QOfi5gbGDvsy/NztJ8G2UPg9/3TpINhCSqySff2+JlTQqV4bFi9W+vv02qQGfJdlqgg/wzjvquVm1Ck6csNx+bWX+fUaaNEmaxjBzpnn2cf483LmjEt+aNc2zD1vTr5/6+ccfcOmSdWMRmVO0aFEOHz7MhAkTqFy5MnXq1GHatGkcOXKE4lmYa7Vt2zYGDRrE7t272bBhA3FxcbRs2ZKHDx8ab/PWW2+xevVqli5dyrZt27h27RpdunQxx8MSyey7to+4xDgKexemlP/jEzsPFw/jUnlSpu9YjA32/ErmeFt6if6th7eIS4jL8faE7TE22JP592YnCb6NqlIFKlVSpeLmGhG0dZaYf/+odu1g3Dh1/s03YfNmy+0bbK/BXnIVKqiyaVCN5izh9u2kLvW2Pu98wAA1or91a9Lf0ZT08vxatVRVS25Qrpw027NHefLkoX///nz11VdMmjSJXr164erqmqVtrFu3jj59+lClShVq1KjBvHnzuHTpEvv3q5HjsLAwvv/+e7788kuaN29OnTp1mDt3Ljt37mS3XuoizMJYnl+iUabLsqWTvmO6EHYByHmDPYD8XvlxdXJFQ+NGxI0cb0/YFk3TpIO+BUmCb6OkTN/8HfTT8+678NJLkJAA3brBuXOW2a+m2fYIPsB776mfP/2k1ik3N330vlIlVQJvy4oXTzoA8tVXpt9+bmqwl1z//urnd99Jsz17cvz4cdatW8eqVatSnLJLn9OfL18+APbv309cXBwtWrQw3qZixYqUKFGCXfo/jkfExMQQHh6e4iSyLivz73WS4DsmUy2RB+BkcKKwT2FA5uE7osvhl7kbdRcXJxeqFrTBUSwHk60E//Lly1zRh1eBf/75h2HDhvGtTJQ0qW7d1M/16+HePevGYg3WGMEHdXBlzhyoVw/u3lWd9S3xPfDWLTVibTCo6QK26IknoHFjiIuDadPMvz9bnn+fFn3JxR9/VGvWm1JuarCXXKdOqgfHtWtqZRFh286fP0+NGjWoWrUq7dq1o1OnTnTq1InOnTvTuXPnbG0zMTGRYcOG0ahRI6r+V95048YN3Nzc8Pf3T3HbQoUKceNG2qN/Y8eOxc/Pz3jKypQBoSRqiey4lPUEX++YLUvlORZTJvgg8/AdmT7/vnKByri75JIyRCvKVoL/4osvsmXLFkB9yD7zzDP8888/fPDBB4wZM8akAeZmlSurUu24OFi50trRWJ61RvABPDxgxQrVKf34cejZU43om5M+el+2LHh5mXdfOaGP4s+ebf7+EPaW4Ddpot6zkZEwd67pthsdrRr4Qe5L8N3dpdmePXnzzTcpVaoUt27dwsvLi2PHjvHXX39Rt25dtm7dmq1tDho0iKNHj/Lzzz/nKDa9u79+uqx/yIhMO3n7JPei7+Hl6kXNwJqZvp+M4Dueh7EPjU0TTdFkD2SpPEcWciMEkPJ8S8lWgn/06FHq1asHwC+//ELVqlXZuXMnCxcuZN68eaaML9fTy/QXLoTr11XikFtWmbHWCL6uSBF1YMXdHdasgQ8/NO/+bL08X9e2rTr49OABfPON+fYTFwf//KPO20uCbzDA4MHq/FdfQWKiabYbEqKejwIFoJRpvkfZFWm2Zz927drFmDFjyJ8/P05OTjg5OfHkk08yduxYhg4dmuXtDR48mDVr1rBlyxaKJTvaGxgYSGxsLPcfKZW5efMmgYGBaW7L3d0dX1/fFCeRNfrofb2i9XB1znxfBVkqz/FcDLsIgJ+7H/4e/ibZZhFvWSrPUcn8e8vKVoIfFxeH+39dnjZu3EiHDh0ANf/t+vXrpotOGMv0N21SCWeePKqLdoECaqS3Th3VhKpzZ3jlFRg2DD7+GL78UjWl+vVX2LgR9u1To4D2RB9csWYVZXAw/PCDOj9uHCxaZL592UuC7+Sk+hQATJ0KMTHm2U9IiHrN5ssH5cubZx/m8NJL4Oenejf8+adptpm8PN8SK0rYmvLl4amn1AETabZn2xISEvDx8QEgf/78XPtvvdOgoKBUy9xlRNM0Bg8ezIoVK9i8eTOlHjmyVadOHVxdXdm0aZPxulOnTnHp0iUaNGhggkci0pKd+fcAZfKVwYCBsJgwbkfeNkdowsJMXZ4PSSP4kuA7Humgb1ku2blTlSpVmD17Nu3atWPDhg18+umnAFy7do2AgACTBpjbVagAr74Ky5apcmhNU42mbt9Wp6xo1Ai2bVOdvm1dTIyakw7WKdFP7sUXVfI9bpz6W5QrpxJ/U7PlDvqPevFF+OADNS964ULo29f0+9D7ZDVooA4q2Is8edTzMWUKzJgBbdrkfJu5tcFecv37w5YtKsEfNQpcsvXpJcytatWqHDp0iFKlSlG/fn0mTJiAm5sb3377LaVLl870dgYNGsSiRYv47bff8PHxMc6r9/Pzw9PTEz8/P1599VXefvtt8uXLh6+vL0OGDKFBgwY8kZvfKGaW3QTfw8WDYr7FuBx+mbN3z1IgTwFzhCcsyCwJvszBd0h3Iu9wKUyV32Vlao/Ivmx9RRo/fjydO3dm4sSJ9O7dmxo1agCwatUqY+m+MJ3vvlOnxESIiFCJ/qOn+/fTvl7/3ZkzsGOHmsP6xhvWfkSPp3do9/AAWzhm9NlncPSoKtXv1An27lUVFaaSmAjHjqnztj6CD6qK5K231Ej+xIlqjrSpk3B9/r09DsYNHKiqG/74A86eVdU2OZFbG+wl17mzarZ39ap6Xtu3t3ZEIi0ffvihca36MWPG8Oyzz9K4cWMCAgJYsmRJprcza9YsAJo1a5bi+rlz59Lnv6YMU6ZMwcnJia5duxITE0OrVq34+uuvTfI4RGo3I25y9u5ZDBhoUDzr/5jL5itrTPCzc39hW8w5gi9z8B2LPv++TN4y+LrL1ChLyFaC36xZM27fvk14eDh5k61d1b9/f7xsuTuYnXNyAl9fdcpq2fqMGTB0KIwcqb4opzNF0Wbo8++LFbONkmRnZzVS3aCBarrXubOqhvDwMM32z5+HqCi1vZwmg5bSvz98+imcPKm6xvfubdrt21uDveTKllUj97//Dl9/rabMZNetWxAaqt4H5qgcsRd6s71Jk1TvB0nwbVOrVq2M58uWLcvJkye5e/cuefPmzfSa6UCm5ml7eHjw1Vdf8ZU51qUUqey8rP4pVylYJVtzrsvmK8uWC1ukk76DCL0fCkApf9M1hiniI3PwHZGU51tetsbcoqKiiImJMSb3Fy9eZOrUqZw6dYqCBQuaNEBhGgMHqvn6YWHw9tvWjubxbGH+/aN8fWHVKjUn/J9/wJQLRujl+ZUr28cUClDPxwcfqPPvvWfaZeEuX1YnZ2f7TWr1Zns//KAqb7JLH72vVEnN7c/NpNmebYuLi8PFxYWjR4+muD5fvnxZSu6Fbcpueb5OXypPOuk7BnOW6EfERhAeY4H1iYVFSIM9y8tWgt+xY0cWLFgAwP3796lfvz6TJ0+mU6dOxrI6YVucndWol5MTLF4MGzZYO6KMWXOJvIyUKZPUdG/ixKSy+pyylwZ7jxo2DCpWVKPMH39suu3q8+9r1ABvb9Nt15JatVIj+WFhqvoju6Q8P0n58tCsmZrSor8Phe1wdXWlRIkSJJh7TVFhFTlN8GWpPMdijgQ/j1se/NzVkWwp03ccB69Lgm9p2UrwDxw4QOPGjQH49ddfKVSoEBcvXmTBggVMnz7dpAEK06lTJ2lUceBA2+6qb+0l8jLSsaM6xcfD66+bZik0e2qwl5ybm5r+ATBzJhw+bJrt2nN5vs7JCQYNUudnzsz+8pbSYC+lAQPUz++/V+9BYVs++OAD/ve//3H37l1rhyJMKCouiv3X9gPQqETOEvwzd8/IUnl2LiI2wrgaQpB/kEm3bSzTl0Z7DiEyLpJTd9QKKlKibznZSvAjIyONy+CsX7+eLl264OTkxBNPPMHFixdNGqAwrU8/Vc3hzp6FsWOtHU36bHUEXzdjhuqWvn27aUYS7XUEH6BFC3juOXWgY9Cg7CeyyTlCgg9qzriXl2rQuG1b1u+fkKCmg4CM4Os6d1aNN69cUaX6wrbMnDmTv/76iyJFilChQgVq166d4iTs075r+4hLjCPQOzDbc67L5CsDwP3o+9yNkgNA9uziffVd39/DP1v9GDIiS+U5lsM3D5OoJRLoHUigt403AHMg2Urwy5Yty8qVK7l8+TJ//vknLVu2BODWrVv4+kp3RFvm6wvTpqnz48ZBFpYltihbHsEHFdd/q0Py3ntJS/plR3S0WuUA7DPBB9VEzstLHfD46aecbSsqCg6qai67T/D9/eHll9X5mTOzfv+TJ+HBA/XcVqli0tDslt5sD9SqIMK2dOrUieHDhzNy5EhefPFFOnbsmOIk7FPy8vzs9lPwcvUyzrGWMn37pjfYM2V5vk6WynMsUp5vHdnqov/RRx/x4osv8tZbb9G8eXMa/LeO1fr166lVS/6Atq5rV9Xh+48/1JJ5mzbZRqf65Gx9BB9gyBBYsABCQmD4cHU+O06cUKPf+fJB4cImDdFiiheHDz+E//1PLZ3XoUP2G8Lt26dKrwsXhhIlTBunNQwerPpfrFypXtdZOWilz78PDpZ135Pr1w8mT1arFGT1ORXm9bEpm3EIm5HT+fe6svnKcvXBVc7cPUP9YlKWZK/0+fem7KCv0xN8mYPvGKTBnnVkawT/ueee49KlS+zbt48///zTeP3TTz/NlClTTBacMA+DAb76Si3JtmVLzkdcTS06Gv79V5235S/uLi4qcTMY1DJxmzZlbzvJy/Nt7UBLVrz9tmqCdvMmjB6d/e0kL8+35+dDV7WqagyXkACzZ2ftvtJgL20VKkizPSEsJVFLNC6Rl9359zrppO8YzNFgTydL5TkWWSLPOrKV4AMEBgZSq1Ytrl27xpX/6qnr1atHxYoVTRacMJ9SpeCjj9T5d94BW+qHdPW//+keHmpU25bVq5fUSO2NN7LXuNBeG+w9yt09qeHejBlJjyurHGX+fXJ6c8tvv83aa0Qa7KWvf3/187vvpNmeLXFycsLZ2Tndk7A/p26f4m7UXTxdPHM8Cied9B2DORN8mYPvOOIS4jhyU30ZlBF8y8pWgp+YmMiYMWPw8/MjKCiIoKAg/P39+fTTT0nMQkvxsWPHEhwcjI+PDwULFqRTp06cesyk8GPHjtG1a1dKliyJwWBg6tSpqW6TkJDAqFGjKFWqFJ6enpQpU4ZPP/1UurY+4p131Lrr//4L779v7WiSJJ9/bw8juJ99psrJz5xRfQ2yyp4b7D2qZUvo0kWNVg8enPWGe5rmmAl+x45qusnt27B0aebuExGhmvOBjOCnJXmzvXXrrB2N0K1YsYLly5cbT0uWLOH999+ncOHCfCtNE+ySXp5fr2g9XJ1dc7QtSfAdg1kTfJmD7zBO3j5JTEIMvu6+lMpr+ukcIn3ZSvA/+OADZs6cybhx4zh48CAHDx7kiy++YMaMGYwaNSrT29m2bRuDBg1i9+7dbNiwgbi4OFq2bMnDhw/TvU9kZCSlS5dm3LhxBAam3Y1x/PjxzJo1i5kzZ3LixAnGjx/PhAkTmKEPLwpALXGmlwzPmZOUWFmbPcy/T87PL6lx4dixWW9cqCdxjpDgA0yZAp6e8NdfsHhx1u579qxKgt3dwZHaebi4qAoPSKpyeJx9+1QJerFiauULkZKHB/Turc5L3mg7Hm2q99xzz/H5558zYcIEVq1aZe3wRDaYav49QLkAVaJ/5u6ZHG9LWI9Zm+z9N4J/I+IGCYkJJt++sBy9PL9mYE2cDNkuGhfZkK1ne/78+Xz33Xe88cYbVK9enerVqzNw4EDmzJnDvHnzMr2ddevW0adPH6pUqUKNGjWYN28ely5dYv/+/eneJzg4mIkTJ9KjRw/c3d3TvM3OnTvp2LEj7dq1o2TJkjz33HO0bNmSf/T1poRR48bQt686P2AAxMVZNx6w/Q76aXnuOWjbFmJj4fXXMz9yfe9e0pQEey/R15UoAR98oM4PHw7h4Zm/765d6mfduirJdySvvaYOqu3dm7T0XUb0+fdSnp8+vUx/7dqk/xvCNj3xxBNsym6jEmFVOy79l+DncP49QJm8aqm8u1F3Zak8OxUeE27825kjwS+YpyBOBicStARuPczBEkU27Ozds8YqCEcmHfStJ1sJ/t27d9Oca1+xYkXu5mAyd1hYGAD5cjjxumHDhmzatInTp08DcOjQIbZv306bNm3SvH1MTAzh4eEpTrnJhAmq1PXoUUhjxoPF2dsIPiQ1LvT0hK1bVdO9zNDL84OC1BKGjmL4cChbFq5fhzFjMn8/RyzP1xUsCD16qPOZWTJPGuw9XoUK0LSpqnT4/ntrRyPSExUVxfTp0ylatKi1QxFZdOvhLeNoe4NiDXK8vTxueSjsrZaLOXf3XI63Jyzv4v2LAOTzzIevu+m/uLg4uRjXS3fEefh3o+5S99u61P+uPtHx2WjcZEekg771ZCvBr1GjBjPT+IY6c+ZMqlevnq1AEhMTGTZsGI0aNaJqDocy33//fXr06EHFihVxdXWlVq1aDBs2jJ49e6Z5+7Fjx+Ln52c8FbenoWMTCAiASZPU+dGj4eJFq4ZjlyP4ACVLgr5C1DvvwJ07j7+PozTYe5S7O0yfrs5PnQrHjmXufnqC3yDn3yNtkt5sb8kSuJXBwISmSYO9zErebC9BqjmtLm/evOTLl894yps3Lz4+Pvzwww9MnDjR2uGJLNK751cpUIW8nnlNsk0p07dv5px/r3PkpfLWnl5LWEwYtx7e4sD1A9YOx2w0TSPkRgggHfStIVsrK0+YMIF27dqxceNGGvz3TXzXrl1cvnyZ33//PVuBDBo0iKNHj7J9+/Zs3T+5X375hYULF7Jo0SKqVKlCSEgIw4YNo0iRIvTWJ20mM3LkSN5++23j5fDw8FyX5PfuDfPmwbZtKglZtcp6De7scQRf9/bbatnBo0fhvfceP6roSA32HtWmjWou99tv6jW1eXPGr6mwsKR+BI6a4AcHqxH5PXtU3wt9KsOjrlxR1Q/OzlC7tmVjtDdduqRstteunbUjyt2mTJmCIdkb3cnJiQIFClC/fn3y5jVNgigsx1ieb4L597qyecvy18W/pNGenbJEgm9cKs8BG+2tOp3Ui2THpR00LO6AJYuoPg1hMWG4O7tTKX8la4eT62RrBL9p06acPn2azp07c//+fe7fv0+XLl04duwYP2a2NjmZwYMHs2bNGrZs2UIxE2R17777rnEUv1q1arz88su89dZbjB07Ns3bu7u74+vrm+KU2xgMMGsWuLrCmjWwcqX1YrHXEXxQz98336jzP/ygGs1lxNEa7D1q6lTVDG3rVjVqnZE9e9TIdenSkE7/TIegj+LPmpX+8m766H2NGuDlZZm47FXyZnv6e09YT58+fejdu7fx9PLLL9O6dWtJ7u2UscGeCebf66STvn0zNtjzK2m2fRg76TtYiX5MfAzrziYt+6K/vxyRPv++asGqOV59Q2RdtlsaFilShM8//5xly5axbNkyPvvsM+7du8f3WZgIqWkagwcPZsWKFWzevJlSpUyzhEJkZCROTikfmrOzc5aW8MuNKlVSo84AQ4bAgweWjyE6Wi3bB/Y5gg9q/rheNvz666rxXlo0zfET/JIlYeRIdf6ddzJ+TTny/PvkunVT8/GvXk3/QJrMv8+afv3UT2m2Z31z585laRprQS5dupT58+dbISKRXdHx0ey/rpoem3IEX0r07Zs+gm/OZc/0TvqOluBvDt1MRGwELk6qgHrn5Z0Ou4S3zL+3LquuWTBo0CB++uknFi1ahI+PDzdu3ODGjRtERUUZb9OrVy9G6hkCEBsbS0hICCEhIcTGxnL16lVCQkI4ezbpSHD79u35/PPPWbt2LRcuXGDFihV8+eWXdO7c2aKPzx598IEaQb16NWk+uSXpHeU9PSGHvRatatw4lcSdOAHpTTu9fFmVpbu4qGZhjuq999Rr6to1+PTT9G+XWxJ8d/ekA0DpNduTBD9rKlaEJk1Us70ffrB2NLnb2LFjyZ8/f6rrCxYsyBdffGGFiER27bu2j9iEWArlKUTpvKVNtl0ZwbdvMgc/+1adUuX5L1d/GXdnd/6N/NdhD3QZE3yZf28VVk3wZ82aRVhYGM2aNaNw4cLG05JktbyXLl3i+vXrxsvXrl2jVq1a1KpVi+vXrzNp0iRq1arFa6+9ZrzNjBkzeO655xg4cCCVKlVi+PDhDBgwgE8zyi4EoBLrr75S56dNg4MHLbv/5PPvrdUDwBTy5lXrwQN89pla3/1R+vz7ChXU8mmOysMjqeHelCnqoMejEhKSklpHT/BBLUnp7Kx6XuivA11cHOzbp85Lg73MGzBA/ZRme9Z16dKlNKvxgoKCuHTpkhUiEtmVfHk8gwk/kPWl8m5H3uZ+9H2TbVdYhszBz55ELdE4/75b5W4EFw0GkhpZOhpZIs+6rJrga5qW5qlPnz7G22zdupV58+YZL5csWTLN+2zdutV4Gx8fH6ZOncrFixeJiori3LlzfPbZZ7g5chZlQq1bw/PPq9GwAQMs+2XZnuffP+qFF6BFCzXtYOBAVZKfnCM32HtUu3bQvr2acz5kSOrn4vhxCA8Hb2/HW1EgLcWKgV5Q9Ogo/pEj6jXj7w/lylk8NLvVpYuq+rl8WTXbE9ZRsGBBDh8+nOr6Q4cOERAQYIWIRHYZ59+bsDwfwMfdx7gMmozi25ew6DDuRd8DIMgvyGz7ccQS/f3X9nPtwTW83bxpXqo5DYup0Qz9QJojuRlxk+sR1zFgoHqh7K2uJnImS130u3TpkuHv79+/n5NYhA2ZMkV9Sd67VzWuGjjQMvu15w76j9IbF1atChs2wM8/q6Rf5+jz7x81dSqsXw+bNsGvv6q56Dq9PL9+fTWynRsMGaKeh59+UlM69B5keoO9+vXByaqHYO2L3mxvyhT49lvppm8tL7zwAkOHDsXHx4cmTZoAsG3bNt5880169Ohh5ehEZmmaZhxZNHWCD6pM/0bEDc7ePUvdInVNvn1hHvrofYBnAD7uPmbbj16ifz/6PpFxkXi52n+3Wb08v1WZVri7uKvGlTsds9GeXp5fIX8F8rjlsXI0uVOWvj4mXys+rVNQUBC9evUyV6zCgooUgc8/V+dHjoQbNyyzX0cawQcoWxY+/FCdf+stuHcv6Xe5aQQf1Dz8999X5996CyIikn6XW+bfJ9e4sfrbR0aqJSp1Mv8++/Rme2vWJPXzEJb16aefUr9+fZ5++mk8PT3x9PSkZcuWNG/eXObg25FTd05xJ+oOHi4eZplDK/Pw7ZMlGuwB+Lr7ksdVJYaOMg//t1O/AdCxQkcA4/J4J26f4G7UXavFZQ5Snm99WRrBnzt3rrniEDbojTdg/nw1H/itt2DxYvPv05FG8HXvvgsLF8LJk+pgyezZap61Phc9tyT4ACNGwIIFEBqqehOMG6euz40JvsGglswbMED1vXjzTTViLwl+9lWqpJrt/fWXarY3apS1I8p93NzcWLJkCZ999hkhISF4enpSrVo1goLMV84rTE8vG65XtB5uzqaf3lgun5p/JAm+fbHE/HsAg8FAEZ8inLl7hqvhV40HhOxV6L1Qjtw6grPBmXblVXlZfq/8VAiowKk7p9h1eZfxekeQ2Q76kZGq6XCjRirnEKYjBaAiXc7OqjzfyUmVl69fb/59OtoIPqiu6bNnq/PffAO7dsHp0yrJ9/aG3PS919NTleoDfPklnDoFt24lNSHMbU3levZUc+3PnVNTYu7dU88JSIKfXfoKBXPmSLM9aypXrhzdunXj2WefleTeDplr/r1OT9gctYO4LiouyqEOYhgTfL+SZt+XI83D18vznyzxJPk8k5aI0kfxHa1MP7Md9JcsUQNgb76pVloSpiMJvshQ7dpqrjCoefjJVjA0C30E35ESfICmTUHvHTlgABw4oM5XrWrfqwVkR/v20LatOsAxZIg64AFQpYpKdnOTPHmgb191fuZM+Ocfdb5sWZB+ZNnTtavqZ3D5Mvz5p7WjyX26du3K+PHjU10/YcIEuiVvvCFsmqUSfEdKftPSb3U/ys0ox6bzm6wdiklcCLsAmH8EHxxrqbxHy/N1+vvLkRL88Jhw4/v6cSP4P/+sfsbFJa22JExDEnzxWGPGqDn5584ljUSbQ3Q03L6tzjtSib5u4kSVtB05okr1IXeV5+sMBrUEo5ubaj748cfq+txUnp/cwIHqOfnjD/jxR3VdbqtkMCW92R6oZnvCsv766y/atm2b6vo2bdrw119/WSEikVX/PvyX03dOA9CgeAOz7ENP8G89vEV4TLhZ9mFtd6Pu8suxXwCYd2iedYMxEUuV6IPjLJV3N+ouf11U//s6VOiQ4neNSqgE/5+r/xCbEGvx2Mzh0I1DABT3LU6AV/ojFbduqabLutmz4cEDc0eXe0iCLx7L1xfeeUed37LFfPvRy/O9vJI6ijuS/Plh0iR1Xm8AlhsTfFAj1O+9p84fUp8FuTbBL1NGVTSAKlUDKc/PKb1Mf82apP8rwjIiIiLSXJLW1dWV8HDHTOQcjd49v3KByinKiU3J192XgnkKAo47ir/8xHLiEuMAVaIdEx9j5YhyLvReKGDZEXx7L9H/48wfJGgJVClQhTL5yqT4XYWACuTzzEd0fDQhN0KsE6CJZbY8f9kyNY2udm2oWBHCwtTUOmEakuCLTGnw30H8PXtSr2FuKvoX8WLFHLdsvXdvVa6vy60JPqgqhuRTcxuYZ6DILgwenPKyJPg5ozfbS0hQ1SLCcqpVq8aSJUtSXf/zzz9TuXJlK0Qkssrc5fk6Ry/T//noz8bz4THhbAq17zL9+9H3CYsJAyyU4DvIHPz0yvNBNRM0zsO/5Bhl+pltsKd/TLz4YtIg4tSpqlxf5Jwk+CJTatUCFxdVUnPpknn24ajz75MzGFQZkpubKiWuXt3aEVmPl1dSw73ChaF8eauGY1UtW0I51VQad3eoUcO68TiCESPUz1mz4M4d68aSm4waNYpPP/2U3r17M3/+fObPn0+vXr347LPPGCXLGtgFSfBz7kbEDbZcUCWPbcupEq1lx5dZM6Qc08vzC3gVsMja5o4wBz8mPoY/zv4BQMeKqRN8cLx5+JlZIu/qVbXaDcDzz8NLL0GhQioPSOP4sMgGSfBFpnh4JCUdeiMwU0s+gu/IKlZUjeU2b4Z85ql+tBudOsFvv8HatY5btZEZTk4waJA6X6+eOgAkcqZNG6hZEx4+hBkzrB1N7tG+fXtWrlzJ2bNnGThwIO+88w5Xr15l8+bNlC1r30td5QbR8dHsu7YPSJofbC76UnmO2En/l2O/kKgl8kSxJxjeYDgAK0+tJC7BfocnLTn/HpLm4F97cA3NXKWjZrb1wlYiYiMo7F2YukXqpnmb5Am+vT5OXUx8DMf+PQZkXKK/dKmqBm7USA3qeXjA0KHqdxMnmq9SODeRBF9kWr166qe5EvzcMIKvq107d5ekJ9ehg6oQye0GDVKJ6DffWDsSx2AwwP/+p85Pny7NeyypXbt27Nixg4cPH3L+/Hmef/55hg8fTg0pTbF5Wy9sJTYhloJ5ClImb5nH3yEHHHkEXy/Pf6HqCzQOakx+r/zcjbrLtovbrBxZ9lk6wS/sUxiA2IRYbkfetsg+TU0vz29fvj1OhrRTrrpF6uLq5MqNiBvG59heHfv3GPGJ8eTzzEdx3/S/zOvd83v0SLrujTfUykKHD6sGzCJnJMEXmaYn+Hv2mGf7uWUEX4i0uLioufiVKlk7EsfRpQtUqAD37qlSfWE5f/31F71796ZIkSJMnjyZ5s2bs3v3bmuHJTKQqCXy4eYPAehepTsGM5dVOWqCf+H+BXZd2YWTwYlulbvh4uRCpwqdAPsu07dkgz0AN2c3YyNGeyzT1zSNVadWAemX5wN4unpSu3BtwP7L9JOX56f3/yM0VOURTk7w3HNJ1+fNC6+9ps5PnGjuSB2fJPgi0/TGX/v3Q3y86befm0bwhRDm5+wM77+vzn/5JURFWTceR3fjxg3GjRtHuXLl6NatG76+vsTExLBy5UrGjRtHcHCwtUMUGVh8ZDH7r+/Hx82HUU3M3y9BT/BvRNzgQYzjlNgsOaomETcr2cw4Cv1cZZXJLD+5nITEBKvFlhMXwi4AUMq/lMX2ac+d9A9cP8DVB1fJ45qH5qWaZ3hbY5m+nTfay0yDvV/UypE0awaBgSl/99Zb6nN740Y4eNBMQeYSkuCLTKtQAXx8IDISjh83/fZlBF8IYWo9e0KJEnDzJvzwg7WjcVzt27enQoUKHD58mKlTp3Lt2jVmSPMDuxEdH83/Nqs5LSOfHEmBPAXMvk9/D3/ye+UH4Ny9c2bfn6UsProYgB5VkuqPnyr1FP4e/tx6eMtuR2ktXaIPSfPwr4bbX4Kvl+e3KtsKDxePDG+r97uw19eGLjNL5KVVnq8LClJN9yBpWWmRPZLgi0xzcgJ9AMbU8/CjouD2f1OsZARfCGEqrq7w3nvq/IQJsgSPufzxxx+8+uqrfPLJJ7Rr1w5nZ2drhySyYPqe6VwKu0Qx32IMe2KYxfbraGX6J/49waGbh3BxcqFr5a7G692c3ehQoQNgn2X6mqZZJcG35xH8jJbHe5S+VN7RW0cJiw4za1zmkpCYwKEbh4D0R/BPnYKQEDUlsUuXtLfz7rvq55IlcPGiGQLNJSTBF1lirnn4+ui9lxf4+5t220KI3K1v3/+3d+dxUZXt/8A/wzayC7IrIoi7gsgmombhgrmk2WLZk5blo6LlWtlmlobbY3uW/Uq/lUtZamZZmSnuCyrivqKgAm4sAgLCnN8fdzOCggJzZs7M8Hm/XvOaAzNzzsVx5HDNfd3XLZbgSU8Hli5VOhrLtG3bNty4cQPh4eGIjo7Gp59+iqtXzbMxVn1ztegqZm2dBQCY9dAs2NvaG+3Yuk761yyjk762uV6f5n3gbl95mZzH2ogy/Z+P/QyNpDF6bPrILc5Ffkk+ACCgYYDRjtvYxTyXykvLSUNqdiqsVFbo16LffZ/v4+SDILcgSJCw64J59io5ff00Cm8VwsHWAS0bVb3usXYJvN69gUaNqt5PWBgQFweUl99eSplqjwk+1Yp2Hr7cI/jaBN/fv34vl0ZE8rO3ByZPFtuzZ4s/HEhenTt3xldffYXMzEz897//xYoVK+Dn5weNRoMNGzbgBpcxMFnvJb2H/JJ8dPTpiGdCnjHqsS1pBF+SJKw4crt7/p16Ne8FJzsnXLxxEXsuGmg5IgNJyxUN9rwcveBg62C04+pK9M1sBP/Xk78CALo27YpGDtVksneouFyeOdKW54d4h8Da6u4KLkkClovZK3jyyXvvSzuK/9VXokku1R4TfKoV7Qj+4cNifWm5aBvscf49ERnC6NGiS++JE8CqVUpHY7kcHR3x/PPPY9u2bTh06BAmT56M2bNnw8vLCwMHDlQ6PLrDqWun8Hny5wCA//X+X7VLeRmKLsHPMf8E/0DWAZy8dhINbBroyvEramDTAP1b9gdgfmX62vJ8YzbYAyqU6JvZHPzalOdrmX2Cn3nvBnuHDgHHjwNqNfDIfU5L795ASIjIM774Qu5I6wcm+FQrfn5A48aARgPs3y/ffiuO4BMRyc3ZGXjpJbH9/vtiNIEMq1WrVpg7dy4uXLiA5dqhGzIpr218DWWaMjzc4uH7dvo2BG2JviWM4GvL8we0HABntXOVz6lYpi+Z0S8hJebfA+ZZop9zMwdJ55IA1DLB/7fR3u4Lu1GmMcBSVQZ2vw762vL8hx8GXF3vvS+VCpgyRWx//DFQUiJXlPUHE3yqNUPMw+cIPhEZ2vjxgKOjaPKzfr3S0dQf1tbWGDRoENauXat0KFTB9vTtWHVsFaxUVpjbc64iMWhH8C/duITCUhnLAo1MI2l0Cf7Q9lW0B/9XfHA87G3skZabpkuIzMGxK8cAKJDg/zuCf6XoCkrKzCPLW396PcqlcrT1bIvm7s1r/Lq2nm3hqnZF4a1CpGanGjBC+UmSdM8O+pJ0u3v+/crztYYOFTlBVhbw/fdyRVp/MMGnWjPEPHyO4BORoTVqBIwZI7ZnzeIovqnbsmULBgwYAD8/P6hUKqxZs6bS4yNGjIBKpap0i4+PVyZYMyNJEib/JRpTjAwbiXZe7RSJw83eTdeMzpyXytuZsRMZ+RlwtnPGwy0ervZ5jnaO6NuiLwDzKdPPLsjG0kOiO2mPZj2Memx3e3eordUAgMyCTKMeu67qUp4PAFYqK8T4xwAQH76Zk4s3LuJq0VVYq6zR3qv9XY8nJwNnz4pG2v3712yftrbAhAlie/58UTlMNccEn2pNO4IvZ4LPEXwiMoZJk8QcwB07gC1blI6G7qWwsBChoaH47LPPqn1OfHw8MjMzdTdOBaiZlUdXYvfF3XC0dcSMHjMUjcUSyvSXHxbvu8FtBt93zXNtmf5Px34yizL9udvn4mbZTUQ1jkKf5n2MemyVSnW70Z4ZzMMvKSvB+lOiPKy2CT5gvvPwtfPv23q2rfL9rx29HzhQVNHV1IsvAi4uYu7+b7/JEWn9wQSfai08XMyPOX8eyM6WZ58cwSciY/D1FcvmAWIUn0xX3759MXPmTAwePLja56jVavj4+Ohubm5uRozQPJWUlWDaxmkAgFdiX4Gvs6+i8WjL9M11qbwyTRlWHl0JoOru+Xfq17If7KztcPLaSRy5csTQ4eklqyALC5MXAgDeeeAdqBRY5sic5uEnnU/CjdIb8HHyQWTjyFq/Xpvg78jYIXdoBnWv8nyNBvjxR7E9tPrZK1VycRENcgFg3jx9Iqx/mOBTrbm4AG3aiG05RvGLioBr18Q2R/CJyNBeeQWwtgY2bAD27lU6GtLH5s2b4eXlhVatWmHMmDG4pr2YVKGkpAT5+fmVbvXR53s/x9mcs/B18sXkmMlKh2P2S+VtStuEy4WX0ci+EeIC4+77fBe1C3o37w3A9Mv0taP30Y2jER+szPQXXSd9M1gq75fjojx/QMsBdVqRIqpxFKxV1sjIz0BGXobc4RnMvRrs7dghBvFcXIC6zKB6+WVRrr91q7y9vywdE3yqEznn4V/893e2oyPQsKH++yMiupdmzYBhw8T2++8rGgrpIT4+Ht9++y02btyIOXPmICkpCX379kV5eXmVz09MTISrq6vu5l8PS8ZybubgvS3vAQDee/A9ONrVol7WQHQl+ma6VJ62ud7jbR+HrbVtjV5TsZu+qcq8kXl79L6HMqP3AMymRF+SJKw9KRqJ1qU8HxA9Gjr6dARgXmX691oiT1ueP3iwmB5XW35+t6/XHMWvOSb4VCdyzsOvOP9eoesHEdUzr70mft+sWQMcMe0qWarG0KFDMXDgQHTo0AGDBg3CunXrsHfvXmzevLnK50+bNg15eXm6W0aG+YyQyWXW1lnIKc5Be6/2GNFxhNLhADDvEv2SshJdkn6v7vl3GthqIGysbHDo8iGcvHbSUOHpZc72OSguK0bnJp2NPve+InMZwT+QdQAX8i/AwdYBcUH3r+Sojm4evpk02rt+8zrO550HAN2HE1plZcBKMXul1uX5FWmXzFu1Cjhtnp8DGh0TfKqTigm+vp0tOf+eiIytTRvg0UfFdmKisrGQPIKCguDh4YHT1fwFqFar4eLiUulWn5zNOYtP9nwCAJjXax6srawVjkjQJvgXb1xE0a0ihaOpnT/P/Im8kjz4OfuhW0C3Gr/Ozd4NDwU+BMA0y/Qzb2Tiy31fAgBm9Jih2Og9YD5z8LXl+X2a97lvo8V7iW1qXo32UrJSAACBDQPh2qDyAvdJScDly2IFm7i6f+aBdu2Ahx8WK98sWKBHsPUIE3yqkw4dgAYNgNxc/T9NYwd9IlLC66+L++XLxRI+ZN4uXLiAa9euwddX2aZxpur1ja+jtLwUvYJ6KToie6dGDo3g1kA0RzybY17/EbXd859s92St51ybcpn+7G2zUVxWjC7+XdArqJeisehK9E18BL+uy+PdqYt/FwDAweyDKCgt0DsuQ9OV51fRYE9bnj9kiJhHr4+pU8X94sXAlSv67as+YIJPdWJrC3TqJLb1LdPnCD4RKaFTJ9H0R6MB5sxROhq6U0FBAVJSUpCSkgIASEtLQ0pKCtLT01FQUICpU6di165dOHfuHDZu3IhHHnkEwcHB6NPHdJJXU7H7wm78cOQHqKDCvF7zFB2RrYo5lukXlhZi7Qkx57om3fPvNKj1IFiprLAvcx/SctLkDq/OLuZf1I3eK9U5vyJdiX7+RZNdVvB87nkczD4IK5UV+rXsp9e+mrg0QVPXptBIGuy+YPpd5aprsFdaCvz872dX+pTnaz3wABARARQXA/dYOZX+xQSf6kyuefgcwScipbzxhrhfsuR2w08yDcnJyQgLC0NYmPjDcdKkSQgLC8Pbb78Na2trpKamYuDAgWjZsiVGjhyJ8PBwbN26Feq6dHKyYJIkYcoGMYl1eMfhCPUJVTiiu5ljJ/1fT/6KoltFaO7WHBF+EbV+vaejJ7oHdAcArDq2Su7w6mz2ttkoKS9BrH8segb1VDoc3Qj+zbKbyCvJUziaqmk/6In1j4WHg4fe+9PNwzeDMv3qEvy//wZycgAfH6B7d/2Po1LdHsX/9FOxAhdVjwk+1Zk2wdd32QqO4BORUrp2FX98lJYC//uf0tFQRT169IAkSXfdlixZAnt7e/z555+4fPkySktLce7cOSxatAje3t5Kh21y1hxfg23p22BvY4/3HnxP6XCqpOukb0YJvrZ7/tD2Q+s8ym1qZfoX8y9i0f5FAJSfe69lb2sPd3t3AKbbSV+u8nwtc0nwi24V4fjV4wDuLtHXluc//rhYllYOjz4KBAaKpbWXLJFnn5aKCT7VmTbBT0kBSkrqvh/tCD4TfCJSgnYu/pdfAlevKhsLkZxuld/Cq3+/CgCYHDMZTVxMs1ROV6J/3TxK9HOLc7H+9HoAteuef6fBbQYDAHZe2IkL+RdkiU0fidsSUVpeim5Nu+maAJoCU56Hn1uci6TzSQDE6ghy0M7D33VhF8o1VS/7aQoOZR+CRtLAy9ELvk63e5/cvClWqAHkKc/XsrEBJk0S2wsWANWsiEpggk96CAoSnTFLS4HU1Lrto6gIuH5dbLNEn4iU0Ls3EB4ufh999JHS0RDJ58t9X+LU9VPwcvTCK7GvKB1OtcytRH/1sdUoLS9Fe6/2aO/Vvs778XP20yVzq4+tliu8OsnIy8BX+78CYDqj91oV5+GbmvWn1qNMU4Y2Hm3QolELWfbZwbsDnOyckF+SjyNXTHcd14rl+RXfL+vXAzduAE2bAp07y3vM554D3N2BM2eA1cr+lzFpTPCpzlQq/efha8vznZwAV9d7P5eIyBBUqtuj+J98AuSZ5jRPolrJK87DjKQZAETC5qx2Vjii6mkT/Iz8DNy8dVPhaO5P2z1/aDv9hyeHtBkCQPkyfe3offeA7ujRrIeisdxJm+Cb4lJ5cpfnA4CNlQ06NxGZ8fZ00y3T13XQv2P+/Q8/iPsnngCsZM40HR2BhASxPW+eWDqP7sYEn/Si7zx8bYLfpIn4I5uISAmDBgFt2ojkfuFCpaMh0t/sbbNxtegqWnu0xgudXlA6nHvycPCAq1p8yp+Wazod5atyufAyNqZtBKBfeb6WNsHfmr4V2QXZeu+vLjLyMvD/9v8/AKY3eg8AjV3+HcE3sRL90vJS3VSNR1rLl+AD5jEPXzeCX2H+fUEB8OuvYlvO8vyKxo0TS3Xv2QNs3WqYY5g7JvikF31H8Dn/nohMgZUVMG2a2F6wgB16ybyl56Xjg10fAADm9pwLGysbhSO6N5VKZTZL5a08shIaSYNIv0g0d2+u9/4CGgYgwi8CGkmDNcfX6B9gHby/9X3c0txCj2Y9TG70HjDdOfhJ55KQX5IPb0dvRDWOknXf2qkbOzJ2yLpfuey7tA/7MvcBACL9InXf//VXMQc/OPj2ctpy8/IChg8X2/PmGeYY5o4JPulFm+CfOAHk5tb+9RVH8ImIlPTUU0CzZsCVK8DXXysdDVHdvfnPmygpL0GPZj3Qv2V/pcOpEXOZh7/iiGgP/lT7p2Tbp5Jl+udzz+PrA+IX3oweM4x+/Jow1RJ9bXn+gJYDYKWSN6Xq3KQzrFRWSMtNQ+aNTFn3ra9yTTlG/zYaGkmDpzs8jUC3QN1j2u75Q4catjJ38mSx/3XrgKNHDXccc8UEn/Ti4SGa7QFAcnLtX88RfCIyFTY2wKui4TjmzhUNRInMzf7M/fgu9TsAwPxe802u3Lo65rBUXnpeOralb4MKKjzR7gnZ9qtN8P9J+wfXiq7Jtt+a0I7ePxT4ELoHyLBguQHoSvRNqMmeJElYe2ItAPnL8wHARe2CDl4dAJhemf4XyV8g+VIyXNWu+F/v2+vL5uYCf/whtp980rAxtGghptYBXOK2KqZds0VmISoKOHtWzMPv2bN2r+UIPhGZkhEjgHffFb+bvv8eeP55pSMiqjlJkjDlrykAgGEdhiHcL1zhiGrOHJbK++Gw6B7WPaC7LumUQ4tGLRDiHYLU7FSsPbEWz4U9J9u+7+V87nl8k/INAOCdB94xyjHrQluin12YjTJNmUlMOUnJSkFGfgYcbB0QFxhnkGPE+sfiYPZBbE/fjsfaPmaQY9RW5o1MvP6P6EqbGJcIHycf3WNr1ogPxtu1A9rXfXGJGps6VXTS//570dDP0bHu+/L2Fh8aWArl/4eQ2YuOFiU5dZmHzxF8IjIlDRoAU6aI8r/Zs8U8P2trpaMiqpnfT/2OTec2QW2txqyHZikdTq2YQ4m+tjxfjuZ6dxrSZghSs1Px87GfjZbgz9o6C2WaMsQFxqFbQDejHLMuvBy9YGNlgzJNGbIKstDERflRIW15fu/mvWFva2+QY3Tx74LPkz/HjgumMw9/8l+TkV+Sj0i/SIwKH1XpsYrl+cYQEwPExgLbtwPx8frty8pK7EfuZf2UwhJ90lvFTvq1Xa6CI/hEZGpGjRLr7J46BaxcqXQ0RPcnSRJ+Pvoz/rvuvwCAl6NfRkDDAIWjqh3tGuLpeekoKStROJq7nbx2Evsz98NaZW2Q0VTtPjec3YC8YsOv1ZmWk4bFKYsBmO7cey0rlRV8nXwBmM48fEMsj3en2Kaik/7+zP0ouqV859cNZzZg+eHlsFJZ4Yv+X8Da6van31euAH//LbYNXZ5f0bx5QMeOQMuWdb95eQEaDZCYaLy4DY0JPuktLEyMcGVn307Ya6KoCLh+XWxzBJ+ITIWTEzBhgtiePVvRUIjua+v5rejyTRc8tvIxXLxxEc0aNsO0btOUDqvWPB084WznDAkSzuacVTqcu6w4LIYnezXvBQ8HD9n339azLVp7tEZpeSnWnVwn+/7vpB297xXUS5dImjJTmoefnpeOlKwUWKms0K9FP4MdJ8A1AH7OfijTlGHvxb0GO05NFJcVY+zvYwEA46PGo5Nv5Rb5q1YB5eWic74xS91jYoADB0Sz77rekpLEvtauBY4fN17shsQEn/Rmbw+EhIjt3btr/jrthwFOToCLi/xxERHVVUKCKNk7eBDINK0GxkQAgKNXjmLg8oHovqQ7dl3YBQdbB7zV/S0cHH0QDRs0VDq8Wqu4VJ6plelLkoTlh5cDkLd7/p2M1U3/bM5Z/N/B/wMAvNPjHYMeSy6mtFSetrleF/8u8HT0NNhxVCoVYv3Fhy9KN9qbvW02Tl8/DT9nP7z74Lt3PW7s8nw5tW4NDBwoti2lYR8TfJJFdLS4r808/Irz782kyS8R1RPu7kCbNmJ73z5lYyGq6EL+BYz8ZSQ6LOyAX0/+CmuVNUaHj8bp8afx7oPvwkVtvp+Ya8v0TS3BT81OxfGrx6G2VmNQ60EGO462TH/96fUoKC0w2HFmbRGj972b99att27qtEvlmcIIvjHK87W0Cf6ODOXm4Z+8dhKJ20T9+kfxH931O+bSpduj4E/It7iEUU2dKu6//RbIylI2FjkwwSdZaOfh1ybB5/x7IjJl4f82IGeCT6YgtzgX0/6ehhaftMA3Kd9AI2nwaJtHcWTsESzsvxC+zr5Kh6i3YDfT7KSvHb3v17KfQT9ACfUORZBbEIrLirH+1HqDHOPM9TO60XtTn3tfkTbBv1Sg7Bz8vOI8bD63GYBxEnztBzA7MnZAI2kMfrw7SZKEsb+NRWl5KfoG99VVmVT000+iB1dMDBBgXq0/dGJjRYO90lLgk0+UjkZ/TPBJFtoEPzlZzMGpCXbQJyJTxgSfaqq4rNhgTbBKykrwwc4P0Pzj5pi9fTaKy4rRtWlX7Hh+B35+4me08mhlkOMqwRRL9CVJ0s2/H9rOsPXHKpXK4GX6s7bOQrlUjvjgeHRuYj4tw01lDv760+tRpilDa4/WuooTQ+ro0xEOtg7IKc7B8avGnyC+4vAKbEzbiAY2DfDpw59CVUXJrTmX52upVLdH8RcuBAoMV0BjFFwmj2TRurWYS19QABw9CnTocP/XaBN8juATkSligk819fup3/H4ysfRwl2sZx7qHYoQ7xCEeIegqWvTKv8ovh+NpMHyQ8vx5qY3cS73HADRiG123Gz0b9m/Tvs0daZYor/rwi6czzsPJzsn9GtpuIZqWo+1fQzzdszDb6d+w81bN2Vdgu309dP49uC3AEx73fuqaOfgb0zbCNv3bBWLo1wjRrGMMXoPALbWtohqHIXN5zZje/p2tPVsa5TjAqJqaOKfEwEAb3Z7E0FuQXc959w5YOdOkSA//rjRQjOIRx4BgoOB06eBr78GXn5Z6Yjqjgk+ycLaGoiMBDZtEmX6NUnwtSX6HMEnIlPUsaNotHfpkmi052v+FdBkICeunoBG0uDEtRM4ce0EVh69vb6iq9pVl+xrk//2Xu3haOdY7f7+OvMXXv37VaRkpQAQyc27Pd7F8I7DYWNluX+6aUfwz+edR2l5Keys7RSO6Hb3/EdaPQIHWweDHy/SLxL+Lv7IyM/AX2f+wiOt5UskZ26ZiXKpHH2D+yK6SbRs+zWGMJ8weDh44GrRVZRpyhSNpYFNA/wn5D9GO16sfyw2n9uMHRd24MXwF4123Dc2voHswmy09miNKV2mVPmcH38U9z16mP810toamDwZGDMG+OAD0WzXxkx/3Zpp2GSKoqJuJ/gjR97/+RzBJyJT5ugoqpOOHhWj+P37Kx0RmarXur6G58KeQ2p2Kg5mHUTq5VSkZqfi2JVjyCvJw9b0rdiavlX3fBVUaO7e/K7R/pybOXht42v4+6xYUNpF7YJpXafhpeiXjJJcKs3b0RtOdk4oKC1AWk6a4tMPyjXl+PGoyGAM2T2/IpVKhUfbPIqPdn+En4/9LFuCf+raKXyX+h0A8+mcX5GbvRsyJmbg+s3rSocCF7ULnOycjHY87Tz87enG66S/5+IeLExeCABY2G8h1DbqKp/3ww/i/sknjRWZYQ0fDrz9NnD+PLByJfCUcf7by07RBD8xMRGrVq3C8ePHYW9vjy5dumDOnDlo1ar6X+hHjhzB22+/jX379uH8+fP44IMPMEG7YHEFFy9exKuvvor169ejqKgIwcHBWLx4MSIiIgz4E9Vv2nn4NV0qjyP4RGTqIiKY4NP9qVQq+Dj5wMfJB72b99Z9v7S8FMevHkdqdqrudjD7ILIKsnD6+mmcvn4aq46tumt/dtZ2SIhMwBvd3kAjh0bG/FEUpV0qLyUrBaevn1Y8wU86n4Ssgiy4NXBDr+a9jHbcx9o+ho92f4S1J9bKVskwc+tMaCQN+rXoh6jGUTJEaXwNbBroSvXrk5gmMQBE88nLhZfh5ehl0OOVacowet1oSJDwbOiz6NGsR5XPO3kS2L9fjHwPubv3nlmytwfGjQOmTwfmzRN9BcxxNpSiCX5SUhISEhIQGRmJsrIyvP766+jduzeOHj0KR8eqS9eKiooQFBSExx9/HBMnTqzyOTk5OYiNjcWDDz6I9evXw9PTE6dOnYKbm5shf5x6T5vgHz4MFBaK0a/qFBYCOTlimyP4RGSqwsPFsjmch091YWdtpxudr+hy4WUcyj6kS/hTs1Nx5MoRlJaXYliHYZj50Ew0a9hMmaAVVjHBV9ryQ6J7/mNtHzPqdIEu/l3g4+SDrIIs/JP2D+KD4/Xa38lrJ/F96vcAzHP0vr5zs3dDO892OHLlCHZk7MCg1oNw6xZQUmKY432+ZxEOnD+Jhg2a4J2Y+dU2nFu6VNz36gV4eBgmFiWMHQvMng0cOAD88w8QF6d0RLWnaIL/xx9/VPp6yZIl8PLywr59+9C9e/cqXxMZGYnIyEgAwGuvvVblc+bMmQN/f38sXrxY973AwECZoqbqNGkC+PmJ+aoHDgBdu1b/XO3ovbMz4OpqnPiIiGqLjfbIELwcvRAXFIe4oNt/Od4qv4XismI4q50VjEx5prJUXml5qa6T/dD2xm0PbqWywuDWg7EweSF+OvqT3gn+e1veg0bSoH/L/ojwYyWrOYr1j9Ul+MFlgxATY8hO72MBjEUugKDp93+2pZTna3l4AM8/D3z2GTB/vnkm+Ca1TF5eXh4AwN3dXa/9rF27FhEREXj88cfh5eWFsLAwfPXVV9U+v6SkBPn5+ZVuVDfaUfw9e+79PM6/JyJzULHRXlaW0tGQJbO1tq33yT2gXCf9gtIC7LqwC18mf4mE3xLQ9ZuuyCnOgY+TDx4IeMCosQCiagAA1hxfU6umcsVlxdifuR+LDyzGxD8mIu7bOCw7tAyA+XXOp9t08/AztuPnn01nGbegIODRR5WOQn6TJolr/x9/AIcOKR1N7ZlMkz2NRoMJEyYgNjYW7du312tfZ8+excKFCzFp0iS8/vrr2Lt3L1566SXY2dlh+PDhdz0/MTERM2bM0OuYJERFAWvW3H8ePuffE5E5uLPRXj/Dr5JFVK9pO+kbKsHXSBqk5aRV6omQmp2KMzlnqnz+qE6jYG1lbZBY7qV7QHc0sm+EazevYcv5LXgo8KFKj0uShAv5F+76OU5eO4lyqfyu/T3X8TmE+4UbK3ySWWzTWABA8qVkOGwvB2CNDz8EXpSxqf6fp//Eoz8MhpXKGtuf344Qn5D7vqZBA5EIW5qgINFXYOVKMYr/f/+ndES1YzIJfkJCAg4fPoxt27bpvS+NRoOIiAi8//77AICwsDAcPnwYX3zxRZUJ/rRp0zBp0iTd1/n5+fBn5lknHMEnIksTHi4S/ORkJvhEhqZN8M/lnsOt8luwta77muf5JfmVGhymZqfi0OVDKCitevjT18m30soGoT6haOfZrs7H14eNlQ0GtR6Erw98jaWpS+Fs56xL4rW3nOKcKl/byL4RQn1CEeJ1e3nGTr6djPwTkJyauzWHl6MXLt+4ip27JADAAw8ADjItrnHz1k1M3jQGsLuJiTGT0Tno/sm9pZs6VST4y5YBs2aZV85iEgn+uHHjsG7dOmzZsgVNZDh7vr6+aNu2baXvtWnTBj///HOVz1er1VCrq17+gWonIkJ0mzx3Drh8GfCqptEnR/CJyFyEhwPffcd5+ETG4OvkCwdbBxTdKoL3fG9Yqeo2PKiRNNUmwGprNdp5tRPJr5dI5Dt4dYCno6c+ocvusbaP4esDX+OblG/wTco3dz1uY2WD1h6t71pu0dfJFypzbP1N1VKpVIj1j8XqpFMovGEDJydAz4LnSmZtnYW03DQ0cWnCRoz/iowUH6IkJQEffSS66psLRRN8SZIwfvx4rF69Gps3b5atEV5sbCxOnDhR6XsnT55EQECALPun6rm6inLWY8eAvXurH+3iCD4RmQvt6qpM8IkMT6VS4aHAh7Du5LpqE/TaaOLSpFLyG+IdgpaNWsLGyiTGuO7pocCHEOQWhLM5Z+Hl6HXXz9HGo02165OT5eni3wWrM8SHUNHRgI1Mb+FjV45h7va5AIBP+n4CJzsneXZsAaZOFQn+l18Cb75pPo3BFf3tlpCQgGXLluGXX36Bs7Mzsv7tYOTq6gp7e3sAwLPPPovGjRsjMTERAFBaWoqjR4/qti9evIiUlBQ4OTkhOFiUdU2cOBFdunTB+++/jyeeeAJ79uzBokWLsGjRIgV+yvonKkok+Lt3V5/gcwSfiMzFnY32fHyUjojIsq15cg1OXT8FSZL02o+3kzfc7fVr3KwkO2s7HB5zGDdKbxh87XMyfbH+scC/CX5MjARA/yoNSZIw9vexuKW5hQEtB+CRVo/ovU9L0rcv0LatmKa3aJFI+M2BStL3t6c+B6+mfGjx4sUYMWIEAKBHjx5o1qwZlixZAgA4d+5clSP9DzzwADZv3qz7et26dZg2bRpOnTqFwMBATJo0CS/WsBNFfn4+XF1dkZeXBxcXl1r9TAR8/jmQkAD06SO6T1bFzQ3IzQWOHBH/cYiITFm7duICv26dcvPweW2SF88nEZmTkrIS2HtnQLoejK9+uIAXntC/DPa7g9/h2TXPwt7GHkcTjqJZw2b6B2phFi8Wy+Y1bgycPQvY2Rn2eHJcmxQv0b+fikk7ADRr1qxGr+vfvz/69+9f19BID9HR4n7PHkCSxJz8igoKRHIPsESfiMyDttEeO+kTEZES8q6rIV0X1colPkkAhum1v+s3r2PyX5MBANMfmM7kvhpPPw288QZw8SKwfDlQRb92k2OBCxuQ0jp0ANRqICcHOFPFqjPa8nwXF3EjIjJ14f+uLpWcrGwcRERUP+3c+e+G52Gk5G7We3/T/p6GK0VX0M6zHSbFTLr/C+optRp4+WWxPX++GLw0dUzwSXZ2dkBYmNjevfvux7UJPkfvichcaBN8NtojIiIl7Njx74b/DmzP2F6nfdwouYGdGTuxYOcCLNovepMt7LdQr+Uo64P//hdwcgIOH65++rEpMf0WomSWoqKAXbtEmf6wOyqItB302WCPiMxFx45iuhEb7RERkRIqJvjHrh7D9ZvXq20iqZE0OJtzFqnZqUjNTsXB7INIzU7F2ZyzlZ73fMfn0S2gm4EjN38NGwKjRgELFojl8vr2VTqie2OCTwZRcR7+nTiCT0TmxskJaNOG8/CJiMj4SkvF8tMAEBiSiTQAOzJ2oH/L/si5mYNDlw/pkvnU7FQcunwIRbeKqtyXn7MfQrxDENMkBpNjJhvvhzBzEyYAH38MbNok/g7QVvaZIib4ZBBRUeL+wAHxS6lix0mO4BOROWKjPSIiUsKBA0BJCeDhATzQqTHSDgIT/5yIsb+NRUZ+RpWvaWDTAO082yHEOwQh3iEI9Q5FB+8O8HDwMHL0lsHfHxg6FPj+ezGKv2KF0hFVjwk+GUTz5oC7O3D9OpCaCkRE3H6MI/hEZI7Cw4HvvuM8fCIiMi5teX6XLkCPZg9gycHFOH39tO7xANcAXSKvvQW7B8PGiqmenKZMEQn+ypVAYiJQxcrtJoH/6mQQKpUYxf/jD1GmXzHB5wg+EZkjNtojIiIlbP+3p16XLsDTHZ7GtZvXoLZWI9QnFO292qNhg4aKxldfhIYCvXsDf/0FfPCBKNk3ReyiTwajLdO/cx4+R/CJyBxpG+1dvCga7RERERmaJFVO8G2tbTEpZhISohLQtWlXJvdGNnWquP/6a+DaNWVjqQ4TfDIYbYJfcam8ggIgN1dscwSfiMyJkxPQurXY5ig+EREZw/nz4kNlG5vKFbGkjLg48YF/URGwcKHS0VSNCT4ZjDbBP34cyMsT29rRexcXwNlZmbiIiOpK+8cVE3wiIjIG7fz7Tp0Ae3tlYyFRyacdxf/kE6C4WNl4qsIEnwzG0/N284nkZHHP+fdEZM44D994tmzZggEDBsDPzw8qlQpr1qyp9LgkSXj77bfh6+sLe3t79OzZE6dOnVImWCIiA6nYYI9Mw+OPA02bApcvA99+q3Q0d2OCTwZ15zx8zr8nInPGBN94CgsLERoais8++6zKx+fOnYuPP/4YX3zxBXbv3g1HR0f06dMHxaY4nEJEVEdM8E2PrS0wYYLY/t//AI1G0XDuwgSfDOrOefgcwScic8ZGe8bTt29fzJw5E4MHD77rMUmS8OGHH+LNN9/EI488gpCQEHz77be4dOnSXSP9RETmqqAAOHhQbMfEKBsLVfbCC4CrK3DyJPDrr0pHUxkTfDKoigm+JHEEn4jMGxvtmYa0tDRkZWWhZ8+euu+5uroiOjoaO3furPI1JSUlyM/Pr3QjIjJle/aI0eGmTfm3s6lxdgbGjBHb8+YpG8udmOCTQXXqBFhbi5Guixc5gk9E5o9l+srL+rd8wtvbu9L3vb29dY/dKTExEa6urrqbPy9ERGTiWJ5v2l56CbCzE8sYVvPZsiKY4JNBOTgAHTqI7T17OIJPROaPCb55mjZtGvLy8nS3DO0nzkREJooJvmnz9QWeeUZsm9IoPhN8MriKZfocwScic8el8pTn4+MDAMjOzq70/ezsbN1jd1Kr1XBxcal0IyIyVRrN7VFhJvima8oUcb9mDXD2rKKh6DDBJ4PTJvgbNwJ5eWKbI/hEZK4qNtq7I78kIwkMDISPjw82btyo+15+fj52796NGHaiIiILcPw4kJsrqmFDQpSOhqrTpg0wYwawYcPt5cGVZqN0AGT5oqPFvXa0y9VVNKYgIjJH2kZ7x46J32sPP6x0RJapoKAAp0+f1n2dlpaGlJQUuLu7o2nTppgwYQJmzpyJFi1aIDAwEG+99Rb8/PwwaNAg5YImIpKJtjw/Kkosy0am6+23lY6gMib4ZHBt2gCOjkBhofiao/dEZO7Cw0WCn5zMBN9QkpOT8eCDD+q+njRpEgBg+PDhWLJkCV555RUUFhZi1KhRyM3NRdeuXfHHH3+gQYMGSoVMRCQbzr+numKCTwZnbS3mrCYlia85/56IzF14OPD995yHb0g9evSAJEnVPq5SqfDuu+/i3XffNWJURETGwQSf6opz8MkotPPwAY7gE5H5Yyd9IiIylKtXgRMnxHbnzsrGQuaHCT4ZhXYePsARfCIyf2FhbLRHRESGsWuXuG/dGmjUSNlYyPwwwSej4Ag+EVkSbaM9gKP4REQkL5bnkz6Y4JNRNGkC+PqK7YAAZWMhIpIDy/SJiMgQmOCTPpjgk1GoVMDHHwNjxwLduysdDRGR/pjgExGR3G7dAvbsEdtM8Kku2EWfjOaxx8SNiMgSaBP85GRl4yAiIstx8CBw8ybg5ga0aqV0NGSOOIJPRERUB2y0R0REctOW58fEAFbM1KgOOIJPRERUB05OYnTl+HFRpv/ww0pHRObs/HlgyxZR6WZvr0wMZWXAjz8CmZnKHJ9Ml7MzMHQo4OKidCSWj/PvSV9M8ImIiOooIoIJPunvu+9Ej5qCAmDOHGD5cqBDB+PGcO4cMGzY7eSC6E6zZwPLlnFddkNjgk/6YoJPRERUR+HhwPffs9Ee1U1+vkjsly4VX9vYAEeOAJGRwPz5QEKCmAZiaD/8APz3v0BenhihHTDAOMcl87F1K5CWBnTtCsyYAbz2GmBtrXRUlicjQ9ysrcXvAaK6YIJPRERUR+ykT3W1axfw9NMiabK2BqZPB154Qdx+/x0YPx746y/gm28ADw/DxFBQII6zZIn4OiZGfNgQGGiY45H5yssDxowR1SVvvgls2CAqT/z9lY7MsuzcKe5DQ8U0MKK6YOsGIiKiOtI22rtwgY32qGbKy4FZs8RIaFoa0KyZmHv/1luAry+wbh3w0UeAnR3w669ASAjw99/yx5GcDHTqJJJ7Kytx/C1bmNxT1VxdxYc///d/IvFMShJJ6KpVSkdmWVieT3Jggk9ERFRH2kZ7AEfx6f4uXAB69hQjoOXlomlZSkrlP+ZVKuCll8Q62G3aiIZ3vXoBr7wClJbqH4NGA8ybJ4556hTQpAmwaRPw7rtiigBRdVQq4NlngQMHRP+RnBxgyBAxvaOoSOnoLAMTfJIDE3wiIiI9sEyfamLVKjEav3kz4OgoRs6XLRMjo1UJDRWj7KNHi68rJuV1lZkJ9OkjPiy4dQt49FGx5nb37nXfJ9U/wcHA9u1iHr5KBSxaJH4PpqQoHZl5KyoSH54ATPBJP0zwiYiI9MAEn+6lqEiMcA4ZIkY8IyLEH/HDh9+/kZ2DA7BwofhwwN1dvMfCwsSHA5JUuzgqlvs7OABffQX89JPYL1Ft2dkBiYliLr6vr1hNJDoa+PDD2r83SUhOFktV+vkBTZsqHQ2ZMyb4REREeoiIEPdM8OlOBw+KD4AWLRLJ/KuvipHPFi1qt5/Bg8W+evQACguB554TDfpyc+//2ps3RSO9gQOBq1eBjh3Fe/WFF9gpn/QXFwekpor3V2kpMHEi0K8fe5LURcXyfP7fJH0wwSciItIDG+3RnSRJNMqLihIjm76+YqRz9mwx8lkXTZqI0ff33xdd91esEMn69u3Vv+bIERHDp5+KrydOFN37W7euWwxEVfHwANasAT7/HGjQAFi/Xkwx+fNPpSMzL5x/T3Jhgk9ERKQHNtqjii5fFiOYEyaIEc0BA8QIZ1yc/vu2tgamTRNJfVAQcP68mD//7ruitFdLkkRpf0QEcPgw4OUlkq4FCwC1Wv84iO6kUoll9PbuBdq3Fx92xscDkycDJSVKR2f6JIkJPsmHCT4REZGeOA+fADFiGRIikmm1Woyc//KL/OvYR0eLefzPPCO64k+fDjz4IJCeLsrwBw0Cxo4FiotFkpWaKu6JDK19e7ECxPjx4usFC4DOnUUlC1Xv1Cng2jVRAREWpnQ0ZO6Y4BMREemJCX79VlIiRirj48XIZbt2omFWQoLh5tK6uADffSduzs7Atm3iw4WQEGDtWjEV4IMPgN9+A7y9DRMDUVXs7YGPPxaNHT08RHf98HDgm2+Ujsx0aUfvIyPrPo2HSIsJPhERkZ6Y4NdvU6aIkUpAJPXaMmVjeOYZMZofHQ3k5Yml8Fq3BnbvFtMErPiXHimkf39RPdKzp1hNYuRIYPVqpaMyTSzPJznx1z4REZGeKjbau3xZ6WjI2KZNE0n1L7+Isnx7e+Mev3lzYOtWYM4c4I03RPVAx47GjYGoKr6+YuqKtmR/1CggK0vZmEwRE3ySExN8IiIiPTk7s9FefebnJ5rZDRyoXAy2tsArrwAzZwKOjsrFQXQnKytg/nzRWf/qVbFEoyQpHZXpyMkRK14AQEyMsrGQZWCCT0REJAOW6ddv1tZKR0BkuuzsgO+/F80nf/sN+OorpSMyHbt2ifsWLQBPT2VjIcvABJ+IiEgG2gQ/OVnZOIiITFH79sD774vtSZOA06eVjcdUsDyf5MYEn4iISAYcwSciurcJE8SSjoWFwLPPAmVlSkekPCb4JDcm+ERERDJgoz0ionuzsgKWLBHLPO7cCcydq3REyiorEyteAEzwST5M8ImIiGTg7Ay0bCm2OYpPRFS1pk3FahMAMH06sH+/svEo6dAhUc3g4gK0bat0NGQpFE3wExMTERkZCWdnZ3h5eWHQoEE4ceLEPV9z5MgRDBkyBM2aNYNKpcKHH354z+fPnj0bKpUKEyZMkC9wIiKiKkREiHsm+ERE1XvmGWDIEDGC/cwzwM2bSkekDG15fkyMqG4gkoOib6WkpCQkJCRg165d2LBhA27duoXevXujsLCw2tcUFRUhKCgIs2fPho+Pzz33v3fvXnz55ZcICQmRO3QiIqK7cB4+EdH9qVTAF18APj7AsWPA668rHZEyOP+eDEHRBP+PP/7AiBEj0K5dO4SGhmLJkiVIT0/Hvnv8ZRQZGYl58+Zh6NChUKvV1T6voKAAw4YNw1dffQU3NzdDhE9ERFQJO+kTEdWMhwfwzTdi+8MPgY0bFQ1HEUzwyRBMqhgkLy8PAODu7q73vhISEtCvXz/07Nnzvs8tKSlBfn5+pRsREVFtsdEeEVHN9e0LjB4ttkeMAHJzlYzGuC5dAs6dE6X5UVFKR0OWxGQSfI1GgwkTJiA2Nhbt27fXa18rVqzA/v37kZiYWKPnJyYmwtXVVXfz9/fX6/hERFQ/sdEeEVHtzJ8PBAeLD0bHjVM6GuPZuVPcd+ggmuwRycVkEvyEhAQcPnwYK1as0Gs/GRkZePnll7F06VI0aNCgRq+ZNm0a8vLydLeMjAy9YiAiovqL8/CJiGrO0RH47jsxkr10KfDjj0pHZBwszydDMYkEf9y4cVi3bh02bdqEJk2a6LWvffv24fLly+jUqRNsbGxgY2ODpKQkfPzxx7CxsUF5efldr1Gr1XBxcal0IyIiqgsm+EREtdO5M/DGG2J79Gjg4kVl4zEGJvhkKIom+JIkYdy4cVi9ejX++ecfBAYG6r3PuLg4HDp0CCkpKbpbREQEhg0bhpSUFFhbW8sQORERUdW4VB4RUe299Zb4gDQnB3j+eUCSlI7IcIqLb18jmOCT3GyUPHhCQgKWLVuGX375Bc7OzsjKygIAuLq6wt7eHgDw7LPPonHjxrr59KWlpTh69Khu++LFi0hJSYGTkxOCg4Ph7Ox81xx+R0dHNGrUSO+5/URERPejbbSXkSEa7Xl5KR0REZHps7UVpfqdOgF//QV8/jmQkKB0VIaxbx9w6xbg7Q3IML5JVImiI/gLFy5EXl4eevToAV9fX93thx9+0D0nPT0dmZmZuq8vXbqEsLAwhIWFITMzE/Pnz0dYWBheeOEFJX4EIiKiSthoj4iobtq0AebMEdtTpwInTigbj6FULM9XqZSNhSyPoiP4Ug1qbzZv3lzp62bNmtXodffaBxERkSGFh4s/TPftE8tAERFRzYwbB/z6K/D338B//gNs3y5G9y0J59+TIZlEkz0iIiJLwkZ7hvfOO+9ApVJVurVu3VrpsIhIT1ZWwOLFQMOGwN69wPvvKx2RvCSJCT4ZFhN8IiIimTHBN4527dohMzNTd9u2bZvSIRGRDJo0EXPwAeC994A9e5SNR05nz4r+LHZ2ot8AkdyY4BMREcksLEzcZ2QAV64oG4sls7GxgY+Pj+7m4eGhdEhEJJOnngKGDgXKy0WpflGR0hHJQzt6Hx4ONGigbCxkmZjgExERyczFBWjVSmxzFN9wTp06BT8/PwQFBWHYsGFIT0+v9rklJSXIz8+vdCMi0/bZZ4CfH3DyJPDKK0pHIw+W55OhKdpkj4iIyFJpG+0lJwPx8UpHY3mio6OxZMkStGrVCpmZmZgxYwa6deuGw4cPw9nZ+a7nJyYmYsaMGQpESkR15e4OLFkC9O4tkv3gYKBpU6Wj0s/GjeKeCT4ZikqqbUv6eiA/Px+urq7Iy8uDi4uL0uEQEZEZWrAAmDwZGDQIWL1a//3x2nRvubm5CAgIwIIFCzBy5Mi7Hi8pKUFJSYnu6/z8fPj7+/N8EpmB8eOBTz9VOgp5XboE+PoqHQWZGjmu9RzBJyIiMoCoKCAk5HapPhlWw4YN0bJlS5w+fbrKx9VqNdRqtZGjIiI5zJkD5OWJBnWWoFcvJvdkOEzwiYiIDKBrV+DgQaWjqD8KCgpw5swZ/Oc//1E6FCKSmYMD8O23SkdBZB7YZI+IiIjMzpQpU5CUlIRz585hx44dGDx4MKytrfHUU08pHRoREZFiOIJPREREZufChQt46qmncO3aNXh6eqJr167YtWsXPD09lQ6NiIhIMUzwiYiIyOysWLFC6RCIiIhMDkv0iYiIiIiIiCwAE3wiIiIiIiIiC8AEn4iIiIiIiMgCMMEnIiIiIiIisgBM8ImIiIiIiIgsABN8IiIiIiIiIgvABJ+IiIiIiIjIAjDBJyIiIiIiIrIANkoHYIokSQIA5OfnKxwJERGRoL0maa9RpB9e64mIyNTIca1ngl+FGzduAAD8/f0VjoSIiKiyGzduwNXVVekwzB6v9UREZKr0udarJA4F3EWj0eDSpUtwdnbGjRs34O/vj4yMDLi4uCgdmlnLz8/nuZQRz6d8eC7lxfMpn4rnUntN8vPzg5UVZ9jpi9d6w+D/f3nxfMqL51M+PJfy0p7P9PR0qFQqva71HMGvgpWVFZo0aQIAUKlUAAAXFxe+eWXCcykvnk/58FzKi+dTPtpzyZF7+fBab1g8l/Li+ZQXz6d8eC7l5erqqvf55BAAERERERERkQVggk9ERERERERkAZjg34darcb06dOhVquVDsXs8VzKi+dTPjyX8uL5lA/PpXHwPMuH51JePJ/y4vmUD8+lvOQ8n2yyR0RERERERGQBOIJPREREREREZAGY4BMRERERERFZACb4RERERERERBaACT4RERERERGRBWCCfw+fffYZmjVrhgYNGiA6Ohp79uxROiSz9M4770ClUlW6tW7dWumwzMKWLVswYMAA+Pn5QaVSYc2aNZUelyQJb7/9Nnx9fWFvb4+ePXvi1KlTygRrBu53PkeMGHHXezU+Pl6ZYE1cYmIiIiMj4ezsDC8vLwwaNAgnTpyo9Jzi4mIkJCSgUaNGcHJywpAhQ5Cdna1QxKatJuezR48ed70/R48erVDEloPXennwWq8fXu/lw2u9fHitl5exrvVM8Kvxww8/YNKkSZg+fTr279+P0NBQ9OnTB5cvX1Y6NLPUrl07ZGZm6m7btm1TOiSzUFhYiNDQUHz22WdVPj537lx8/PHH+OKLL7B79244OjqiT58+KC4uNnKk5uF+5xMA4uPjK71Xly9fbsQIzUdSUhISEhKwa9cubNiwAbdu3ULv3r1RWFioe87EiRPx66+/YuXKlUhKSsKlS5fw6KOPKhi16arJ+QSAF198sdL7c+7cuQpFbBl4rZcXr/V1x+u9fHitlw+v9fIy2rVeoipFRUVJCQkJuq/Ly8slPz8/KTExUcGozNP06dOl0NBQpcMwewCk1atX677WaDSSj4+PNG/ePN33cnNzJbVaLS1fvlyBCM3LnedTkiRp+PDh0iOPPKJIPObu8uXLEgApKSlJkiTxXrS1tZVWrlype86xY8ckANLOnTuVCtNs3Hk+JUmSHnjgAenll19WLigLxGu9fHitlw+v9/LhtV5evNbLy1DXeo7gV6G0tBT79u1Dz549dd+zsrJCz549sXPnTgUjM1+nTp2Cn58fgoKCMGzYMKSnpysdktlLS0tDVlZWpfepq6sroqOj+T7Vw+bNm+Hl5YVWrVphzJgxuHbtmtIhmYW8vDwAgLu7OwBg3759uHXrVqX3Z+vWrdG0aVO+P2vgzvOptXTpUnh4eKB9+/aYNm0aioqKlAjPIvBaLz9e6w2D13v58VpfN7zWy8tQ13ob2SK0IFevXkV5eTm8vb0rfd/b2xvHjx9XKCrzFR0djSVLlqBVq1bIzMzEjBkz0K1bNxw+fBjOzs5Kh2e2srKyAKDK96n2Maqd+Ph4PProowgMDMSZM2fw+uuvo2/fvti5cyesra2VDs9kaTQaTJgwAbGxsWjfvj0A8f60s7NDw4YNKz2X78/7q+p8AsDTTz+NgIAA+Pn5ITU1Fa+++ipOnDiBVatWKRit+eK1Xl681hsOr/fy4rW+bnitl5chr/VM8Mng+vbtq9sOCQlBdHQ0AgIC8OOPP2LkyJEKRkZU2dChQ3XbHTp0QEhICJo3b47NmzcjLi5OwchMW0JCAg4fPsz5tjKp7nyOGjVKt92hQwf4+voiLi4OZ86cQfPmzY0dJlElvNaTueC1vm54rZeXIa/1LNGvgoeHB6ytre/qAJmdnQ0fHx+ForIcDRs2RMuWLXH69GmlQzFr2vci36eGExQUBA8PD75X72HcuHFYt24dNm3ahCZNmui+7+Pjg9LSUuTm5lZ6Pt+f91bd+axKdHQ0APD9WUe81hsWr/Xy4fXesHitvz9e6+Vl6Gs9E/wq2NnZITw8HBs3btR9T6PRYOPGjYiJiVEwMstQUFCAM2fOwNfXV+lQzFpgYCB8fHwqvU/z8/Oxe/duvk9lcuHCBVy7do3v1SpIkoRx48Zh9erV+OeffxAYGFjp8fDwcNja2lZ6f544cQLp6el8f1bhfuezKikpKQDA92cd8VpvWLzWy4fXe8Pitb56vNbLy1jXepboV2PSpEkYPnw4IiIiEBUVhQ8//BCFhYV47rnnlA7N7EyZMgUDBgxAQEAALl26hOnTp8Pa2hpPPfWU0qGZvIKCgkqf2KWlpSElJQXu7u5o2rQpJkyYgJkzZ6JFixYIDAzEW2+9BT8/PwwaNEi5oE3Yvc6nu7s7ZsyYgSFDhsDHxwdnzpzBK6+8guDgYPTp00fBqE1TQkICli1bhl9++QXOzs66uXaurq6wt7eHq6srRo4ciUmTJsHd3R0uLi4YP348YmJi0LlzZ4WjNz33O59nzpzBsmXL8PDDD6NRo0ZITU3FxIkT0b17d4SEhCgcvfnitV4+vNbrh9d7+fBaLx9e6+VltGu9Xj34Ldwnn3wiNW3aVLKzs5OioqKkXbt2KR2SWXryySclX19fyc7OTmrcuLH05JNPSqdPn1Y6LLOwadMmCcBdt+HDh0uSJJbOeeuttyRvb29JrVZLcXFx0okTJ5QN2oTd63wWFRVJvXv3ljw9PSVbW1spICBAevHFF6WsrCylwzZJVZ1HANLixYt1z7l586Y0duxYyc3NTXJwcJAGDx4sZWZmKhe0Cbvf+UxPT5e6d+8uubu7S2q1WgoODpamTp0q5eXlKRu4BeC1Xh681uuH13v58FovH17r5WWsa73q34MRERERERERkRnjHHwiIiIiIiIiC8AEn4iIiIiIiMgCMMEnIiIiIiIisgBM8ImIiIiIiIgsABN8IiIiIiIiIgvABJ+IiIiIiIjIAjDBJyIiIiIiIrIATPCJiIiIiIiILAATfCIyCSqVCmvWrFE6DCIiIjIQXuuJDI8JPhFhxIgRUKlUd93i4+OVDo2IiIhkwGs9Uf1go3QARGQa4uPjsXjx4krfU6vVCkVDREREcuO1nsjycQSfiACIC7yPj0+lm5ubGwBRUrdw4UL07dsX9vb2CAoKwk8//VTp9YcOHcJDDz0Ee3t7NGrUCKNGjUJBQUGl53zzzTdo164d1Go1fH19MW7cuEqPX716FYMHD4aDgwNatGiBtWvX6h7LycnBsGHD4OnpCXt7e7Ro0eKuP1KIiIioerzWE1k+JvhEVCNvvfUWhgwZgoMHD2LYsGEYOnQojh07BgAoLCxEnz594Obmhr1792LlypX4+++/K13UFy5ciISEBIwaNQqHDh3C2rVrERwcXOkYM2bMwBNPPIHU1FQ8/PDDGDZsGK5fv647/tGjR7F+/XocO3YMCxcuhIeHh/FOABERkYXjtZ7IAkhEVO8NHz5csra2lhwdHSvdZs2aJUmSJAGQRo8eXek10dHR0pgxYyRJkqRFixZJbm5uUkFBge7x3377TbKyspKysrIkSZIkPz8/6Y033qg2BgDSm2++qfu6oKBAAiCtX79ekiRJGjBggPTcc8/J8wMTERHVM7zWE9UPnINPRACABx98EAsXLqz0PXd3d912TExMpcdiYmKQkpICADh27BhCQ0Ph6Oioezw2NhYajQYnTpyASqXCpUuXEBcXd88YQkJCdNuOjo5wcXHB5cuXAQBjxozBkCFDsH//fvTu3RuDBg1Cly5d6vSzEhER1Ue81hNZPib4RARAXGTvLKOTi729fY2eZ2trW+lrlUoFjUYDAOjbty/Onz+P33//HRs2bEBcXBwSEhIwf/582eMlIiKyRLzWE1k+zsEnohrZtWvXXV+3adMGANCmTRscPHgQhYWFuse3b98OKysrtGrVCs7OzmjWrBk2btyoVwyenp4YPnw4vv/+e3z44YdYtGiRXvsjIiKi23itJzJ/HMEnIgBASUkJsrKyKn3PxsZG19xm5cqViIiIQNeuXbF06VLs2bMHX3/9NQBg2LBhmD59OoYPH4533nkHV65cwfjx4/Gf//wH3t7eAIB33nkHo0ePhpeXF/r27YsbN25g+/btGD9+fI3ie/vttxEeHo527dqhpKQE69at0/3RQURERPfHaz2R5WOCT0QAgD/++AO+vr6VvteqVSscP34cgOh6u2LFCowdOxa+vr5Yvnw52rZtCwBwcHDAn3/+iZdffhmRkZFwcHDAkCFDsGDBAt2+hg8fjuLiYnzwwQeYMmUKPDw88Nhjj9U4Pjs7O0ybNg3nzp2Dvb09unXrhhUrVsjwkxMREdUPvNYTWT6VJEmS0kEQkWlTqVRYvXo1Bg0apHQoREREZAC81hNZBs7BJyIiIiIiIrIATPCJiIiIiIiILABL9ImIiIiIiIgsAEfwiYiIiIiIiCwAE3wiIiIiIiIiC8AEn4iIiIiIiMgCMMEnIiIiIiIisgBM8ImIiIiIiIgsABN8IiIiIiIiIgvABJ+IiIiIiIjIAjDBJyIiIiIiIrIA/x/8N2BRQj1sjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_imf and y_one_hot are already defined\n",
    "# Convert the DataFrame to a NumPy array\n",
    "X_imf_array = X_imf.to_numpy()\n",
    "\n",
    "# Initialize an empty array to hold the reshaped data\n",
    "reshaped_data = np.zeros((414, 84, 7680))\n",
    "\n",
    "# Populate the reshaped_data array\n",
    "for i in range(414):\n",
    "    for j in range(84):\n",
    "        reshaped_data[i, j, :] = X_imf_array[i, j]\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "standardized_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "# Normalize each channel separately across the entire dataset\n",
    "mean = standardized_data.mean(dim=(0, 2), keepdim=True)\n",
    "std = standardized_data.std(dim=(0, 2), keepdim=True)\n",
    "standardized_data = (standardized_data - mean) / std\n",
    "\n",
    "# Convert y_one_hot to PyTorch tensor\n",
    "y_one_hot = torch.tensor(y_one_hot, dtype=torch.float32)\n",
    "\n",
    "# Reshape standardized_data to fit the RNN input requirements\n",
    "standardized_data = standardized_data.view(414, 84, 7680)\n",
    "\n",
    "# Convert y_one_hot to class indices\n",
    "y_indices = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(standardized_data, y_indices)\n",
    "\n",
    "# Define the LSTM-based RNN model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=7680, hidden_size=128, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 9)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 128).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(0), 128).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Path to save the best model\n",
    "model_path = './Models/best_lstm_model.pth'\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{k_folds}')\n",
    "    \n",
    "    # Create data loaders for training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, optimizer, and scheduler\n",
    "    model = LSTMModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    patience = 3\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/20], Training Loss: {train_losses[-1]:.4f}')\n",
    "        \n",
    "        # Evaluate the model on the training set\n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                train_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_train_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_train_predictions += batch_x.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train_predictions / total_train_predictions * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print(f\"Accuracy on training set: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                correct_predictions += (predicted_classes == batch_y).sum().item()\n",
    "                total_predictions += batch_x.size(0)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "        \n",
    "        print(f\"Accuracy on validation set: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            save_model(model, model_path)\n",
    "            print(\"Saved the best model parameters.\")\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model parameters for final evaluation or further training\n",
    "load_model(model, model_path)\n",
    "print(\"Loaded the best model parameters for final evaluation or further training.\")\n",
    "\n",
    "# Plotting the metrics\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion-Recognition-kuwpk_hL",
   "language": "python",
   "name": "emotion-recognition-kuwpk_hl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
